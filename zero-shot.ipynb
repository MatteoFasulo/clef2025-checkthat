{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity in News Articles\n",
    "\n",
    "## Group:\n",
    "- Luca Babboni - luca.babboni2@studio.unibo.it\n",
    "- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n",
    "- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n",
    "\n",
    "This is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n",
    "\n",
    "The task comprises three settings:\n",
    "\n",
    "* Monolingual: train and test on data in a given language\n",
    "* Multilingual: train and test on data comprising several languages\n",
    "* Zero-shot: train on several languages and test on unseen languages\n",
    "\n",
    "training data in five languages:\n",
    "* Arabic\n",
    "* Bulgarian\n",
    "* English\n",
    "* German\n",
    "* Italian\n",
    "\n",
    "The official evaluation is macro-averaged F1 between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing dependencies\n",
    "\n",
    "This notebook uses quantized models, and some additional libraries are required. If you are running this notebook on either Colab or Kaggle, please run the cell below once, then run the whole notebook normally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers[torch] bitsandbytes trl peft sacremoses ctranslate2 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding, \n",
    "    BitsAndBytesConfig,\n",
    "    PreTrainedModel,\n",
    "    DebertaV2Model, \n",
    "    DebertaV2Config, \n",
    "    pipeline\n",
    ")\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "from transformers.models.deberta.modeling_deberta import ContextPooler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tqdm.pandas() # display tqdm on pandas apply functions\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Library Seeds\n",
    "\n",
    "This step is necessary to guarantee reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Data Folder  \n",
    "\n",
    "Please modify this path with your dataset's local path. The `data` folder should follow the official challenge structure:\n",
    "```\n",
    "data/\n",
    "|---- arabic/\n",
    "|--------- xxxx.tsv\n",
    "|---- bulgarian/\n",
    "|--------- xxxx.tsv\n",
    "|---- english/\n",
    "|--------- xxxx.tsv\n",
    "|---- german/\n",
    "|--------- xxxx.tsv\n",
    "|---- italian/\n",
    "|--------- xxxx.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/kaggle/input/clef2025-checkthat/data' # data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging face notebook login\n",
    "\n",
    "To correctly download and use the hugging face models, a token key needs to be provided. Please refer to this [page](https://huggingface.co/docs/hub/security-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c4c9f7184d436dbb0827cd62a6235c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity Class  \n",
    "This class is used throughout the whole notebook as a utility toolbox to avoid code redundancy. When a method of this class is called for the first time, its behavior will be explained.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subjectivity:\n",
    "    \"\"\"\n",
    "    A class for handling multilingual subjectivity classification datasets.\n",
    "    \n",
    "    This class provides functionality to load, process, and prepare datasets for \n",
    "    subjective/objective text classification across multiple languages. It supports:\n",
    "    - Loading and organizing datasets from multiple languages\n",
    "    - Splitting data into train/dev/test sets\n",
    "    - Analyzing label distributions\n",
    "    - Loading pre-trained tokenizers and models\n",
    "    - Computing class weights for imbalanced datasets\n",
    "    \n",
    "    Attributes:\n",
    "        seed (int): Random seed for reproducibility\n",
    "        device (str): Computation device ('cuda' or 'cpu')\n",
    "        languages (list): List of available languages in the dataset\n",
    "        dataset (pd.DataFrame): Combined dataset with all languages and splits\n",
    "        train (pd.DataFrame): Training split of the dataset\n",
    "        dev (pd.DataFrame): Development split of the dataset\n",
    "        test (pd.DataFrame): Test split of the dataset\n",
    "        all_data (dict): Nested dictionary organizing data by language and split\n",
    "        tokenizer (AutoTokenizer, optional): Hugging Face tokenizer\n",
    "        model (AutoModelForSequenceClassification, optional): Classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, data_folder: str = 'data', seed: int = 42, device: str = 'cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the Subjectivity class.\n",
    "        \n",
    "        Args:\n",
    "            data_folder (str): Directory path containing the dataset files.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "            device (str): Device to use for computations ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "        self.languages = [language for language in os.listdir(data_folder)]\n",
    "\n",
    "        dataset = self.create_dataset(data_folder=data_folder)\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        train, dev, test = self.get_splits(dataset, print_shapes=True)\n",
    "        self.train = train\n",
    "        self.dev = dev\n",
    "        self.test = test\n",
    "\n",
    "        self.all_data = self.get_per_lang_dataset()\n",
    "        \n",
    "\n",
    "    def create_dataset(self, data_folder: str = 'data'):\n",
    "        \"\"\"\n",
    "        Create a consolidated dataset from files in multiple languages.\n",
    "        \n",
    "        Args:\n",
    "            data_folder (str): Directory path containing subdirectories for each language.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Combined dataset with columns for sentence_id, sentence, label, \n",
    "                          language, and split information.\n",
    "        \"\"\"\n",
    "        dataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n",
    "        for language in os.listdir(data_folder):\n",
    "            for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n",
    "                if '.tsv' in filename:\n",
    "                    abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n",
    "                    df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "                    if 'solved_conflict' in df.columns:\n",
    "                        df.drop(columns=['solved_conflict'], inplace=True)\n",
    "                    df['lang'] = language\n",
    "                    df['split'] = Path(filename).stem\n",
    "                    dataset = pd.concat([dataset, df], axis=0)\n",
    "        return dataset\n",
    "\n",
    "    def get_splits(self, dataset: pd.DataFrame, print_shapes: bool = True):\n",
    "        \"\"\"\n",
    "        Split the dataset into training, development, and test sets.\n",
    "        \n",
    "        Args:\n",
    "            dataset (pd.DataFrame): The combined dataset to split.\n",
    "            print_shapes (bool): Whether to print the shapes of the resulting splits.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing three pandas DataFrames (train, dev, test).\n",
    "        \"\"\"\n",
    "        train = dataset[dataset['split'].str.contains('train')].copy()\n",
    "        dev = dataset[(dataset['split'].str.contains('dev')) & ~(dataset['split'].str.contains('dev_test'))].copy()\n",
    "        test = dataset[dataset['split'].str.contains('dev_test')].copy()\n",
    "\n",
    "        # encode the target variable to int (0: obj; 1: subj)\n",
    "        train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "        dev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "        test.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "\n",
    "        # cast to int\n",
    "        train['label'] = train['label'].astype(int)\n",
    "        dev['label'] = dev['label'].astype(int)\n",
    "        test['label'] = test['label'].astype(int)\n",
    "\n",
    "        if print_shapes:\n",
    "            print(f\"Train: {train.shape}\")\n",
    "            print(f\"Dev: {dev.shape}\")\n",
    "            print(f\"Test: {test.shape}\")\n",
    "            \n",
    "        return train, dev, test\n",
    "\n",
    "    def get_per_lang_dataset(self):\n",
    "        \"\"\"\n",
    "        Organize the dataset by language and split (train, dev, test).\n",
    "        \n",
    "        Returns:\n",
    "            dict: A nested dictionary with languages as the outer keys and\n",
    "                  split names ('train', 'dev', 'test') as inner keys.\n",
    "                  For example:\n",
    "                  {\n",
    "                      'english': {\n",
    "                          'train': pd.DataFrame,\n",
    "                          'dev': pd.DataFrame,\n",
    "                          'test': pd.DataFrame\n",
    "                      },\n",
    "                      ...\n",
    "                  }\n",
    "        \"\"\"\n",
    "        dataset_dict = {}\n",
    "        for language in self.languages:\n",
    "            dataset_dict[language] = {}\n",
    "            # get the train data\n",
    "            dataset_dict[language]['train'] = self.train[self.train['lang']==language].copy()\n",
    "            # get the dev data\n",
    "            dataset_dict[language]['dev'] = self.dev[self.dev['lang']==language].copy()\n",
    "            # get the test data\n",
    "            dataset_dict[language]['test'] = self.test[self.test['lang']==language].copy()\n",
    "        return dataset_dict\n",
    "\n",
    "    def print_label_distrib(self, dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Print the normalized distribution of labels in the dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataset (pd.DataFrame): The dataset containing a 'label' column.\n",
    "            \n",
    "        Returns:\n",
    "            None: Prints the percentage distribution of each label.\n",
    "        \"\"\"\n",
    "        print(dataset['label'].value_counts(normalize=True))\n",
    "\n",
    "    def get_tokenizer(self, model_card: str = \"microsoft/mdeberta-v3-base\"):\n",
    "        \"\"\"\n",
    "        Load a tokenizer from the Hugging Face model hub.\n",
    "        \n",
    "        Args:\n",
    "            model_card (str): Identifier for the pre-trained tokenizer to load.\n",
    "            \n",
    "        Returns:\n",
    "            AutoTokenizer: The loaded tokenizer.\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "        self.tokenizer = tokenizer\n",
    "        return tokenizer\n",
    "\n",
    "    def get_model(self, model_card: str = \"microsoft/mdeberta-v3-base\", *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Load a pre-trained model from the Hugging Face model hub.\n",
    "        \n",
    "        Args:\n",
    "            model_card (str): Identifier for the pre-trained model to load.\n",
    "            *args: Variable length argument list to pass to the model constructor.\n",
    "            **kwargs: Arbitrary keyword arguments to pass to the model constructor.\n",
    "            \n",
    "        Returns:\n",
    "            AutoModelForSequenceClassification: The loaded model.\n",
    "        \"\"\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_card, *args, **kwargs)\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def get_class_weights(self, dataset: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Compute class weights for imbalanced datasets.\n",
    "        \n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset containing a 'label' column.\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Array of class weights where the index corresponds to the class label.\n",
    "        \"\"\"\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(dataset['label']), y=dataset['label'])\n",
    "        return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Detector Object  \n",
    "\n",
    "The `__init__()` method of the `Subjectivity` class will load the dataset, set the device and the seeds, and automatically create the `test`, `dev`, and `train` splits from the datasets. It will also convert the `SUBJ` and `OBJ` labels to their corresponding numerical versions, so they are ready to be fed into a model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6418, 5)\n",
      "Dev: (2401, 5)\n",
      "Test: (2332, 5)\n"
     ]
    }
   ],
   "source": [
    "detector = Subjectivity(data_folder=data_folder, seed=SEED, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.631349\n",
      "1    0.368651\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.612245\n",
      "1    0.387755\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.657376\n",
      "1    0.342624\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "detector.print_label_distrib(detector.train)\n",
    "detector.print_label_distrib(detector.dev)\n",
    "detector.print_label_distrib(detector.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    800.000000\n",
       "mean     126.296250\n",
       "std       67.334117\n",
       "min       31.000000\n",
       "25%       80.000000\n",
       "50%      112.500000\n",
       "75%      161.000000\n",
       "max      625.000000\n",
       "Name: sentence, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detector.all_data['german']['train']['sentence'].str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the `results` dictionary and the `predictions_dict` dictionary. They will be used to store all the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "predictions_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trainer  \n",
    "\n",
    "This class extends the `Trainer` provided by Hugging Face. Since we needed to tweak some details in the training process, we opted to override some `Trainer` functions with custom ones.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    Custom Trainer class extending Hugging Face's Trainer with additional functionality:\n",
    "    - Support for class weights to handle imbalanced datasets\n",
    "    - Custom loss computation with weighted cross-entropy\n",
    "    - Threshold optimization for binary classification\n",
    "    - Custom prediction with threshold application\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, class_weights=None, weights_dtype=torch.float32, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the CustomTrainer.\n",
    "        \n",
    "        Args:\n",
    "            class_weights (array-like, optional): Weights for each class to handle class imbalance.\n",
    "            weights_dtype (torch.dtype): Data type for the class weights tensor.\n",
    "            *args, **kwargs: Arguments passed to the parent Trainer class.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=weights_dtype).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        Compute the training loss with optional class weighting.\n",
    "        \n",
    "        Args:\n",
    "            model: The model to train\n",
    "            inputs: The inputs and targets of the model\n",
    "            return_outputs (bool): Whether to return the outputs along with the loss\n",
    "            num_items_in_batch: Not used but kept for compatibility\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor or tuple: Loss value alone or with model outputs\n",
    "        \"\"\"\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits \n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_best_threshold(self, dataset, ignore_keys=None, metric_key_prefix=\"test\"):\n",
    "        \"\"\"\n",
    "        Find the optimal classification threshold that maximizes macro F1 score.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The dataset to use for threshold optimization\n",
    "            ignore_keys (list, optional): Keys to ignore in the model outputs\n",
    "            metric_key_prefix (str): Prefix for metric keys in the output\n",
    "            \n",
    "        Returns:\n",
    "            float: The optimal threshold value\n",
    "        \"\"\"\n",
    "        # Get raw predictions from parent class\n",
    "        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n",
    "\n",
    "        # Convert logits to probabilities using softmax (for binary classification)\n",
    "        logits = output.predictions\n",
    "        logits_tensor = torch.tensor(logits)\n",
    "        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n",
    "\n",
    "        # Calculate optimal threshold\n",
    "        labels = output.label_ids\n",
    "        thresholds = np.linspace(0.1, 0.9, 100) \n",
    "\n",
    "        best_threshold = 0.5  # Default threshold\n",
    "        best_f1 = 0\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            predictions = (probabilities[:, 1] >= threshold).astype(int)\n",
    "            _, _, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\", zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "\n",
    "        # Return the best threshold found\n",
    "        return best_threshold\n",
    "        \n",
    "    def predict(self, dataset, threshold: float = 0.5, ignore_keys=None, metric_key_prefix=\"test\"):\n",
    "        \"\"\"\n",
    "        Generate predictions with a custom threshold for binary classification.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The dataset to generate predictions for\n",
    "            threshold (float): The classification threshold (default: 0.5)\n",
    "            ignore_keys (list, optional): Keys to ignore in model outputs\n",
    "            metric_key_prefix (str): Prefix for metric keys in the output\n",
    "            \n",
    "        Returns:\n",
    "            PredictionOutput: Object containing predictions, labels, and metrics\n",
    "        \"\"\"\n",
    "        # Get raw predictions from parent class\n",
    "        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # Convert logits to probabilities using softmax (for binary classification)\n",
    "        logits = output.predictions\n",
    "        logits_tensor = torch.tensor(logits)\n",
    "        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n",
    "        \n",
    "        final_predictions = (probabilities[:, 1] >= threshold).astype(int)\n",
    "\n",
    "        # Update predictions in the output object\n",
    "        return PredictionOutput(\n",
    "            predictions=final_predictions,\n",
    "            label_ids=output.label_ids,\n",
    "            metrics=output.metrics\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(texts):\n",
    "    \"\"\"\n",
    "    Tokenize text data using the current tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        texts (dict): Dictionary containing text data with a 'sentence' field\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with tokenized text features including input_ids, \n",
    "              attention_mask, and potentially token_type_ids\n",
    "    \"\"\"\n",
    "    return tokenizer(texts['sentence'], padding=True, truncation=True, max_length=256, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for subjectivity classification models.\n",
    "    \n",
    "    This function computes various performance metrics for classification results:\n",
    "    - Accuracy: Overall correctness of predictions\n",
    "    - Macro-averaged precision, recall, and F1: Averages across both classes with equal weight\n",
    "    - Class-specific metrics: Precision, recall, and F1 specifically for the subjective class\n",
    "    \n",
    "    Args:\n",
    "        eval_pred (tuple): Tuple containing (predictions, labels) where:\n",
    "            - predictions: Raw model outputs/logits with shape (n_samples, n_classes)\n",
    "            - labels: Ground truth labels with shape (n_samples,)\n",
    "            \n",
    "    Returns:\n",
    "        dict: Dictionary containing the following metrics:\n",
    "            - macro_F1: Macro-averaged F1 score across all classes\n",
    "            - macro_P: Macro-averaged precision across all classes\n",
    "            - macro_R: Macro-averaged recall across all classes\n",
    "            - SUBJ_F1: F1 score for the subjective class (label 1)\n",
    "            - SUBJ_P: Precision for the subjective class\n",
    "            - SUBJ_R: Recall for the subjective class\n",
    "            - accuracy: Overall accuracy\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                                zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                                zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'macro_F1': m_f1,\n",
    "        'macro_P': m_prec,\n",
    "        'macro_R': m_rec,\n",
    "        'SUBJ_F1': p_f1[0],\n",
    "        'SUBJ_P': p_prec[0],\n",
    "        'SUBJ_R': p_rec[0],\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(test_data, predictions, filename: str, save_dir: str = 'results'):\n",
    "    \"\"\"\n",
    "    Save model predictions to a TSV file with sentence IDs and predicted labels.\n",
    "    \n",
    "    Args:\n",
    "        test_data: Dataset containing the 'sentence_id' field to match with predictions\n",
    "        predictions: Array of binary predictions (0 for OBJ, 1 for SUBJ)\n",
    "        filename: Name of the output file (should end with .tsv)\n",
    "        save_dir: Directory to save the predictions file (default: 'results')\n",
    "        \n",
    "    Returns:\n",
    "        str: Full path to the saved predictions file\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df['sentence_id'] = test_data['sentence_id']\n",
    "    pred_df['label'] = predictions\n",
    "    pred_df['label'] = pred_df['label'].apply(lambda x: 'OBJ' if x == 0 else 'SUBJ')\n",
    "\n",
    "    predictions_filepath = os.path.join(save_dir, filename)\n",
    "    pred_df.to_csv(predictions_filepath, index=False, sep='\\t')\n",
    "\n",
    "    print(f\"Saved predictions into file:\", predictions_filepath)\n",
    "    return predictions_filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(PreTrainedModel):\n",
    "    config_class = DebertaV2Config\n",
    "\n",
    "    def __init__(self, config, sentiment_dim=3, num_labels=2, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.pooler = ContextPooler(config)\n",
    "        output_dim = self.pooler.output_dim\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.classifier = nn.Linear(output_dim + sentiment_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, positive, neutral, negative, attention_mask=None, labels=None):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        encoder_layer = outputs[0]\n",
    "        pooled_output = self.pooler(encoder_layer)\n",
    "        \n",
    "        # Sentiment features as a single tensor\n",
    "        sentiment_features = torch.stack((positive, neutral, negative), dim=1)  # Shape: (batch_size, 3)\n",
    "        \n",
    "        # Combine CLS embedding with sentiment features\n",
    "        combined_features = torch.cat((pooled_output, sentiment_features), dim=1)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classifier(self.dropout(combined_features))\n",
    "        \n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Pipeline for Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment(text):\n",
    "    sentiments = pipe(text)[0]\n",
    "    return {k:v for k,v in [(list(sentiment.values())[0], list(sentiment.values())[1]) for sentiment in sentiments]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "\n",
    "This function can be then integrated in the Subjectivity class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:06:16.183313Z",
     "iopub.status.busy": "2025-03-13T10:06:16.182974Z",
     "iopub.status.idle": "2025-03-13T10:06:16.187690Z",
     "shell.execute_reply": "2025-03-13T10:06:16.186800Z",
     "shell.execute_reply.started": "2025-03-13T10:06:16.183283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zero_shot_prepare_data(train_languages : list,\n",
    "                           train : pd.DataFrame,\n",
    "                           dev : pd.DataFrame,\n",
    "                           test : pd.DataFrame):\n",
    "\n",
    "    \n",
    "    train_set = train[train[\"lang\"].isin(train_languages)].copy()\n",
    "    dev_set = dev[~dev[\"lang\"].isin(train_languages)].copy()\n",
    "    test_set = test[~test[\"lang\"].isin(train_languages)].copy()\n",
    "\n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:06:16.436192Z",
     "iopub.status.busy": "2025-03-13T10:06:16.435880Z",
     "iopub.status.idle": "2025-03-13T10:06:16.440296Z",
     "shell.execute_reply": "2025-03-13T10:06:16.439435Z",
     "shell.execute_reply.started": "2025-03-13T10:06:16.436169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_name(names : tuple):\n",
    "    generated_name = \"\"\n",
    "    for x in names:\n",
    "        generated_name = generated_name + x.capitalize()[:2]\n",
    "    return generated_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loop to test triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:00:03.746456Z",
     "iopub.status.busy": "2025-03-13T11:00:03.746112Z",
     "iopub.status.idle": "2025-03-13T11:17:10.123905Z",
     "shell.execute_reply": "2025-03-13T11:17:10.123148Z",
     "shell.execute_reply.started": "2025-03-13T11:00:03.746429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING 0 - ('english', 'italian')\n",
      "Model deleted!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5096cebddbbb42d49c99cd0d136175c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a295088f83164c6496b7e3ab2b2539a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ccb46ff1124a5a818c829943590d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 07:50, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro P</th>\n",
       "      <th>Macro R</th>\n",
       "      <th>Subj F1</th>\n",
       "      <th>Subj P</th>\n",
       "      <th>Subj R</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657371</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>0.668303</td>\n",
       "      <td>0.669217</td>\n",
       "      <td>0.607522</td>\n",
       "      <td>0.602294</td>\n",
       "      <td>0.612840</td>\n",
       "      <td>0.680031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.645487</td>\n",
       "      <td>0.692242</td>\n",
       "      <td>0.645853</td>\n",
       "      <td>0.519512</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.414397</td>\n",
       "      <td>0.690252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.014649</td>\n",
       "      <td>0.661566</td>\n",
       "      <td>0.695910</td>\n",
       "      <td>0.659338</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.457198</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>1.211402</td>\n",
       "      <td>0.649742</td>\n",
       "      <td>0.693915</td>\n",
       "      <td>0.649397</td>\n",
       "      <td>0.527207</td>\n",
       "      <td>0.696486</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.692610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>1.423838</td>\n",
       "      <td>0.649075</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.649497</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.412451</td>\n",
       "      <td>0.694969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>1.465843</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>0.694394</td>\n",
       "      <td>0.647485</td>\n",
       "      <td>0.521951</td>\n",
       "      <td>0.699346</td>\n",
       "      <td>0.416342</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2443/2443 [00:20<00:00, 121.21it/s]\n",
      "100%|██████████| 1272/1272 [00:10<00:00, 120.28it/s]\n",
      "100%|██████████| 1335/1335 [00:11<00:00, 119.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7e4f46a9bf43b5b7747e378701cfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d5000d7f1648ae8473934035cc1eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25701476bbb403380a22e7d27feed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 07:43, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro P</th>\n",
       "      <th>Macro R</th>\n",
       "      <th>Subj F1</th>\n",
       "      <th>Subj P</th>\n",
       "      <th>Subj R</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725917</td>\n",
       "      <td>0.612220</td>\n",
       "      <td>0.629209</td>\n",
       "      <td>0.631277</td>\n",
       "      <td>0.603379</td>\n",
       "      <td>0.514403</td>\n",
       "      <td>0.729572</td>\n",
       "      <td>0.612421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.972189</td>\n",
       "      <td>0.634223</td>\n",
       "      <td>0.718251</td>\n",
       "      <td>0.640070</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.757202</td>\n",
       "      <td>0.357977</td>\n",
       "      <td>0.694182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.685101</td>\n",
       "      <td>0.702052</td>\n",
       "      <td>0.681131</td>\n",
       "      <td>0.596721</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.531128</td>\n",
       "      <td>0.709906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.252779</td>\n",
       "      <td>0.653370</td>\n",
       "      <td>0.706423</td>\n",
       "      <td>0.653422</td>\n",
       "      <td>0.527744</td>\n",
       "      <td>0.720539</td>\n",
       "      <td>0.416342</td>\n",
       "      <td>0.698899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.394758</td>\n",
       "      <td>0.648759</td>\n",
       "      <td>0.707636</td>\n",
       "      <td>0.649911</td>\n",
       "      <td>0.518148</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.402724</td>\n",
       "      <td>0.697327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.417408</td>\n",
       "      <td>0.653806</td>\n",
       "      <td>0.701681</td>\n",
       "      <td>0.653355</td>\n",
       "      <td>0.531060</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.697327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_card = \"microsoft/mdeberta-v3-base\"\n",
    "tokenizer = detector.get_tokenizer(model_card=model_card)\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 16\n",
    "lr = 1e-5\n",
    "weight_decay = 0.0\n",
    "label_smoothing = 0.0\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    #warmup_ratio=0.5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "for i, group in enumerate(itertools.combinations(detector.languages, 3)):\n",
    "    print(f\"TESTING {i} - {group}\")\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        del model\n",
    "        print(\"Model deleted!\")\n",
    "    gc.collect()\n",
    "    \n",
    "    group_name = generate_name(group)\n",
    "\n",
    "    zs_train, zs_dev, zs_test = zero_shot_prepare_data(group, detector.train, detector.dev, detector.test)\n",
    "\n",
    "    language = group_name+'-NoSentiment'\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = Dataset.from_pandas(zs_train)\n",
    "    dev_data = Dataset.from_pandas(zs_dev)\n",
    "    test_data = Dataset.from_pandas(zs_test)\n",
    "    \n",
    "    train_data = train_data.map(tokenize_text, batched=True)\n",
    "    dev_data = dev_data.map(tokenize_text, batched=True)\n",
    "    test_data = test_data.map(tokenize_text, batched=True)\n",
    "    \n",
    "    class_weights = detector.get_class_weights(zs_train)\n",
    "\n",
    "    model = detector.get_model(\n",
    "        model_card=model_card, \n",
    "        num_labels=2, \n",
    "        id2label={0: 'OBJ', 1: 'SUBJ'}, \n",
    "        label2id={'OBJ': 0, 'SUBJ': 1},\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    \n",
    "    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=dev_data,\n",
    "        data_collator=collator_fn,\n",
    "        compute_metrics=evaluate_metrics,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n",
    "    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n",
    "    predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                                zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                                zero_division=0)\n",
    "    stats = {\n",
    "            'macro_F1': m_f1,\n",
    "            'macro_P': m_prec,\n",
    "            'macro_R': m_rec,\n",
    "            'SUBJ_F1': p_f1[0],\n",
    "            'SUBJ_P': p_prec[0],\n",
    "            'SUBJ_R': p_rec[0],\n",
    "            'accuracy': acc\n",
    "        }\n",
    "    \n",
    "    results[language] = stats\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        del model\n",
    "        print(\"Model deleted!\")\n",
    "    gc.collect()\n",
    "\n",
    "    language = group_name+'-Sentiment'\n",
    "    \n",
    "    model = CustomModel(\n",
    "        model_name=model_card, \n",
    "        num_labels=2, \n",
    "        sentiment_dim=3\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    zs_train[['positive', 'neutral', 'negative']] = zs_train.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "    zs_dev[['positive', 'neutral', 'negative']] = zs_dev.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "    zs_test[['positive', 'neutral', 'negative']] = zs_test.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "\n",
    "    train_data = Dataset.from_pandas(zs_train)\n",
    "    dev_data = Dataset.from_pandas(zs_dev)\n",
    "    test_data = Dataset.from_pandas(zs_test)\n",
    "\n",
    "    train_data = train_data.map(tokenize_text, batched=True)\n",
    "    dev_data = dev_data.map(tokenize_text, batched=True)\n",
    "    test_data = test_data.map(tokenize_text, batched=True)\n",
    "\n",
    "    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_data,\n",
    "        eval_dataset = dev_data,\n",
    "        data_collator = collator_fn,\n",
    "        compute_metrics = evaluate_metrics,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n",
    "    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n",
    "    predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                                zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                                zero_division=0)\n",
    "    stats = {\n",
    "            'macro_F1': m_f1,\n",
    "            'macro_P': m_prec,\n",
    "            'macro_R': m_rec,\n",
    "            'SUBJ_F1': p_f1[0],\n",
    "            'SUBJ_P': p_prec[0],\n",
    "            'SUBJ_R': p_rec[0],\n",
    "            'accuracy': acc\n",
    "        }\n",
    "    \n",
    "    results[language] = stats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:22:07.192577Z",
     "iopub.status.busy": "2025-03-13T11:22:07.192225Z",
     "iopub.status.idle": "2025-03-13T11:22:07.204695Z",
     "shell.execute_reply": "2025-03-13T11:22:07.204011Z",
     "shell.execute_reply.started": "2025-03-13T11:22:07.192552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_F1</th>\n",
       "      <th>macro_P</th>\n",
       "      <th>macro_R</th>\n",
       "      <th>SUBJ_F1</th>\n",
       "      <th>SUBJ_P</th>\n",
       "      <th>SUBJ_R</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EnIt-NoSentiment</th>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>0.6135</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.5661</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.6397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnIt-Sentiment</th>\n",
       "      <td>0.6022</td>\n",
       "      <td>0.6074</td>\n",
       "      <td>0.6012</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.5451</td>\n",
       "      <td>0.4695</td>\n",
       "      <td>0.6262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  macro_F1  macro_P  macro_R  SUBJ_F1  SUBJ_P  SUBJ_R  \\\n",
       "EnIt-NoSentiment    0.6147   0.6219   0.6135   0.5166  0.5661  0.4750   \n",
       "EnIt-Sentiment      0.6022   0.6074   0.6012   0.5045  0.5451  0.4695   \n",
       "\n",
       "                  accuracy  \n",
       "EnIt-NoSentiment    0.6397  \n",
       "EnIt-Sentiment      0.6262  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T.sort_values(by='macro_F1', ascending=False).round(4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame(results).to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot without Arabic language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:03.126861Z",
     "iopub.status.busy": "2025-03-16T14:58:03.126516Z",
     "iopub.status.idle": "2025-03-16T14:58:03.694451Z",
     "shell.execute_reply": "2025-03-16T14:58:03.693460Z",
     "shell.execute_reply.started": "2025-03-16T14:58:03.126832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10705"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "    print(\"Model deleted!\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:06.390436Z",
     "iopub.status.busy": "2025-03-16T14:58:06.390150Z",
     "iopub.status.idle": "2025-03-16T14:58:06.394358Z",
     "shell.execute_reply": "2025-03-16T14:58:06.393549Z",
     "shell.execute_reply.started": "2025-03-16T14:58:06.390415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 16\n",
    "lr = 1e-5\n",
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:07.521759Z",
     "iopub.status.busy": "2025-03-16T14:58:07.521455Z",
     "iopub.status.idle": "2025-03-16T14:58:14.107408Z",
     "shell.execute_reply": "2025-03-16T14:58:14.106685Z",
     "shell.execute_reply.started": "2025-03-16T14:58:07.521737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_card = \"microsoft/mdeberta-v3-base\"\n",
    "\n",
    "tokenizer = detector.get_tokenizer(model_card=model_card)\n",
    "\n",
    "# Load the config\n",
    "config = DebertaV2Config.from_pretrained(\n",
    "    model_card,\n",
    "    num_labels=2,\n",
    "    id2label={0: 'OBJ', 1: 'SUBJ'},\n",
    "    label2id={'OBJ': 0, 'SUBJ': 1},\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomModel(config=config, sentiment_dim=3, num_labels=2)\n",
    "\n",
    "# Load pretrained weights from the original DeBERTa model\n",
    "model.deberta = DebertaV2Model.from_pretrained(model_card, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:14.108862Z",
     "iopub.status.busy": "2025-03-16T14:58:14.108613Z",
     "iopub.status.idle": "2025-03-16T14:58:32.396070Z",
     "shell.execute_reply": "2025-03-16T14:58:32.395313Z",
     "shell.execute_reply.started": "2025-03-16T14:58:14.108841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|██████████| 830/830 [00:06<00:00, 123.08it/s]\n",
      "100%|██████████| 462/462 [00:03<00:00, 121.79it/s]\n",
      "100%|██████████| 484/484 [00:04<00:00, 120.10it/s]\n"
     ]
    }
   ],
   "source": [
    "detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "detector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "detector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:32.397628Z",
     "iopub.status.busy": "2025-03-16T14:58:32.397400Z",
     "iopub.status.idle": "2025-03-16T14:58:32.906176Z",
     "shell.execute_reply": "2025-03-16T14:58:32.905432Z",
     "shell.execute_reply.started": "2025-03-16T14:58:32.397607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6df636525b74eb19aa7e1dc60b1463f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94934f60720648c483f1433b1a7e53af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7a411f0ddf423cba9849f31958a948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = Dataset.from_pandas(detector.all_data[language]['train'])\n",
    "dev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\n",
    "test_data = Dataset.from_pandas(detector.all_data[language]['test'])\n",
    "\n",
    "train_data = train_data.map(tokenize_text, batched=True)\n",
    "dev_data = dev_data.map(tokenize_text, batched=True)\n",
    "test_data = test_data.map(tokenize_text, batched=True)\n",
    "\n",
    "collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class_weights = detector.get_class_weights(detector.all_data[language]['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:32.907558Z",
     "iopub.status.busy": "2025-03-16T14:58:32.907285Z",
     "iopub.status.idle": "2025-03-16T14:58:32.936372Z",
     "shell.execute_reply": "2025-03-16T14:58:32.935730Z",
     "shell.execute_reply.started": "2025-03-16T14:58:32.907536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    #warmup_ratio=0.5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:32.937424Z",
     "iopub.status.busy": "2025-03-16T14:58:32.937141Z",
     "iopub.status.idle": "2025-03-16T14:58:33.442338Z",
     "shell.execute_reply": "2025-03-16T14:58:33.441525Z",
     "shell.execute_reply.started": "2025-03-16T14:58:32.937396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = dev_data,\n",
    "    data_collator = collator_fn,\n",
    "    compute_metrics = evaluate_metrics,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:33.443718Z",
     "iopub.status.busy": "2025-03-16T14:58:33.443497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/312 02:15 < 00:29, 1.87 it/s, Epoch 4.90/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro P</th>\n",
       "      <th>Macro R</th>\n",
       "      <th>Subj F1</th>\n",
       "      <th>Subj P</th>\n",
       "      <th>Subj R</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676271</td>\n",
       "      <td>0.638498</td>\n",
       "      <td>0.650187</td>\n",
       "      <td>0.644707</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.640693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492784</td>\n",
       "      <td>0.766216</td>\n",
       "      <td>0.768179</td>\n",
       "      <td>0.767736</td>\n",
       "      <td>0.764192</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.766234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512452</td>\n",
       "      <td>0.767482</td>\n",
       "      <td>0.779157</td>\n",
       "      <td>0.772016</td>\n",
       "      <td>0.752887</td>\n",
       "      <td>0.844560</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>0.768398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>0.792145</td>\n",
       "      <td>0.795240</td>\n",
       "      <td>0.794088</td>\n",
       "      <td>0.788546</td>\n",
       "      <td>0.836449</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.792208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decision threshold calibration on dev set\n",
    "best_thr = trainer.compute_best_threshold(dataset=dev_data)\n",
    "# Predictions on dev set (with best threshold on dev set)\n",
    "pred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n",
    "\n",
    "predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "\n",
    "# Save dev set predictions\n",
    "save_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n",
    "\n",
    "# Predictions on test set (with best threshold on dev set)\n",
    "pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n",
    "\n",
    "predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "\n",
    "# Save test set predictions\n",
    "save_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n",
    "\n",
    "acc = accuracy_score(labels, predictions)\n",
    "m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                            zero_division=0)\n",
    "p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                            zero_division=0)\n",
    "stats = {\n",
    "        'macro_F1': m_f1,\n",
    "        'macro_P': m_prec,\n",
    "        'macro_R': m_rec,\n",
    "        'SUBJ_F1': p_f1[0],\n",
    "        'SUBJ_P': p_prec[0],\n",
    "        'SUBJ_R': p_rec[0],\n",
    "        'accuracy': acc\n",
    "    }\n",
    "\n",
    "print(stats)\n",
    "results[f\"{language}-sentiment-thr\"] = stats\n",
    "\n",
    "cm = confusion_matrix(labels, predictions, normalize='all')\n",
    "ConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\n",
    "plt.title(f\"Confusion Matrix ({language})\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6742324,
     "isSourceIdPinned": false,
     "sourceId": 10881030,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
