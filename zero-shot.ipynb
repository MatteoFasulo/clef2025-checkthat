{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10881030,"sourceType":"datasetVersion","datasetId":6742324,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subjectivity in News Articles\n\n## Group:\n- Luca Babboni - luca.babboni2@studio.unibo.it\n- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n\n## Description\n\nThis notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n\nThis is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n\nThe task comprises three settings:\n\n* Monolingual: train and test on data in a given language\n* Multilingual: train and test on data comprising several languages\n* Zero-shot: train on several languages and test on unseen languages\n\ntraining data in five languages:\n* Arabic\n* Bulgarian\n* English\n* German\n* Italian\n\nThe official evaluation is macro-averaged F1 between the two classes.","metadata":{}},{"cell_type":"markdown","source":"# Installing dependencies\n\nThis notebook uses quantized models, and some additional libraries are required. If you are running this notebook on either Colab or Kaggle, please run the cell below once, then run the whole notebook normally.\n\n","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers[torch] trl accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:01:06.533515Z","iopub.execute_input":"2025-03-20T18:01:06.533808Z","iopub.status.idle":"2025-03-20T18:01:24.569363Z","shell.execute_reply.started":"2025-03-20T18:01:06.533778Z","shell.execute_reply":"2025-03-20T18:01:24.568171Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-20T18:01:29.510Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport itertools\nfrom pathlib import Path\n\nimport csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom datasets import Dataset\nfrom huggingface_hub import notebook_login\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding, \n    BitsAndBytesConfig,\n    PreTrainedModel,\n    DebertaV2Model, \n    DebertaV2Config, \n    pipeline\n)\nfrom transformers.trainer_utils import PredictionOutput\nfrom transformers.models.deberta.modeling_deberta import ContextPooler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:01:32.701216Z","iopub.execute_input":"2025-03-20T18:01:32.701642Z","iopub.status.idle":"2025-03-20T18:01:54.021930Z","shell.execute_reply.started":"2025-03-20T18:01:32.701596Z","shell.execute_reply":"2025-03-20T18:01:54.021004Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Setting the device","metadata":{}},{"cell_type":"code","source":"SEED = 42\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntqdm.pandas() # display tqdm on pandas apply functions\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:01:54.023141Z","iopub.execute_input":"2025-03-20T18:01:54.023694Z","iopub.status.idle":"2025-03-20T18:01:54.078562Z","shell.execute_reply.started":"2025-03-20T18:01:54.023669Z","shell.execute_reply":"2025-03-20T18:01:54.077593Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Setting Library Seeds\n\nThis step is necessary to guarantee reproducibility.\n\n","metadata":{}},{"cell_type":"code","source":"np.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:01:54.079923Z","iopub.execute_input":"2025-03-20T18:01:54.080235Z","iopub.status.idle":"2025-03-20T18:01:54.103324Z","shell.execute_reply.started":"2025-03-20T18:01:54.080204Z","shell.execute_reply":"2025-03-20T18:01:54.102672Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Setting the Data Folder  \n\nPlease modify this path with your dataset's local path. The `data` folder should follow the official challenge structure:\n```\ndata/\n|---- arabic/\n|--------- xxxx.tsv\n|---- bulgarian/\n|--------- xxxx.tsv\n|---- english/\n|--------- xxxx.tsv\n|---- german/\n|--------- xxxx.tsv\n|---- italian/\n|--------- xxxx.tsv\n```","metadata":{}},{"cell_type":"code","source":"data_folder = '/kaggle/input/clef2025-checkthat/data' # data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:02:53.983094Z","iopub.execute_input":"2025-03-20T18:02:53.983456Z","iopub.status.idle":"2025-03-20T18:02:53.987208Z","shell.execute_reply.started":"2025-03-20T18:02:53.983423Z","shell.execute_reply":"2025-03-20T18:02:53.986423Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Hugging face notebook login\n\nTo correctly download and use the hugging face models, a token key needs to be provided. Please refer to this [page](https://huggingface.co/docs/hub/security-tokens)","metadata":{}},{"cell_type":"code","source":"notebook_login()","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"66c4c9f7184d436dbb0827cd62a6235c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"execution_count":null},{"cell_type":"markdown","source":"# Subjectivity Class  \nThis class is used throughout the whole notebook as a utility toolbox to avoid code redundancy. When a method of this class is called for the first time, its behavior will be explained.  ","metadata":{}},{"cell_type":"code","source":"class Subjectivity:\n    \"\"\"\n    A class for handling multilingual subjectivity classification datasets.\n    \n    This class provides functionality to load, process, and prepare datasets for \n    subjective/objective text classification across multiple languages. It supports:\n    - Loading and organizing datasets from multiple languages\n    - Splitting data into train/dev/test sets\n    - Analyzing label distributions\n    - Loading pre-trained tokenizers and models\n    - Computing class weights for imbalanced datasets\n    \n    Attributes:\n        seed (int): Random seed for reproducibility\n        device (str): Computation device ('cuda' or 'cpu')\n        languages (list): List of available languages in the dataset\n        dataset (pd.DataFrame): Combined dataset with all languages and splits\n        train (pd.DataFrame): Training split of the dataset\n        dev (pd.DataFrame): Development split of the dataset\n        test (pd.DataFrame): Test split of the dataset\n        all_data (dict): Nested dictionary organizing data by language and split\n        tokenizer (AutoTokenizer, optional): Hugging Face tokenizer\n        model (AutoModelForSequenceClassification, optional): Classification model\n    \"\"\"\n    def __init__(self, data_folder: str = 'data', seed: int = 42, device: str = 'cuda'):\n        \"\"\"\n        Initialize the Subjectivity class.\n        \n        Args:\n            data_folder (str): Directory path containing the dataset files.\n            seed (int): Random seed for reproducibility.\n            device (str): Device to use for computations ('cuda' or 'cpu').\n        \"\"\"\n        self.seed = seed\n        self.device = device\n        self.languages = [language for language in os.listdir(data_folder)]\n\n        dataset = self.create_dataset(data_folder=data_folder)\n        self.dataset = dataset\n        \n        train, dev, test = self.get_splits(dataset, print_shapes=True)\n        self.train = train\n        self.dev = dev\n        self.test = test\n\n        self.all_data = self.get_per_lang_dataset()\n        \n\n    def create_dataset(self, data_folder: str = 'data'):\n        \"\"\"\n        Create a consolidated dataset from files in multiple languages.\n        \n        Args:\n            data_folder (str): Directory path containing subdirectories for each language.\n            \n        Returns:\n            pd.DataFrame: Combined dataset with columns for sentence_id, sentence, label, \n                          language, and split information.\n        \"\"\"\n        dataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n        for language in os.listdir(data_folder):\n            for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n                if '.tsv' in filename:\n                    abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n                    df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n                    if 'solved_conflict' in df.columns:\n                        df.drop(columns=['solved_conflict'], inplace=True)\n                    df['lang'] = language\n                    df['split'] = Path(filename).stem\n                    dataset = pd.concat([dataset, df], axis=0)\n        return dataset\n\n    def get_splits(self, dataset: pd.DataFrame, print_shapes: bool = True):\n        \"\"\"\n        Split the dataset into training, development, and test sets.\n        \n        Args:\n            dataset (pd.DataFrame): The combined dataset to split.\n            print_shapes (bool): Whether to print the shapes of the resulting splits.\n            \n        Returns:\n            tuple: A tuple containing three pandas DataFrames (train, dev, test).\n        \"\"\"\n        train = dataset[dataset['split'].str.contains('train')].copy()\n        dev = dataset[(dataset['split'].str.contains('dev')) & ~(dataset['split'].str.contains('dev_test'))].copy()\n        test = dataset[dataset['split'].str.contains('dev_test')].copy()\n\n        # encode the target variable to int (0: obj; 1: subj)\n        train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n        dev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n        test.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n\n        # cast to int\n        train['label'] = train['label'].astype(int)\n        dev['label'] = dev['label'].astype(int)\n        test['label'] = test['label'].astype(int)\n\n        if print_shapes:\n            print(f\"Train: {train.shape}\")\n            print(f\"Dev: {dev.shape}\")\n            print(f\"Test: {test.shape}\")\n            \n        return train, dev, test\n\n    def get_per_lang_dataset(self):\n        \"\"\"\n        Organize the dataset by language and split (train, dev, test).\n        \n        Returns:\n            dict: A nested dictionary with languages as the outer keys and\n                  split names ('train', 'dev', 'test') as inner keys.\n                  For example:\n                  {\n                      'english': {\n                          'train': pd.DataFrame,\n                          'dev': pd.DataFrame,\n                          'test': pd.DataFrame\n                      },\n                      ...\n                  }\n        \"\"\"\n        dataset_dict = {}\n        for language in self.languages:\n            dataset_dict[language] = {}\n            # get the train data\n            dataset_dict[language]['train'] = self.train[self.train['lang']==language].copy()\n            # get the dev data\n            dataset_dict[language]['dev'] = self.dev[self.dev['lang']==language].copy()\n            # get the test data\n            dataset_dict[language]['test'] = self.test[self.test['lang']==language].copy()\n        return dataset_dict\n\n    def print_label_distrib(self, dataset: pd.DataFrame):\n        \"\"\"\n        Print the normalized distribution of labels in the dataset.\n        \n        Args:\n            dataset (pd.DataFrame): The dataset containing a 'label' column.\n            \n        Returns:\n            None: Prints the percentage distribution of each label.\n        \"\"\"\n        print(dataset['label'].value_counts(normalize=True))\n\n    def get_tokenizer(self, model_card: str = \"microsoft/mdeberta-v3-base\"):\n        \"\"\"\n        Load a tokenizer from the Hugging Face model hub.\n        \n        Args:\n            model_card (str): Identifier for the pre-trained tokenizer to load.\n            \n        Returns:\n            AutoTokenizer: The loaded tokenizer.\n        \"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(model_card)\n        self.tokenizer = tokenizer\n        return tokenizer\n\n    def get_model(self, model_card: str = \"microsoft/mdeberta-v3-base\", *args, **kwargs):\n        \"\"\"\n        Load a pre-trained model from the Hugging Face model hub.\n        \n        Args:\n            model_card (str): Identifier for the pre-trained model to load.\n            *args: Variable length argument list to pass to the model constructor.\n            **kwargs: Arbitrary keyword arguments to pass to the model constructor.\n            \n        Returns:\n            AutoModelForSequenceClassification: The loaded model.\n        \"\"\"\n        model = AutoModelForSequenceClassification.from_pretrained(model_card, *args, **kwargs)\n        self.model = model\n        return model\n\n    def get_class_weights(self, dataset: pd.DataFrame):\n        \"\"\"\n        Compute class weights for imbalanced datasets.\n        \n        Args:\n            dataset (pd.DataFrame): Dataset containing a 'label' column.\n            \n        Returns:\n            numpy.ndarray: Array of class weights where the index corresponds to the class label.\n        \"\"\"\n        class_weights = compute_class_weight('balanced', classes=np.unique(dataset['label']), y=dataset['label'])\n        return class_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:00.726796Z","iopub.execute_input":"2025-03-20T18:03:00.727088Z","iopub.status.idle":"2025-03-20T18:03:00.741811Z","shell.execute_reply.started":"2025-03-20T18:03:00.727065Z","shell.execute_reply":"2025-03-20T18:03:00.741076Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Creating Our Detector Object  \n\nThe `__init__()` method of the `Subjectivity` class will load the dataset, set the device and the seeds, and automatically create the `test`, `dev`, and `train` splits from the datasets. It will also convert the `SUBJ` and `OBJ` labels to their corresponding numerical versions, so they are ready to be fed into a model.  \n","metadata":{}},{"cell_type":"code","source":"detector = Subjectivity(data_folder=data_folder, seed=SEED, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:03.226187Z","iopub.execute_input":"2025-03-20T18:03:03.226520Z","iopub.status.idle":"2025-03-20T18:03:03.566490Z","shell.execute_reply.started":"2025-03-20T18:03:03.226496Z","shell.execute_reply":"2025-03-20T18:03:03.565559Z"}},"outputs":[{"name":"stdout","text":"Train: (6418, 5)\nDev: (2401, 5)\nTest: (2332, 5)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"detector.print_label_distrib(detector.train)\ndetector.print_label_distrib(detector.dev)\ndetector.print_label_distrib(detector.test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:03.567755Z","iopub.execute_input":"2025-03-20T18:03:03.568065Z","iopub.status.idle":"2025-03-20T18:03:03.582960Z","shell.execute_reply.started":"2025-03-20T18:03:03.568034Z","shell.execute_reply":"2025-03-20T18:03:03.582262Z"}},"outputs":[{"name":"stdout","text":"label\n0    0.631349\n1    0.368651\nName: proportion, dtype: float64\nlabel\n0    0.612245\n1    0.387755\nName: proportion, dtype: float64\nlabel\n0    0.657376\n1    0.342624\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"detector.all_data['german']['train']['sentence'].str.len().describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:04.568661Z","iopub.execute_input":"2025-03-20T18:03:04.568952Z","iopub.status.idle":"2025-03-20T18:03:04.581036Z","shell.execute_reply.started":"2025-03-20T18:03:04.568930Z","shell.execute_reply":"2025-03-20T18:03:04.580275Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"count    800.000000\nmean     126.296250\nstd       67.334117\nmin       31.000000\n25%       80.000000\n50%      112.500000\n75%      161.000000\nmax      625.000000\nName: sentence, dtype: float64"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Here we create the `results` dictionary and the `predictions_dict` dictionary. They will be used to store all the model's output","metadata":{}},{"cell_type":"code","source":"results = {}\npredictions_dict = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:06.227014Z","iopub.execute_input":"2025-03-20T18:03:06.227385Z","iopub.status.idle":"2025-03-20T18:03:06.231086Z","shell.execute_reply.started":"2025-03-20T18:03:06.227333Z","shell.execute_reply":"2025-03-20T18:03:06.230277Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Custom Trainer  \n\nThis class extends the `Trainer` provided by Hugging Face. Since we needed to tweak some details in the training process, we opted to override some `Trainer` functions with custom ones.  \n","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    \"\"\"\n    Custom Trainer class extending Hugging Face's Trainer with additional functionality:\n    - Support for class weights to handle imbalanced datasets\n    - Custom loss computation with weighted cross-entropy\n    - Threshold optimization for binary classification\n    - Custom prediction with threshold application\n    \"\"\"\n    \n    def __init__(self, *args, class_weights=None, weights_dtype=torch.float32, **kwargs):\n        \"\"\"\n        Initialize the CustomTrainer.\n        \n        Args:\n            class_weights (array-like, optional): Weights for each class to handle class imbalance.\n            weights_dtype (torch.dtype): Data type for the class weights tensor.\n            *args, **kwargs: Arguments passed to the parent Trainer class.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        # Ensure label_weights is a tensor\n        if class_weights is not None:\n            self.class_weights = torch.tensor(class_weights, dtype=weights_dtype).to(self.args.device)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Compute the training loss with optional class weighting.\n        \n        Args:\n            model: The model to train\n            inputs: The inputs and targets of the model\n            return_outputs (bool): Whether to return the outputs along with the loss\n            num_items_in_batch: Not used but kept for compatibility\n            \n        Returns:\n            torch.Tensor or tuple: Loss value alone or with model outputs\n        \"\"\"\n        # Extract labels\n        labels = inputs.get(\"labels\")\n\n        # Forward pass\n        outputs = model(**inputs)\n\n        # Extract logits \n        logits = outputs.get('logits')\n\n        # Compute loss with class weights for imbalanced data handling\n        if self.class_weights is not None:\n            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n        else:\n            loss = F.cross_entropy(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss\n\n    def compute_best_threshold(self, dataset, ignore_keys=None, metric_key_prefix=\"test\"):\n        \"\"\"\n        Find the optimal classification threshold that maximizes macro F1 score.\n        \n        Args:\n            dataset: The dataset to use for threshold optimization\n            ignore_keys (list, optional): Keys to ignore in the model outputs\n            metric_key_prefix (str): Prefix for metric keys in the output\n            \n        Returns:\n            float: The optimal threshold value\n        \"\"\"\n        # Get raw predictions from parent class\n        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n\n        # Convert logits to probabilities using softmax (for binary classification)\n        logits = output.predictions\n        logits_tensor = torch.tensor(logits)\n        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n\n        # Calculate optimal threshold\n        labels = output.label_ids\n        thresholds = np.linspace(0.1, 0.9, 100) \n\n        best_threshold = 0.5  # Default threshold\n        best_f1 = 0\n\n        for threshold in thresholds:\n            predictions = (probabilities[:, 1] >= threshold).astype(int)\n            _, _, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\", zero_division=0)\n            \n            if f1 > best_f1:\n                best_f1 = f1\n                best_threshold = threshold\n\n        # Return the best threshold found\n        return best_threshold\n        \n    def predict(self, dataset, threshold: float = 0.5, ignore_keys=None, metric_key_prefix=\"test\"):\n        \"\"\"\n        Generate predictions with a custom threshold for binary classification.\n        \n        Args:\n            dataset: The dataset to generate predictions for\n            threshold (float): The classification threshold (default: 0.5)\n            ignore_keys (list, optional): Keys to ignore in model outputs\n            metric_key_prefix (str): Prefix for metric keys in the output\n            \n        Returns:\n            PredictionOutput: Object containing predictions, labels, and metrics\n        \"\"\"\n        # Get raw predictions from parent class\n        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n        \n        # Convert logits to probabilities using softmax (for binary classification)\n        logits = output.predictions\n        logits_tensor = torch.tensor(logits)\n        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n        \n        final_predictions = (probabilities[:, 1] >= threshold).astype(int)\n\n        # Update predictions in the output object\n        return PredictionOutput(\n            predictions=final_predictions,\n            label_ids=output.label_ids,\n            metrics=output.metrics\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:07.933835Z","iopub.execute_input":"2025-03-20T18:03:07.934128Z","iopub.status.idle":"2025-03-20T18:03:07.943595Z","shell.execute_reply.started":"2025-03-20T18:03:07.934105Z","shell.execute_reply":"2025-03-20T18:03:07.942707Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Text Tokenizer","metadata":{}},{"cell_type":"code","source":"def tokenize_text(texts):\n    \"\"\"\n    Tokenize text data using the current tokenizer.\n    \n    Args:\n        texts (dict): Dictionary containing text data with a 'sentence' field\n        \n    Returns:\n        dict: Dictionary with tokenized text features including input_ids, \n              attention_mask, and potentially token_type_ids\n    \"\"\"\n    return tokenizer(texts['sentence'], padding=True, truncation=True, max_length=256, return_tensors='pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:09.999099Z","iopub.execute_input":"2025-03-20T18:03:09.999457Z","iopub.status.idle":"2025-03-20T18:03:10.003449Z","shell.execute_reply.started":"2025-03-20T18:03:09.999425Z","shell.execute_reply":"2025-03-20T18:03:10.002617Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"def evaluate_metrics(eval_pred):\n    \"\"\"\n    Calculate evaluation metrics for subjectivity classification models.\n    \n    This function computes various performance metrics for classification results:\n    - Accuracy: Overall correctness of predictions\n    - Macro-averaged precision, recall, and F1: Averages across both classes with equal weight\n    - Class-specific metrics: Precision, recall, and F1 specifically for the subjective class\n    \n    Args:\n        eval_pred (tuple): Tuple containing (predictions, labels) where:\n            - predictions: Raw model outputs/logits with shape (n_samples, n_classes)\n            - labels: Ground truth labels with shape (n_samples,)\n            \n    Returns:\n        dict: Dictionary containing the following metrics:\n            - macro_F1: Macro-averaged F1 score across all classes\n            - macro_P: Macro-averaged precision across all classes\n            - macro_R: Macro-averaged recall across all classes\n            - SUBJ_F1: F1 score for the subjective class (label 1)\n            - SUBJ_P: Precision for the subjective class\n            - SUBJ_R: Recall for the subjective class\n            - accuracy: Overall accuracy\n    \"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, predictions)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                                zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                                zero_division=0)\n\n    return {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:10.303804Z","iopub.execute_input":"2025-03-20T18:03:10.304039Z","iopub.status.idle":"2025-03-20T18:03:10.309142Z","shell.execute_reply.started":"2025-03-20T18:03:10.304019Z","shell.execute_reply":"2025-03-20T18:03:10.308245Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Saving Predictions","metadata":{}},{"cell_type":"code","source":"def save_predictions(test_data, predictions, filename: str, save_dir: str = 'results'):\n    \"\"\"\n    Save model predictions to a TSV file with sentence IDs and predicted labels.\n    \n    Args:\n        test_data: Dataset containing the 'sentence_id' field to match with predictions\n        predictions: Array of binary predictions (0 for OBJ, 1 for SUBJ)\n        filename: Name of the output file (should end with .tsv)\n        save_dir: Directory to save the predictions file (default: 'results')\n        \n    Returns:\n        str: Full path to the saved predictions file\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    pred_df = pd.DataFrame()\n    pred_df['sentence_id'] = test_data['sentence_id']\n    pred_df['label'] = predictions\n    pred_df['label'] = pred_df['label'].apply(lambda x: 'OBJ' if x == 0 else 'SUBJ')\n\n    predictions_filepath = os.path.join(save_dir, filename)\n    pred_df.to_csv(predictions_filepath, index=False, sep='\\t')\n\n    print(f\"Saved predictions into file:\", predictions_filepath)\n    return predictions_filepath","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:11.511014Z","iopub.execute_input":"2025-03-20T18:03:11.511306Z","iopub.status.idle":"2025-03-20T18:03:11.516159Z","shell.execute_reply.started":"2025-03-20T18:03:11.511283Z","shell.execute_reply":"2025-03-20T18:03:11.515401Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Custom Model","metadata":{}},{"cell_type":"code","source":"class CustomModel(PreTrainedModel):\n    config_class = DebertaV2Config\n\n    def __init__(self, config, sentiment_dim=3, num_labels=2, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n        self.deberta = DebertaV2Model(config)\n        self.pooler = ContextPooler(config)\n        output_dim = self.pooler.output_dim\n        self.dropout = nn.Dropout(0.1)\n\n        self.classifier = nn.Linear(output_dim + sentiment_dim, num_labels)\n\n    def forward(self, input_ids, positive, neutral, negative, attention_mask=None, labels=None):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        encoder_layer = outputs[0]\n        pooled_output = self.pooler(encoder_layer)\n        \n        # Sentiment features as a single tensor\n        sentiment_features = torch.stack((positive, neutral, negative), dim=1)  # Shape: (batch_size, 3)\n        \n        # Combine CLS embedding with sentiment features\n        combined_features = torch.cat((pooled_output, sentiment_features), dim=1)\n        \n        # Classification head\n        logits = self.classifier(self.dropout(combined_features))\n        \n        return {'logits': logits}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:11.942600Z","iopub.execute_input":"2025-03-20T18:03:11.942879Z","iopub.status.idle":"2025-03-20T18:03:11.948850Z","shell.execute_reply.started":"2025-03-20T18:03:11.942857Z","shell.execute_reply":"2025-03-20T18:03:11.948029Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Sentiment Pipeline for Data Augmentation","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", top_k=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:13.858582Z","iopub.execute_input":"2025-03-20T18:03:13.858890Z","iopub.status.idle":"2025-03-20T18:03:22.865184Z","shell.execute_reply.started":"2025-03-20T18:03:13.858869Z","shell.execute_reply":"2025-03-20T18:03:22.863875Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c9500b8eeb40be9c0f3be4a02a0aa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)3e7660c69a21a57cfb477892f95f539e3e171196:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9550580150248a4a1c254e6e34fa27a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84050d15ab6c432aba430a382c2b847c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abeba84b64bb4ad4b94be078b43a8405"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a5830352ad462fb1eed1c764f7a59c"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def extract_sentiment(text):\n    sentiments = pipe(text)[0]\n    return {k:v for k,v in [(list(sentiment.values())[0], list(sentiment.values())[1]) for sentiment in sentiments]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:03:22.866904Z","iopub.execute_input":"2025-03-20T18:03:22.867156Z","iopub.status.idle":"2025-03-20T18:03:22.872023Z","shell.execute_reply.started":"2025-03-20T18:03:22.867134Z","shell.execute_reply":"2025-03-20T18:03:22.870870Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Zero-shot Inference\n\nThe third and last subtask of the challenge is Zero-Shot classification. This task is defined as training the model on several languages and testing it on previously unseen languages. We tackled this task by splitting the total five languages into groups of 3 and 2, using three languages for training and two languages for testing.","metadata":{}},{"cell_type":"markdown","source":"## Utility functions","metadata":{}},{"cell_type":"markdown","source":"### Data preparation function","metadata":{}},{"cell_type":"code","source":"def zero_shot_prepare_data(train_languages: list,\n                          train: pd.DataFrame,\n                          dev: pd.DataFrame,\n                          test: pd.DataFrame):\n    \"\"\"\n    Prepare data for zero-shot learning by splitting datasets based on specified languages.\n    \n    This function filters the training set to include only the specified languages,\n    while filtering the development and test sets to exclude those languages.\n    \n    Parameters:\n    -----------\n    train_languages : list\n        List of language codes to include in the training set.\n    train : pd.DataFrame\n        The complete training dataset containing a 'lang' column.\n    dev : pd.DataFrame\n        The complete development dataset containing a 'lang' column.\n    test : pd.DataFrame\n        The complete test dataset containing a 'lang' column.\n    \n    Returns:\n    --------\n    tuple\n        A tuple containing three DataFrames:\n        - train_set: DataFrame containing only rows with languages in train_languages\n        - dev_set: DataFrame containing only rows with languages not in train_languages\n        - test_set: DataFrame containing only rows with languages not in train_languages\n    \n    \"\"\"\n    \n    train_set = train[train[\"lang\"].isin(train_languages)].copy()\n    dev_set = dev[~dev[\"lang\"].isin(train_languages)].copy()\n    test_set = test[~test[\"lang\"].isin(train_languages)].copy()\n\n    return train_set, dev_set, test_set","metadata":{"execution":{"iopub.status.busy":"2025-03-20T18:03:22.873985Z","iopub.execute_input":"2025-03-20T18:03:22.874248Z","iopub.status.idle":"2025-03-20T18:03:22.891379Z","shell.execute_reply.started":"2025-03-20T18:03:22.874226Z","shell.execute_reply":"2025-03-20T18:03:22.890188Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### Model name generator function","metadata":{}},{"cell_type":"code","source":"def generate_name(names: tuple) -> str:\n    \"\"\"\n    Generate a name by concatenating the first two capitalized letters of each string in the given tuple.\n\n    Args:\n        names (tuple): A tuple of strings.\n\n    Returns:\n        str: The generated name consisting of the first two capitalized letters of each string.\n    \"\"\"\n    generated_name = \"\"\n    for x in names:\n        generated_name = generated_name + x.capitalize()[:2]\n    return generated_name\n","metadata":{"execution":{"iopub.status.busy":"2025-03-20T18:03:22.892650Z","iopub.execute_input":"2025-03-20T18:03:22.892950Z","iopub.status.idle":"2025-03-20T18:03:22.912870Z","shell.execute_reply.started":"2025-03-20T18:03:22.892916Z","shell.execute_reply":"2025-03-20T18:03:22.911931Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"## Custom Loop to Test Triplets\n\nTo test our models (mDeBERTa and mDeBERTa + sentiment) with all possible dataset combinations, we employed a `for` loop to iterate through them. Each time a new model is trained, the previous one is deleted, ensuring that no trained models persist—only the results are saved. This approach prevents GPU and storage memory overflow.\n","metadata":{}},{"cell_type":"markdown","source":"## Model settings","metadata":{}},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:52.400261Z","iopub.execute_input":"2025-03-20T18:14:52.400633Z","iopub.status.idle":"2025-03-20T18:14:54.447017Z","shell.execute_reply.started":"2025-03-20T18:14:52.400606Z","shell.execute_reply":"2025-03-20T18:14:54.446034Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"epochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\nlabel_smoothing = 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:54.448461Z","iopub.execute_input":"2025-03-20T18:14:54.448731Z","iopub.status.idle":"2025-03-20T18:14:54.452757Z","shell.execute_reply.started":"2025-03-20T18:14:54.448702Z","shell.execute_reply":"2025-03-20T18:14:54.451847Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"config = DebertaV2Config.from_pretrained(\n    model_card,\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:54.454496Z","iopub.execute_input":"2025-03-20T18:14:54.454953Z","iopub.status.idle":"2025-03-20T18:14:54.552858Z","shell.execute_reply.started":"2025-03-20T18:14:54.454929Z","shell.execute_reply":"2025-03-20T18:14:54.551981Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"training_args = TrainingArguments(\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T18:14:54.553794Z","iopub.execute_input":"2025-03-20T18:14:54.554017Z","iopub.status.idle":"2025-03-20T18:14:54.583263Z","shell.execute_reply.started":"2025-03-20T18:14:54.553997Z","shell.execute_reply":"2025-03-20T18:14:54.582539Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"for i, group in enumerate(itertools.combinations(detector.languages, 3)):\n    print(f\"Training group {i} - {group}\")\n\n    # Deleting old model and cache\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    if 'model' in locals() or 'model' in globals():\n        del model\n        print(\"Model deleted!\")\n    gc.collect()\n\n    # Preparing data and generation model name\n    group_name = generate_name(group)\n    zs_train, zs_dev, zs_test = zero_shot_prepare_data(group, detector.train, detector.dev, detector.test)\n    language = group_name+'-NoSentiment'\n    \n    train_data = Dataset.from_pandas(zs_train)\n    dev_data = Dataset.from_pandas(zs_dev)\n    test_data = Dataset.from_pandas(zs_test)\n    \n    train_data = train_data.map(tokenize_text, batched=True)\n    dev_data = dev_data.map(tokenize_text, batched=True)\n    test_data = test_data.map(tokenize_text, batched=True)\n    \n    class_weights = detector.get_class_weights(zs_train)\n\n    # Creating the model\n    model = detector.get_model(\n        model_card=model_card, \n        num_labels=2, \n        id2label={0: 'OBJ', 1: 'SUBJ'}, \n        label2id={'OBJ': 0, 'SUBJ': 1},\n        output_attentions = False,\n        output_hidden_states = False\n    )\n    \n    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=dev_data,\n        data_collator=collator_fn,\n        compute_metrics=evaluate_metrics,\n        class_weights=class_weights,\n    )\n\n    # Training the model\n    trainer.train()\n\n    # Computing predicitons with best threshold\n    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n    predictions, labels = pred_info.predictions, pred_info.label_ids\n    \n    acc = accuracy_score(labels, predictions)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                                zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                                zero_division=0)\n    stats = {\n            'macro_F1': m_f1,\n            'macro_P': m_prec,\n            'macro_R': m_rec,\n            'SUBJ_F1': p_f1[0],\n            'SUBJ_P': p_prec[0],\n            'SUBJ_R': p_rec[0],\n            'accuracy': acc\n        }\n    \n    results[language] = stats\n\n    # Deleting the model\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    if 'model' in locals() or 'model' in globals():\n        del model\n        print(\"Model deleted!\")\n    gc.collect()\n\n    language = group_name+'-Sentiment'\n    \n    model = CustomModel(\n        config=config, \n        model_name=model_card, \n        num_labels=2, \n        sentiment_dim=3\n    )\n\n    model.deberta = DebertaV2Model.from_pretrained(model_card, config=config)\n\n    # Data augmentation with sentiment value\n    zs_train[['positive', 'neutral', 'negative']] = zs_train.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n    zs_dev[['positive', 'neutral', 'negative']] = zs_dev.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n    zs_test[['positive', 'neutral', 'negative']] = zs_test.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n\n    train_data = Dataset.from_pandas(zs_train)\n    dev_data = Dataset.from_pandas(zs_dev)\n    test_data = Dataset.from_pandas(zs_test)\n\n    train_data = train_data.map(tokenize_text, batched=True)\n    dev_data = dev_data.map(tokenize_text, batched=True)\n    test_data = test_data.map(tokenize_text, batched=True)\n\n    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    trainer = CustomTrainer(\n        model = model,\n        args = training_args,\n        train_dataset = train_data,\n        eval_dataset = dev_data,\n        data_collator = collator_fn,\n        compute_metrics = evaluate_metrics,\n        class_weights=class_weights,\n    )\n\n    # Training \n    trainer.train()\n\n    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n    predictions, labels = pred_info.predictions, pred_info.label_ids\n    \n    acc = accuracy_score(labels, predictions)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                                zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                                zero_division=0)\n    stats = {\n            'macro_F1': m_f1,\n            'macro_P': m_prec,\n            'macro_R': m_rec,\n            'SUBJ_F1': p_f1[0],\n            'SUBJ_P': p_prec[0],\n            'SUBJ_R': p_rec[0],\n            'accuracy': acc\n        }\n    \n    results[language] = stats\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2025-03-20T18:28:48.063273Z","iopub.execute_input":"2025-03-20T18:28:48.063646Z","iopub.status.idle":"2025-03-20T18:37:41.157840Z","shell.execute_reply.started":"2025-03-20T18:28:48.063616Z","shell.execute_reply":"2025-03-20T18:37:41.157096Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training group 0 - ('english', 'italian')\nModel deleted!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2443/2443 [00:19<00:00, 124.45it/s]\n100%|██████████| 1272/1272 [00:10<00:00, 125.72it/s]\n100%|██████████| 1335/1335 [00:10<00:00, 125.84it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2443 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0056416e9384f3c962b25b6a1d373aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1272 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd41607dd1034683b41ca8a8de10a419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1335 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"751f6417c4a24132afbf5aaf0cf964bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [918/918 07:46, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Macro F1</th>\n      <th>Macro P</th>\n      <th>Macro R</th>\n      <th>Subj F1</th>\n      <th>Subj P</th>\n      <th>Subj R</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.729381</td>\n      <td>0.666765</td>\n      <td>0.693356</td>\n      <td>0.663755</td>\n      <td>0.563284</td>\n      <td>0.680441</td>\n      <td>0.480545</td>\n      <td>0.698899</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.079605</td>\n      <td>0.592030</td>\n      <td>0.713598</td>\n      <td>0.610741</td>\n      <td>0.408571</td>\n      <td>0.768817</td>\n      <td>0.278210</td>\n      <td>0.674528</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.926564</td>\n      <td>0.672689</td>\n      <td>0.696953</td>\n      <td>0.669245</td>\n      <td>0.573363</td>\n      <td>0.682796</td>\n      <td>0.494163</td>\n      <td>0.702830</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.464200</td>\n      <td>1.132665</td>\n      <td>0.644669</td>\n      <td>0.710647</td>\n      <td>0.647059</td>\n      <td>0.508906</td>\n      <td>0.735294</td>\n      <td>0.389105</td>\n      <td>0.696541</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.464200</td>\n      <td>1.246540</td>\n      <td>0.639970</td>\n      <td>0.707305</td>\n      <td>0.643168</td>\n      <td>0.501279</td>\n      <td>0.731343</td>\n      <td>0.381323</td>\n      <td>0.693396</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.464200</td>\n      <td>1.232105</td>\n      <td>0.651777</td>\n      <td>0.706147</td>\n      <td>0.652136</td>\n      <td>0.524752</td>\n      <td>0.721088</td>\n      <td>0.412451</td>\n      <td>0.698113</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"## Printing the results","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(results).T.sort_values(by='macro_F1', ascending=False).round(4)","metadata":{"execution":{"iopub.status.busy":"2025-03-20T18:37:41.158936Z","iopub.execute_input":"2025-03-20T18:37:41.159191Z","iopub.status.idle":"2025-03-20T18:37:41.171848Z","shell.execute_reply.started":"2025-03-20T18:37:41.159168Z","shell.execute_reply":"2025-03-20T18:37:41.171182Z"},"trusted":true},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"                  macro_F1  macro_P  macro_R  SUBJ_F1  SUBJ_P  SUBJ_R  \\\nArBuGe-Sentiment    0.7461   0.7630   0.7342   0.6134  0.6697  0.5659   \nEnIt-Sentiment      0.6121   0.6215   0.6111   0.5087  0.5685  0.4603   \n\n                  accuracy  \nArBuGe-Sentiment    0.8154  \nEnIt-Sentiment      0.6397  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>macro_F1</th>\n      <th>macro_P</th>\n      <th>macro_R</th>\n      <th>SUBJ_F1</th>\n      <th>SUBJ_P</th>\n      <th>SUBJ_R</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ArBuGe-Sentiment</th>\n      <td>0.7461</td>\n      <td>0.7630</td>\n      <td>0.7342</td>\n      <td>0.6134</td>\n      <td>0.6697</td>\n      <td>0.5659</td>\n      <td>0.8154</td>\n    </tr>\n    <tr>\n      <th>EnIt-Sentiment</th>\n      <td>0.6121</td>\n      <td>0.6215</td>\n      <td>0.6111</td>\n      <td>0.5087</td>\n      <td>0.5685</td>\n      <td>0.4603</td>\n      <td>0.6397</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Zero-Shot Without Arabic Language\n\nBased on previous results, we observed that when Arabic is included in the test set, the metrics drop significantly. To assess whether overall model performance improves, we conducted zero-shot inference excluding Arabic from our datasets. \n\nFurthermore, we aimed to examine the relationship between an increasing SUBJ F1 score and the testing language. To do so, we enforced two specific language groupings:  \n- **Training:** Italian - English  \n- **Testing:** Bulgarian - German  \n\nand vice versa\n","metadata":{}},{"cell_type":"code","source":"results = {}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"execution":{"iopub.execute_input":"2025-03-16T14:58:03.126861Z","iopub.status.busy":"2025-03-16T14:58:03.126516Z","iopub.status.idle":"2025-03-16T14:58:03.694451Z","shell.execute_reply":"2025-03-16T14:58:03.693460Z","shell.execute_reply.started":"2025-03-16T14:58:03.126832Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model deleted!\n"]},{"data":{"text/plain":["10705"]},"execution_count":129,"metadata":{},"output_type":"execute_result"}],"execution_count":129},{"cell_type":"markdown","source":"## Custom loop","metadata":{}},{"cell_type":"code","source":"for i, group in enumerate([(\"english\", \"italian\"),(\"bulgarian\", \"german\")]):\n    print(f\"Training group {i} - {group}\")\n\n    # Deleting old model and cache\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    if 'model' in locals() or 'model' in globals():\n        del model\n        print(\"Model deleted!\")\n    gc.collect()\n\n    # Preparing data and generation model name\n    group_name = generate_name(group)\n    zs_train, zs_dev, zs_test = zero_shot_prepare_data(group, detector.train, detector.dev, detector.test)\n    language = group_name+'-NoSentiment'\n    \n    train_data = Dataset.from_pandas(zs_train)\n    dev_data = Dataset.from_pandas(zs_dev)\n    test_data = Dataset.from_pandas(zs_test)\n    \n    train_data = train_data.map(tokenize_text, batched=True)\n    dev_data = dev_data.map(tokenize_text, batched=True)\n    test_data = test_data.map(tokenize_text, batched=True)\n    \n    class_weights = detector.get_class_weights(zs_train)\n\n    # Creating the model\n    model = detector.get_model(\n        model_card=model_card, \n        num_labels=2, \n        id2label={0: 'OBJ', 1: 'SUBJ'}, \n        label2id={'OBJ': 0, 'SUBJ': 1},\n        output_attentions = False,\n        output_hidden_states = False\n    )\n    \n    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    trainer = CustomTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=dev_data,\n        data_collator=collator_fn,\n        compute_metrics=evaluate_metrics,\n        class_weights=class_weights,\n    )\n\n    # Training the model\n    trainer.train()\n\n    # Computing predicitons with best threshold\n    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n    predictions, labels = pred_info.predictions, pred_info.label_ids\n    \n    acc = accuracy_score(labels, predictions)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                                zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                                zero_division=0)\n    stats = {\n            'macro_F1': m_f1,\n            'macro_P': m_prec,\n            'macro_R': m_rec,\n            'SUBJ_F1': p_f1[0],\n            'SUBJ_P': p_prec[0],\n            'SUBJ_R': p_rec[0],\n            'accuracy': acc\n        }\n    \n    results[language] = stats\n\n    # Deleting the model\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    if 'model' in locals() or 'model' in globals():\n        del model\n        print(\"Model deleted!\")\n    gc.collect()\n\n    language = group_name+'-Sentiment'\n    \n    model = CustomModel(\n        model_name=model_card, \n        num_labels=2, \n        sentiment_dim=3\n    )\n\n    # Data augmentation with sentiment value\n    zs_train[['positive', 'neutral', 'negative']] = zs_train.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n    zs_dev[['positive', 'neutral', 'negative']] = zs_dev.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n    zs_test[['positive', 'neutral', 'negative']] = zs_test.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n\n    train_data = Dataset.from_pandas(zs_train)\n    dev_data = Dataset.from_pandas(zs_dev)\n    test_data = Dataset.from_pandas(zs_test)\n\n    train_data = train_data.map(tokenize_text, batched=True)\n    dev_data = dev_data.map(tokenize_text, batched=True)\n    test_data = test_data.map(tokenize_text, batched=True)\n\n    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    trainer = CustomTrainer(\n        model = model,\n        args = training_args,\n        train_dataset = train_data,\n        eval_dataset = dev_data,\n        data_collator = collator_fn,\n        compute_metrics = evaluate_metrics,\n        class_weights=class_weights,\n    )\n\n    # Training \n    trainer.train()\n\n    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n    predictions, labels = pred_info.predictions, pred_info.label_ids\n    \n    acc = accuracy_score(labels, predictions)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                                zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                                zero_division=0)\n    stats = {\n            'macro_F1': m_f1,\n            'macro_P': m_prec,\n            'macro_R': m_rec,\n            'SUBJ_F1': p_f1[0],\n            'SUBJ_P': p_prec[0],\n            'SUBJ_R': p_rec[0],\n            'accuracy': acc\n        }\n    \n    results[language] = stats\n    \n    ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Printing the results","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(results).T.sort_values(by='macro_F1', ascending=False).round(4)","metadata":{},"outputs":[],"execution_count":null}]}