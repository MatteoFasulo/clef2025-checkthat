{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity in News Articles\n",
    "\n",
    "## Group:\n",
    "- Luca Babboni - luca.babboni2@studio.unibo.it\n",
    "- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n",
    "- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n",
    "\n",
    "This is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n",
    "\n",
    "The task comprises three settings:\n",
    "\n",
    "* Monolingual: train and test on data in a given language\n",
    "* Multilingual: train and test on data comprising several languages\n",
    "* Zero-shot: train on several languages and test on unseen languages\n",
    "\n",
    "training data in five languages:\n",
    "* Arabic\n",
    "* Bulgarian\n",
    "* English\n",
    "* German\n",
    "* Italian\n",
    "\n",
    "The official evaluation is macro-averaged F1 between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing dependencies\n",
    "\n",
    "This notebook uses quantized models, and some additional libraries are required. If you are running this notebook on either Colab or Kaggle, please run the cell below once, then run the whole notebook normally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:18:24.982197Z",
     "iopub.status.busy": "2025-03-16T14:18:24.981893Z",
     "iopub.status.idle": "2025-03-16T14:18:43.176453Z",
     "shell.execute_reply": "2025-03-16T14:18:43.175317Z",
     "shell.execute_reply.started": "2025-03-16T14:18:24.982174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers[torch] bitsandbytes trl peft sacremoses ctranslate2 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-16T14:18:44.370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:06.233335Z",
     "iopub.status.busy": "2025-03-16T14:19:06.233069Z",
     "iopub.status.idle": "2025-03-16T14:19:30.823676Z",
     "shell.execute_reply": "2025-03-16T14:19:30.822545Z",
     "shell.execute_reply.started": "2025-03-16T14:19:06.233315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorWithPadding, \n",
    "    PreTrainedModel,\n",
    "    DebertaV2Model, \n",
    "    DebertaV2Config, \n",
    "    pipeline\n",
    ")\n",
    "from transformers.trainer_utils import PredictionOutput\n",
    "from transformers.models.deberta.modeling_deberta import ContextPooler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:30.825474Z",
     "iopub.status.busy": "2025-03-16T14:19:30.824883Z",
     "iopub.status.idle": "2025-03-16T14:19:30.890180Z",
     "shell.execute_reply": "2025-03-16T14:19:30.889311Z",
     "shell.execute_reply.started": "2025-03-16T14:19:30.825443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tqdm.pandas() # display tqdm on pandas apply functions\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Library Seeds\n",
    "\n",
    "This step is necessary to guarantee reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:30.892724Z",
     "iopub.status.busy": "2025-03-16T14:19:30.892334Z",
     "iopub.status.idle": "2025-03-16T14:19:30.926587Z",
     "shell.execute_reply": "2025-03-16T14:19:30.925645Z",
     "shell.execute_reply.started": "2025-03-16T14:19:30.892689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity Class  \n",
    "This class is used throughout the whole notebook as a utility toolbox to avoid code redundancy. When a method of this class is called for the first time, its behavior will be explained.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:30.928091Z",
     "iopub.status.busy": "2025-03-16T14:19:30.927835Z",
     "iopub.status.idle": "2025-03-16T14:19:30.950847Z",
     "shell.execute_reply": "2025-03-16T14:19:30.950003Z",
     "shell.execute_reply.started": "2025-03-16T14:19:30.928070Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Subjectivity:\n",
    "    def __init__(self, data_folder: str = 'data', seed: int = 42, device: str = 'cuda'):\n",
    "        self.seed = seed\n",
    "        self.device = device\n",
    "        self.languages = [language for language in os.listdir(data_folder)]\n",
    "\n",
    "        dataset = self.create_dataset(data_folder=data_folder)\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        train, dev, test = self.get_splits(dataset, print_shapes=True)\n",
    "        self.train = train\n",
    "        self.dev = dev\n",
    "        self.test = test\n",
    "\n",
    "        self.all_data = self.get_per_lang_dataset()\n",
    "        \n",
    "\n",
    "    def create_dataset(self, data_folder: str = 'data'):\n",
    "        dataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n",
    "        for language in os.listdir(data_folder):\n",
    "            for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n",
    "                if '.tsv' in filename:\n",
    "                    abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n",
    "                    df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "                    if 'solved_conflict' in df.columns:\n",
    "                        df.drop(columns=['solved_conflict'], inplace=True)\n",
    "                    df['lang'] = language\n",
    "                    df['split'] = Path(filename).stem\n",
    "                    dataset = pd.concat([dataset, df], axis=0)\n",
    "        return dataset\n",
    "\n",
    "    def get_splits(self, dataset: pd.DataFrame, print_shapes: bool = True):\n",
    "        train = dataset[dataset['split'].str.contains('train')].copy()\n",
    "        dev = dataset[(dataset['split'].str.contains('dev')) & ~(dataset['split'].str.contains('dev_test'))].copy()\n",
    "        test = dataset[dataset['split'].str.contains('dev_test')].copy()\n",
    "\n",
    "        # encode the target variable to int (0: obj; 1: subj)\n",
    "        train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "        dev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "        test.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "\n",
    "        # cast to int\n",
    "        train['label'] = train['label'].astype(int)\n",
    "        dev['label'] = dev['label'].astype(int)\n",
    "        test['label'] = test['label'].astype(int)\n",
    "\n",
    "        if print_shapes:\n",
    "            print(f\"Train: {train.shape}\")\n",
    "            print(f\"Dev: {dev.shape}\")\n",
    "            print(f\"Test: {test.shape}\")\n",
    "            \n",
    "        return train, dev, test\n",
    "\n",
    "    def get_per_lang_dataset(self):\n",
    "        \"\"\"\n",
    "        dataset_dict = {\n",
    "            'english': {\n",
    "                'train': ...\n",
    "                'dev': ...\n",
    "                'test': ...\n",
    "            },\n",
    "        }\n",
    "        \"\"\"\n",
    "        dataset_dict = {}\n",
    "        for language in self.languages:\n",
    "            dataset_dict[language] = {}\n",
    "            # get the train data\n",
    "            dataset_dict[language]['train'] = self.train[self.train['lang']==language].copy()\n",
    "            # get the dev data\n",
    "            dataset_dict[language]['dev'] = self.dev[self.dev['lang']==language].copy()\n",
    "            # get the test data\n",
    "            dataset_dict[language]['test'] = self.test[self.test['lang']==language].copy()\n",
    "        return dataset_dict\n",
    "\n",
    "    def print_label_distrib(self, dataset: pd.DataFrame):\n",
    "        print(dataset['label'].value_counts(normalize=True))\n",
    "\n",
    "    def get_baseline_model(self, model_name: str = \"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "        vect = SentenceTransformer(model_name)\n",
    "        self.vect = vect\n",
    "        return vect\n",
    "\n",
    "    def train_baseline_model(self, vect, train_data: pd.DataFrame, test_data: pd.DataFrame, solver: str = 'saga'):\n",
    "        model = LogisticRegression(class_weight=\"balanced\", solver=solver, random_state=self.seed)\n",
    "        model.fit(X=vect.encode(train_data['sentence'].values), y=train_data['label'].values)\n",
    "        predictions = model.predict(X=vect.encode(test_data['sentence'].values)).tolist()\n",
    "\n",
    "        # eval performances\n",
    "        perfs = self.evaluate_model(gold_values=test_data['label'].values, predicted_values=predictions)\n",
    "\n",
    "        return perfs\n",
    "\n",
    "    def get_tokenizer(self, model_card: str = \"microsoft/mdeberta-v3-base\"):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
    "        self.tokenizer = tokenizer\n",
    "        return tokenizer\n",
    "\n",
    "    def get_model(self, model_card: str = \"microsoft/mdeberta-v3-base\", *args, **kwargs):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_card, *args, **kwargs)\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def get_class_weights(self, dataset: pd.DataFrame):\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(dataset['label']), y=dataset['label'])\n",
    "        return class_weights\n",
    "\n",
    "    def evaluate_model(self, gold_values, predicted_values):\n",
    "        acc = accuracy_score(gold_values, predicted_values)\n",
    "        m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(gold_values, predicted_values, average=\"macro\",\n",
    "                                                                   zero_division=0)\n",
    "        p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(gold_values, predicted_values, labels=[1],\n",
    "                                                                   zero_division=0)\n",
    "    \n",
    "        return {\n",
    "            'macro_F1': m_f1,\n",
    "            'macro_P': m_prec,\n",
    "            'macro_R': m_rec,\n",
    "            'SUBJ_F1': p_f1[0],\n",
    "            'SUBJ_P': p_prec[0],\n",
    "            'SUBJ_R': p_rec[0],\n",
    "            'accuracy': acc\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Data Folder  \n",
    "\n",
    "Please modify this path with your dataset's local path. The `data` folder should follow the official challenge structure:\n",
    "```\n",
    "data/\n",
    "|---- arabic/\n",
    "|--------- xxxx.tsv\n",
    "|---- bulgarian/\n",
    "|--------- xxxx.tsv\n",
    "|---- english/\n",
    "|--------- xxxx.tsv\n",
    "|---- german/\n",
    "|--------- xxxx.tsv\n",
    "|---- italian/\n",
    "|--------- xxxx.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:30.952042Z",
     "iopub.status.busy": "2025-03-16T14:19:30.951776Z",
     "iopub.status.idle": "2025-03-16T14:19:30.970000Z",
     "shell.execute_reply": "2025-03-16T14:19:30.969077Z",
     "shell.execute_reply.started": "2025-03-16T14:19:30.952021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_folder = '/kaggle/input/clef2025-checkthat/data' # data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Our Detector Object  \n",
    "\n",
    "The `__init__()` method of the `Subjectivity` class will load the dataset, set the device and the seeds, and automatically create the `test`, `dev`, and `train` splits from the datasets. It will also convert the `SUBJ` and `OBJ` labels to their corresponding numerical versions, so they are ready to be fed into a model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:30.971293Z",
     "iopub.status.busy": "2025-03-16T14:19:30.971033Z",
     "iopub.status.idle": "2025-03-16T14:19:31.281795Z",
     "shell.execute_reply": "2025-03-16T14:19:31.280954Z",
     "shell.execute_reply.started": "2025-03-16T14:19:30.971271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6418, 5)\n",
      "Dev: (2401, 5)\n",
      "Test: (2332, 5)\n"
     ]
    }
   ],
   "source": [
    "detector = Subjectivity(data_folder=data_folder, seed=SEED, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:31.282954Z",
     "iopub.status.busy": "2025-03-16T14:19:31.282621Z",
     "iopub.status.idle": "2025-03-16T14:19:31.297071Z",
     "shell.execute_reply": "2025-03-16T14:19:31.296182Z",
     "shell.execute_reply.started": "2025-03-16T14:19:31.282925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.631349\n",
      "1    0.368651\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.612245\n",
      "1    0.387755\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.657376\n",
      "1    0.342624\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "detector.print_label_distrib(detector.train)\n",
    "detector.print_label_distrib(detector.dev)\n",
    "detector.print_label_distrib(detector.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:31.299546Z",
     "iopub.status.busy": "2025-03-16T14:19:31.299263Z",
     "iopub.status.idle": "2025-03-16T14:19:31.320619Z",
     "shell.execute_reply": "2025-03-16T14:19:31.319819Z",
     "shell.execute_reply.started": "2025-03-16T14:19:31.299521Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    800.000000\n",
       "mean     126.296250\n",
       "std       67.334117\n",
       "min       31.000000\n",
       "25%       80.000000\n",
       "50%      112.500000\n",
       "75%      161.000000\n",
       "max      625.000000\n",
       "Name: sentence, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.all_data['german']['train']['sentence'].str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face notebook login\n",
    "\n",
    "To correctly download and use the hugging face models, a token key needs to be provided. Please refer to this [page](https://huggingface.co/docs/hub/security-tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:31.321828Z",
     "iopub.status.busy": "2025-03-16T14:19:31.321549Z",
     "iopub.status.idle": "2025-03-16T14:19:31.356453Z",
     "shell.execute_reply": "2025-03-16T14:19:31.355524Z",
     "shell.execute_reply.started": "2025-03-16T14:19:31.321803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c4c9f7184d436dbb0827cd62a6235c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the `results` dictionary and the `predictions_dict` dictionary. They will be used to store all the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:38.525400Z",
     "iopub.status.busy": "2025-03-16T14:19:38.525082Z",
     "iopub.status.idle": "2025-03-16T14:19:38.529242Z",
     "shell.execute_reply": "2025-03-16T14:19:38.528247Z",
     "shell.execute_reply.started": "2025-03-16T14:19:38.525376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "predictions_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Trainer  \n",
    "\n",
    "This class extends the `Trainer` provided by Hugging Face. Since we needed to tweak some details in the training process, we opted to override some `Trainer` functions with custom ones.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:38.785855Z",
     "iopub.status.busy": "2025-03-16T14:19:38.785563Z",
     "iopub.status.idle": "2025-03-16T14:19:38.794582Z",
     "shell.execute_reply": "2025-03-16T14:19:38.793636Z",
     "shell.execute_reply.started": "2025-03-16T14:19:38.785833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3700 (with some minor changes removing unused parts)\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, weights_dtype=torch.float32, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=weights_dtype).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Extract labels\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits \n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_best_threshold(self, dataset, ignore_keys=None, metric_key_prefix=\"test\"):\n",
    "        # Get raw predictions from parent class\n",
    "        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n",
    "\n",
    "        # Convert logits to probabilities using softmax (for binary classification)\n",
    "        logits = output.predictions\n",
    "        logits_tensor = torch.tensor(logits)\n",
    "        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n",
    "\n",
    "        # Calculate optimal threshold\n",
    "        labels = output.label_ids\n",
    "        thresholds = np.linspace(0.1, 0.9, 100) \n",
    "\n",
    "        best_threshold = 0.5  # Default threshold\n",
    "        best_f1 = 0\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            predictions = (probabilities[:, 1] >= threshold).astype(int)\n",
    "            _, _, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\", zero_division=0)\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "\n",
    "        # Return the best threshold found\n",
    "        return best_threshold\n",
    "        \n",
    "\n",
    "    def predict(self, dataset, threshold: float = 0.5, ignore_keys=None, metric_key_prefix=\"test\"):\n",
    "        # Get raw predictions from parent class\n",
    "        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # Convert logits to probabilities using softmax (for binary classification)\n",
    "        logits = output.predictions\n",
    "        logits_tensor = torch.tensor(logits)\n",
    "        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n",
    "        \n",
    "        final_predictions = (probabilities[:, 1] >= threshold).astype(int)\n",
    "\n",
    "        # Update predictions in the output object\n",
    "        return PredictionOutput(\n",
    "            predictions=final_predictions,\n",
    "            label_ids=output.label_ids,\n",
    "            metrics=output.metrics\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:40.109558Z",
     "iopub.status.busy": "2025-03-16T14:19:40.109279Z",
     "iopub.status.idle": "2025-03-16T14:19:40.113423Z",
     "shell.execute_reply": "2025-03-16T14:19:40.112510Z",
     "shell.execute_reply.started": "2025-03-16T14:19:40.109539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(texts):\n",
    "    return tokenizer(texts['sentence'], padding=True, truncation=True, max_length=256, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:41.435156Z",
     "iopub.status.busy": "2025-03-16T14:19:41.434780Z",
     "iopub.status.idle": "2025-03-16T14:19:41.440347Z",
     "shell.execute_reply": "2025-03-16T14:19:41.439434Z",
     "shell.execute_reply.started": "2025-03-16T14:19:41.435128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                                zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                                zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'macro_F1': m_f1,\n",
    "        'macro_P': m_prec,\n",
    "        'macro_R': m_rec,\n",
    "        'SUBJ_F1': p_f1[0],\n",
    "        'SUBJ_P': p_prec[0],\n",
    "        'SUBJ_R': p_rec[0],\n",
    "        'accuracy': acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:19:41.603663Z",
     "iopub.status.busy": "2025-03-16T14:19:41.603363Z",
     "iopub.status.idle": "2025-03-16T14:19:41.608739Z",
     "shell.execute_reply": "2025-03-16T14:19:41.607676Z",
     "shell.execute_reply.started": "2025-03-16T14:19:41.603637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(test_data, predictions, filename: str, save_dir: str = 'results'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df['sentence_id'] = test_data['sentence_id']\n",
    "    pred_df['label'] = predictions\n",
    "    pred_df['label'] = pred_df['label'].apply(lambda x: 'OBJ' if x == 0 else 'SUBJ')\n",
    "\n",
    "    predictions_filepath = os.path.join(save_dir, filename)\n",
    "    pred_df.to_csv(predictions_filepath, index=False, sep='\\t')\n",
    "\n",
    "    print(f\"Saved predictions into file:\", predictions_filepath)\n",
    "    return predictions_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(PreTrainedModel):\n",
    "    config_class = DebertaV2Config\n",
    "\n",
    "    def __init__(self, config, sentiment_dim=3, num_labels=2, *args, **kwargs):\n",
    "        super().__init__(config, *args, **kwargs)\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        self.pooler = ContextPooler(config)\n",
    "        output_dim = self.pooler.output_dim\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.classifier = nn.Linear(output_dim + sentiment_dim, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, positive, neutral, negative, attention_mask=None, labels=None):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        encoder_layer = outputs[0]\n",
    "        pooled_output = self.pooler(encoder_layer)\n",
    "        \n",
    "        # Sentiment features as a single tensor\n",
    "        sentiment_features = torch.stack((positive, neutral, negative), dim=1)  # Shape: (batch_size, 3)\n",
    "        \n",
    "        # Combine CLS embedding with sentiment features\n",
    "        combined_features = torch.cat((pooled_output, sentiment_features), dim=1)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classifier(self.dropout(combined_features))\n",
    "        \n",
    "        return {'logits': logits}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility function\n",
    "\n",
    "This function can be then integrated in the Subjectivity class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:06:16.183313Z",
     "iopub.status.busy": "2025-03-13T10:06:16.182974Z",
     "iopub.status.idle": "2025-03-13T10:06:16.187690Z",
     "shell.execute_reply": "2025-03-13T10:06:16.186800Z",
     "shell.execute_reply.started": "2025-03-13T10:06:16.183283Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zero_shot_prepare_data(train_languages : list,\n",
    "                           train : pd.DataFrame,\n",
    "                           dev : pd.DataFrame,\n",
    "                           test : pd.DataFrame):\n",
    "\n",
    "    \n",
    "    train_set = train[train[\"lang\"].isin(train_languages)].copy()\n",
    "    dev_set = dev[~dev[\"lang\"].isin(train_languages)].copy()\n",
    "    test_set = test[~test[\"lang\"].isin(train_languages)].copy()\n",
    "\n",
    "    return train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:06:16.436192Z",
     "iopub.status.busy": "2025-03-13T10:06:16.435880Z",
     "iopub.status.idle": "2025-03-13T10:06:16.440296Z",
     "shell.execute_reply": "2025-03-13T10:06:16.439435Z",
     "shell.execute_reply.started": "2025-03-13T10:06:16.436169Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_name(names : tuple):\n",
    "    generated_name = \"\"\n",
    "    for x in names:\n",
    "        generated_name = generated_name + x.capitalize()[:2]\n",
    "    return generated_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loop to test triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:26:40.833435Z",
     "iopub.status.busy": "2025-03-13T10:26:40.833092Z",
     "iopub.status.idle": "2025-03-13T10:26:40.837773Z",
     "shell.execute_reply": "2025-03-13T10:26:40.836905Z",
     "shell.execute_reply.started": "2025-03-13T10:26:40.833407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_sentiment(text):\n",
    "    sentiments = pipe(text)[0]\n",
    "    return {k:v for k,v in [(list(sentiment.values())[0], list(sentiment.values())[1]) for sentiment in sentiments]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:00:03.746456Z",
     "iopub.status.busy": "2025-03-13T11:00:03.746112Z",
     "iopub.status.idle": "2025-03-13T11:17:10.123905Z",
     "shell.execute_reply": "2025-03-13T11:17:10.123148Z",
     "shell.execute_reply.started": "2025-03-13T11:00:03.746429Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING 0 - ('english', 'italian')\n",
      "Model deleted!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5096cebddbbb42d49c99cd0d136175c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a295088f83164c6496b7e3ab2b2539a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ccb46ff1124a5a818c829943590d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 07:50, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro P</th>\n",
       "      <th>Macro R</th>\n",
       "      <th>Subj F1</th>\n",
       "      <th>Subj P</th>\n",
       "      <th>Subj R</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657371</td>\n",
       "      <td>0.668724</td>\n",
       "      <td>0.668303</td>\n",
       "      <td>0.669217</td>\n",
       "      <td>0.607522</td>\n",
       "      <td>0.602294</td>\n",
       "      <td>0.612840</td>\n",
       "      <td>0.680031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.828612</td>\n",
       "      <td>0.645487</td>\n",
       "      <td>0.692242</td>\n",
       "      <td>0.645853</td>\n",
       "      <td>0.519512</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.414397</td>\n",
       "      <td>0.690252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.014649</td>\n",
       "      <td>0.661566</td>\n",
       "      <td>0.695910</td>\n",
       "      <td>0.659338</td>\n",
       "      <td>0.550351</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.457198</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>1.211402</td>\n",
       "      <td>0.649742</td>\n",
       "      <td>0.693915</td>\n",
       "      <td>0.649397</td>\n",
       "      <td>0.527207</td>\n",
       "      <td>0.696486</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.692610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>1.423838</td>\n",
       "      <td>0.649075</td>\n",
       "      <td>0.700674</td>\n",
       "      <td>0.649497</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.711409</td>\n",
       "      <td>0.412451</td>\n",
       "      <td>0.694969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>1.465843</td>\n",
       "      <td>0.647287</td>\n",
       "      <td>0.694394</td>\n",
       "      <td>0.647485</td>\n",
       "      <td>0.521951</td>\n",
       "      <td>0.699346</td>\n",
       "      <td>0.416342</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2443/2443 [00:20<00:00, 121.21it/s]\n",
      "100%|██████████| 1272/1272 [00:10<00:00, 120.28it/s]\n",
      "100%|██████████| 1335/1335 [00:11<00:00, 119.28it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7e4f46a9bf43b5b7747e378701cfbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d5000d7f1648ae8473934035cc1eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1272 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25701476bbb403380a22e7d27feed21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1335 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [918/918 07:43, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro P</th>\n",
       "      <th>Macro R</th>\n",
       "      <th>Subj F1</th>\n",
       "      <th>Subj P</th>\n",
       "      <th>Subj R</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.725917</td>\n",
       "      <td>0.612220</td>\n",
       "      <td>0.629209</td>\n",
       "      <td>0.631277</td>\n",
       "      <td>0.603379</td>\n",
       "      <td>0.514403</td>\n",
       "      <td>0.729572</td>\n",
       "      <td>0.612421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.972189</td>\n",
       "      <td>0.634223</td>\n",
       "      <td>0.718251</td>\n",
       "      <td>0.640070</td>\n",
       "      <td>0.486129</td>\n",
       "      <td>0.757202</td>\n",
       "      <td>0.357977</td>\n",
       "      <td>0.694182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.955200</td>\n",
       "      <td>0.685101</td>\n",
       "      <td>0.702052</td>\n",
       "      <td>0.681131</td>\n",
       "      <td>0.596721</td>\n",
       "      <td>0.680798</td>\n",
       "      <td>0.531128</td>\n",
       "      <td>0.709906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.252779</td>\n",
       "      <td>0.653370</td>\n",
       "      <td>0.706423</td>\n",
       "      <td>0.653422</td>\n",
       "      <td>0.527744</td>\n",
       "      <td>0.720539</td>\n",
       "      <td>0.416342</td>\n",
       "      <td>0.698899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.394758</td>\n",
       "      <td>0.648759</td>\n",
       "      <td>0.707636</td>\n",
       "      <td>0.649911</td>\n",
       "      <td>0.518148</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>0.402724</td>\n",
       "      <td>0.697327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.504100</td>\n",
       "      <td>1.417408</td>\n",
       "      <td>0.653806</td>\n",
       "      <td>0.701681</td>\n",
       "      <td>0.653355</td>\n",
       "      <td>0.531060</td>\n",
       "      <td>0.710098</td>\n",
       "      <td>0.424125</td>\n",
       "      <td>0.697327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_card = \"microsoft/mdeberta-v3-base\"\n",
    "tokenizer = detector.get_tokenizer(model_card=model_card)\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 16\n",
    "lr = 1e-5\n",
    "weight_decay = 0.0\n",
    "label_smoothing = 0.0\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    #warmup_ratio=0.5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "for i, group in enumerate(itertools.combinations(detector.languages, 3)):\n",
    "    print(f\"TESTING {i} - {group}\")\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        del model\n",
    "        print(\"Model deleted!\")\n",
    "    gc.collect()\n",
    "    \n",
    "    group_name = generate_name(group)\n",
    "\n",
    "    zs_train, zs_dev, zs_test = zero_shot_prepare_data(group, detector.train, detector.dev, detector.test)\n",
    "\n",
    "    language = group_name+'-NoSentiment'\n",
    "\n",
    "\n",
    "    \n",
    "    train_data = Dataset.from_pandas(zs_train)\n",
    "    dev_data = Dataset.from_pandas(zs_dev)\n",
    "    test_data = Dataset.from_pandas(zs_test)\n",
    "    \n",
    "    train_data = train_data.map(tokenize_text, batched=True)\n",
    "    dev_data = dev_data.map(tokenize_text, batched=True)\n",
    "    test_data = test_data.map(tokenize_text, batched=True)\n",
    "    \n",
    "    class_weights = detector.get_class_weights(zs_train)\n",
    "\n",
    "    model = detector.get_model(\n",
    "        model_card=model_card, \n",
    "        num_labels=2, \n",
    "        id2label={0: 'OBJ', 1: 'SUBJ'}, \n",
    "        label2id={'OBJ': 0, 'SUBJ': 1},\n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    \n",
    "    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=dev_data,\n",
    "        data_collator=collator_fn,\n",
    "        compute_metrics=evaluate_metrics,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n",
    "    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n",
    "    predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                                zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                                zero_division=0)\n",
    "    stats = {\n",
    "            'macro_F1': m_f1,\n",
    "            'macro_P': m_prec,\n",
    "            'macro_R': m_rec,\n",
    "            'SUBJ_F1': p_f1[0],\n",
    "            'SUBJ_P': p_prec[0],\n",
    "            'SUBJ_R': p_rec[0],\n",
    "            'accuracy': acc\n",
    "        }\n",
    "    \n",
    "    results[language] = stats\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    if 'model' in locals() or 'model' in globals():\n",
    "        del model\n",
    "        print(\"Model deleted!\")\n",
    "    gc.collect()\n",
    "\n",
    "    language = group_name+'-Sentiment'\n",
    "    \n",
    "    model = CustomModel(\n",
    "        model_name=model_card, \n",
    "        num_labels=2, \n",
    "        sentiment_dim=3\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    zs_train[['positive', 'neutral', 'negative']] = zs_train.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "    zs_dev[['positive', 'neutral', 'negative']] = zs_dev.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "    zs_test[['positive', 'neutral', 'negative']] = zs_test.progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "\n",
    "    train_data = Dataset.from_pandas(zs_train)\n",
    "    dev_data = Dataset.from_pandas(zs_dev)\n",
    "    test_data = Dataset.from_pandas(zs_test)\n",
    "\n",
    "    train_data = train_data.map(tokenize_text, batched=True)\n",
    "    dev_data = dev_data.map(tokenize_text, batched=True)\n",
    "    test_data = test_data.map(tokenize_text, batched=True)\n",
    "\n",
    "    collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_data,\n",
    "        eval_dataset = dev_data,\n",
    "        data_collator = collator_fn,\n",
    "        compute_metrics = evaluate_metrics,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    best_thr = trainer.compute_best_threshold(dataset=dev_data)\n",
    "    pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n",
    "    predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "    \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                                zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                                zero_division=0)\n",
    "    stats = {\n",
    "            'macro_F1': m_f1,\n",
    "            'macro_P': m_prec,\n",
    "            'macro_R': m_rec,\n",
    "            'SUBJ_F1': p_f1[0],\n",
    "            'SUBJ_P': p_prec[0],\n",
    "            'SUBJ_R': p_rec[0],\n",
    "            'accuracy': acc\n",
    "        }\n",
    "    \n",
    "    results[language] = stats\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:22:07.192577Z",
     "iopub.status.busy": "2025-03-13T11:22:07.192225Z",
     "iopub.status.idle": "2025-03-13T11:22:07.204695Z",
     "shell.execute_reply": "2025-03-13T11:22:07.204011Z",
     "shell.execute_reply.started": "2025-03-13T11:22:07.192552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro_F1</th>\n",
       "      <th>macro_P</th>\n",
       "      <th>macro_R</th>\n",
       "      <th>SUBJ_F1</th>\n",
       "      <th>SUBJ_P</th>\n",
       "      <th>SUBJ_R</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EnIt-NoSentiment</th>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>0.6135</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.5661</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.6397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnIt-Sentiment</th>\n",
       "      <td>0.6022</td>\n",
       "      <td>0.6074</td>\n",
       "      <td>0.6012</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.5451</td>\n",
       "      <td>0.4695</td>\n",
       "      <td>0.6262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  macro_F1  macro_P  macro_R  SUBJ_F1  SUBJ_P  SUBJ_R  \\\n",
       "EnIt-NoSentiment    0.6147   0.6219   0.6135   0.5166  0.5661  0.4750   \n",
       "EnIt-Sentiment      0.6022   0.6074   0.6012   0.5045  0.5451  0.4695   \n",
       "\n",
       "                  accuracy  \n",
       "EnIt-NoSentiment    0.6397  \n",
       "EnIt-Sentiment      0.6262  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results).T.sort_values(by='macro_F1', ascending=False).round(4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame(results).to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot without Arabic language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:03.126861Z",
     "iopub.status.busy": "2025-03-16T14:58:03.126516Z",
     "iopub.status.idle": "2025-03-16T14:58:03.694451Z",
     "shell.execute_reply": "2025-03-16T14:58:03.693460Z",
     "shell.execute_reply.started": "2025-03-16T14:58:03.126832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model deleted!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10705"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "if 'model' in locals() or 'model' in globals():\n",
    "    del model\n",
    "    print(\"Model deleted!\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:06.390436Z",
     "iopub.status.busy": "2025-03-16T14:58:06.390150Z",
     "iopub.status.idle": "2025-03-16T14:58:06.394358Z",
     "shell.execute_reply": "2025-03-16T14:58:06.393549Z",
     "shell.execute_reply.started": "2025-03-16T14:58:06.390415Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "language = 'english'\n",
    "\n",
    "epochs = 6\n",
    "batch_size = 16\n",
    "lr = 1e-5\n",
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:07.521759Z",
     "iopub.status.busy": "2025-03-16T14:58:07.521455Z",
     "iopub.status.idle": "2025-03-16T14:58:14.107408Z",
     "shell.execute_reply": "2025-03-16T14:58:14.106685Z",
     "shell.execute_reply.started": "2025-03-16T14:58:07.521737Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_card = \"microsoft/mdeberta-v3-base\"\n",
    "\n",
    "tokenizer = detector.get_tokenizer(model_card=model_card)\n",
    "\n",
    "# Load the config\n",
    "config = DebertaV2Config.from_pretrained(\n",
    "    model_card,\n",
    "    num_labels=2,\n",
    "    id2label={0: 'OBJ', 1: 'SUBJ'},\n",
    "    label2id={'OBJ': 0, 'SUBJ': 1},\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# Initialize the custom model\n",
    "model = CustomModel(config=config, sentiment_dim=3, num_labels=2)\n",
    "\n",
    "# Load pretrained weights from the original DeBERTa model\n",
    "model.deberta = DebertaV2Model.from_pretrained(model_card, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:14.108862Z",
     "iopub.status.busy": "2025-03-16T14:58:14.108613Z",
     "iopub.status.idle": "2025-03-16T14:58:32.396070Z",
     "shell.execute_reply": "2025-03-16T14:58:32.395313Z",
     "shell.execute_reply.started": "2025-03-16T14:58:14.108841Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "100%|██████████| 830/830 [00:06<00:00, 123.08it/s]\n",
      "100%|██████████| 462/462 [00:03<00:00, 121.79it/s]\n",
      "100%|██████████| 484/484 [00:04<00:00, 120.10it/s]\n"
     ]
    }
   ],
   "source": [
    "detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "detector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\n",
    "detector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:32.397628Z",
     "iopub.status.busy": "2025-03-16T14:58:32.397400Z",
     "iopub.status.idle": "2025-03-16T14:58:32.906176Z",
     "shell.execute_reply": "2025-03-16T14:58:32.905432Z",
     "shell.execute_reply.started": "2025-03-16T14:58:32.397607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6df636525b74eb19aa7e1dc60b1463f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94934f60720648c483f1433b1a7e53af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7a411f0ddf423cba9849f31958a948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = Dataset.from_pandas(detector.all_data[language]['train'])\n",
    "dev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\n",
    "test_data = Dataset.from_pandas(detector.all_data[language]['test'])\n",
    "\n",
    "train_data = train_data.map(tokenize_text, batched=True)\n",
    "dev_data = dev_data.map(tokenize_text, batched=True)\n",
    "test_data = test_data.map(tokenize_text, batched=True)\n",
    "\n",
    "collator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "class_weights = detector.get_class_weights(detector.all_data[language]['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:32.907558Z",
     "iopub.status.busy": "2025-03-16T14:58:32.907285Z",
     "iopub.status.idle": "2025-03-16T14:58:32.936372Z",
     "shell.execute_reply": "2025-03-16T14:58:32.935730Z",
     "shell.execute_reply.started": "2025-03-16T14:58:32.907536Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    #warmup_ratio=0.5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:32.937424Z",
     "iopub.status.busy": "2025-03-16T14:58:32.937141Z",
     "iopub.status.idle": "2025-03-16T14:58:33.442338Z",
     "shell.execute_reply": "2025-03-16T14:58:33.441525Z",
     "shell.execute_reply.started": "2025-03-16T14:58:32.937396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = dev_data,\n",
    "    data_collator = collator_fn,\n",
    "    compute_metrics = evaluate_metrics,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T14:58:33.443718Z",
     "iopub.status.busy": "2025-03-16T14:58:33.443497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/312 02:15 < 00:29, 1.87 it/s, Epoch 4.90/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro P</th>\n",
       "      <th>Macro R</th>\n",
       "      <th>Subj F1</th>\n",
       "      <th>Subj P</th>\n",
       "      <th>Subj R</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.676271</td>\n",
       "      <td>0.638498</td>\n",
       "      <td>0.650187</td>\n",
       "      <td>0.644707</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.640693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.492784</td>\n",
       "      <td>0.766216</td>\n",
       "      <td>0.768179</td>\n",
       "      <td>0.767736</td>\n",
       "      <td>0.764192</td>\n",
       "      <td>0.802752</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.766234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.512452</td>\n",
       "      <td>0.767482</td>\n",
       "      <td>0.779157</td>\n",
       "      <td>0.772016</td>\n",
       "      <td>0.752887</td>\n",
       "      <td>0.844560</td>\n",
       "      <td>0.679167</td>\n",
       "      <td>0.768398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.506866</td>\n",
       "      <td>0.792145</td>\n",
       "      <td>0.795240</td>\n",
       "      <td>0.794088</td>\n",
       "      <td>0.788546</td>\n",
       "      <td>0.836449</td>\n",
       "      <td>0.745833</td>\n",
       "      <td>0.792208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Decision threshold calibration on dev set\n",
    "best_thr = trainer.compute_best_threshold(dataset=dev_data)\n",
    "# Predictions on dev set (with best threshold on dev set)\n",
    "pred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n",
    "\n",
    "predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "\n",
    "# Save dev set predictions\n",
    "save_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n",
    "\n",
    "# Predictions on test set (with best threshold on dev set)\n",
    "pred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n",
    "\n",
    "predictions, labels = pred_info.predictions, pred_info.label_ids\n",
    "\n",
    "# Save test set predictions\n",
    "save_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n",
    "\n",
    "acc = accuracy_score(labels, predictions)\n",
    "m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n",
    "                                                            zero_division=0)\n",
    "p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n",
    "                                                            zero_division=0)\n",
    "stats = {\n",
    "        'macro_F1': m_f1,\n",
    "        'macro_P': m_prec,\n",
    "        'macro_R': m_rec,\n",
    "        'SUBJ_F1': p_f1[0],\n",
    "        'SUBJ_P': p_prec[0],\n",
    "        'SUBJ_R': p_rec[0],\n",
    "        'accuracy': acc\n",
    "    }\n",
    "\n",
    "print(stats)\n",
    "results[f\"{language}-sentiment-thr\"] = stats\n",
    "\n",
    "cm = confusion_matrix(labels, predictions, normalize='all')\n",
    "ConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\n",
    "plt.title(f\"Confusion Matrix ({language})\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6742324,
     "isSourceIdPinned": false,
     "sourceId": 10881030,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
