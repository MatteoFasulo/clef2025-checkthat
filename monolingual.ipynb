{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":11646389,"datasetId":6742324,"databundleVersionId":12114762}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subjectivity in News Articles\n\n## Group:\n- Luca Babboni - luca.babboni2@studio.unibo.it\n- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n\n## Description\n\nThis notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n\nThis is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n\nThe task comprises three settings:\n\n* Monolingual: train and test on data in a given language\n* Multilingual: train and test on data comprising several languages\n* Zero-shot: train on several languages and test on unseen languages\n\ntraining data in five languages:\n* Arabic\n* Bulgarian\n* English\n* German\n* Italian\n\nThe official evaluation is macro-averaged F1 between the two classes.","metadata":{}},{"cell_type":"markdown","source":"# Installing dependencies\n\nThis notebook uses quantized models, and some additional libraries are required. If you are running this notebook on either Colab or Kaggle, please run the cell below once, then run the whole notebook normally.\n\n","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers[torch] bitsandbytes trl peft sacremoses ctranslate2 accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom pathlib import Path\n\nimport csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_recall_fscore_support\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\nfrom datasets import Dataset\nfrom huggingface_hub import notebook_login\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments, \n    DataCollatorWithPadding, \n    BitsAndBytesConfig,\n    PreTrainedModel,\n    ModernBertPreTrainedModel,\n    DebertaV2Model, \n    ModernBertModel,\n    DebertaV2Config,\n    ModernBertConfig,\n    pipeline\n)\nfrom transformers.trainer_utils import PredictionOutput\nfrom transformers.models.deberta.modeling_deberta import ContextPooler\nfrom transformers.models.modernbert.modeling_modernbert import ModernBertPredictionHead\nfrom transformers.activations import GELUActivation","metadata":{"execution":{"iopub.status.busy":"2025-05-02T15:44:07.086232Z","iopub.execute_input":"2025-05-02T15:44:07.086839Z","iopub.status.idle":"2025-05-02T15:44:07.093198Z","shell.execute_reply.started":"2025-05-02T15:44:07.086820Z","shell.execute_reply":"2025-05-02T15:44:07.092335Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Setting the device","metadata":{}},{"cell_type":"code","source":"SEED = 42\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntqdm.pandas() # display tqdm on pandas apply functions\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2025-05-02T15:44:07.094093Z","iopub.execute_input":"2025-05-02T15:44:07.094437Z","iopub.status.idle":"2025-05-02T15:44:07.128563Z","shell.execute_reply.started":"2025-05-02T15:44:07.094408Z","shell.execute_reply":"2025-05-02T15:44:07.127875Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Setting Library Seeds\n\nThis step is necessary to guarantee reproducibility.\n\n","metadata":{}},{"cell_type":"code","source":"np.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"execution":{"iopub.status.busy":"2025-05-02T15:44:07.131554Z","iopub.execute_input":"2025-05-02T15:44:07.131844Z","iopub.status.idle":"2025-05-02T15:44:07.144976Z","shell.execute_reply.started":"2025-05-02T15:44:07.131820Z","shell.execute_reply":"2025-05-02T15:44:07.144249Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Setting the Data Folder  \n\nPlease modify this path with your dataset's local path. The `data` folder should follow the official challenge structure:\n```\ndata/\n|---- arabic/\n|--------- xxxx.tsv\n|---- bulgarian/\n|--------- xxxx.tsv\n|---- english/\n|--------- xxxx.tsv\n|---- german/\n|--------- xxxx.tsv\n|---- italian/\n|--------- xxxx.tsv\n```","metadata":{}},{"cell_type":"code","source":"data_folder = '/kaggle/input/data' # data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:44:07.146849Z","iopub.execute_input":"2025-05-02T15:44:07.147163Z","iopub.status.idle":"2025-05-02T15:44:07.157882Z","shell.execute_reply.started":"2025-05-02T15:44:07.147145Z","shell.execute_reply":"2025-05-02T15:44:07.157168Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Hugging face notebook login\n\nTo correctly download and use the hugging face models, a token key needs to be provided. Please refer to this [page](https://huggingface.co/docs/hub/security-tokens)","metadata":{}},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T15:44:07.158640Z","iopub.execute_input":"2025-05-02T15:44:07.158924Z","iopub.status.idle":"2025-05-02T15:44:07.185043Z","shell.execute_reply.started":"2025-05-02T15:44:07.158901Z","shell.execute_reply":"2025-05-02T15:44:07.184177Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3069637d3be429ab7efae17b1825a30"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Subjectivity Class  \nThis class is used throughout the whole notebook as a utility toolbox to avoid code redundancy. When a method of this class is called for the first time, its behavior will be explained.  ","metadata":{}},{"cell_type":"code","source":"class Subjectivity:\n    \"\"\"\n    A class for handling multilingual subjectivity classification datasets.\n    \n    This class provides functionality to load, process, and prepare datasets for \n    subjective/objective text classification across multiple languages. It supports:\n    - Loading and organizing datasets from multiple languages\n    - Splitting data into train/dev/test sets\n    - Analyzing label distributions\n    - Loading pre-trained tokenizers and models\n    - Computing class weights for imbalanced datasets\n    \n    Attributes:\n        seed (int): Random seed for reproducibility\n        device (str): Computation device ('cuda' or 'cpu')\n        languages (list): List of available languages in the dataset\n        dataset (pd.DataFrame): Combined dataset with all languages and splits\n        train (pd.DataFrame): Training split of the dataset\n        dev (pd.DataFrame): Development split of the dataset\n        test (pd.DataFrame): Test split of the dataset\n        all_data (dict): Nested dictionary organizing data by language and split\n        tokenizer (AutoTokenizer, optional): Hugging Face tokenizer\n        model (AutoModelForSequenceClassification, optional): Classification model\n    \"\"\"\n    def __init__(self, data_folder: str = 'data', seed: int = 42, device: str = 'cuda'):\n        \"\"\"\n        Initialize the Subjectivity class.\n        \n        Args:\n            data_folder (str): Directory path containing the dataset files.\n            seed (int): Random seed for reproducibility.\n            device (str): Device to use for computations ('cuda' or 'cpu').\n        \"\"\"\n        self.seed = seed\n        self.device = device\n        self.languages = [language for language in os.listdir(data_folder)]\n\n        dataset = self.create_dataset(data_folder=data_folder)\n        self.dataset = dataset\n        \n        train, dev, test = self.get_splits(dataset, print_shapes=True)\n        self.train = train\n        self.dev = dev\n        self.test = test\n\n        self.all_data = self.get_per_lang_dataset()\n        \n\n    def create_dataset(self, data_folder: str = 'data'):\n        \"\"\"\n        Create a consolidated dataset from files in multiple languages.\n        \n        Args:\n            data_folder (str): Directory path containing subdirectories for each language.\n            \n        Returns:\n            pd.DataFrame: Combined dataset with columns for sentence_id, sentence, label, \n                          language, and split information.\n        \"\"\"\n        dataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n        for language in os.listdir(data_folder):\n            for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n                if '.tsv' in filename:\n                    abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n                    df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n                    if 'solved_conflict' in df.columns:\n                        df.drop(columns=['solved_conflict'], inplace=True)\n                    df['lang'] = language\n                    df['split'] = Path(filename).stem\n                    dataset = pd.concat([dataset, df], axis=0)\n        return dataset\n\n    def get_splits(self, dataset: pd.DataFrame, print_shapes: bool = True):\n        \"\"\"\n        Split the dataset into training, development, and test sets.\n        \n        Args:\n            dataset (pd.DataFrame): The combined dataset to split.\n            print_shapes (bool): Whether to print the shapes of the resulting splits.\n            \n        Returns:\n            tuple: A tuple containing three pandas DataFrames (train, dev, test).\n        \"\"\"\n        train = dataset[dataset['split'].str.contains('train')].copy()\n        dev = dataset[dataset['split'].str.contains('dev_test')].copy()\n        test = dataset[dataset['split'].str.contains('unlabeled')].copy()\n\n        # encode the target variable to int (0: obj; 1: subj)\n        train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n        dev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n        test.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n\n        # cast to int\n        train['label'] = train['label'].astype(int)\n        dev['label'] = dev['label'].astype(int)\n        test['label'] = test['label'].astype(int)\n\n        if print_shapes:\n            print(f\"Train: {train.shape}\")\n            print(f\"Dev: {dev.shape}\")\n            print(f\"Test: {test.shape}\")\n            \n        return train, dev, test\n\n    def get_per_lang_dataset(self):\n        \"\"\"\n        Organize the dataset by language and split (train, dev, test).\n        \n        Returns:\n            dict: A nested dictionary with languages as the outer keys and\n                  split names ('train', 'dev', 'test') as inner keys.\n                  For example:\n                  {\n                      'english': {\n                          'train': pd.DataFrame,\n                          'dev': pd.DataFrame,\n                          'test': pd.DataFrame\n                      },\n                      ...\n                  }\n        \"\"\"\n        dataset_dict = {}\n        for language in self.languages:\n            dataset_dict[language] = {}\n            # get the train data\n            dataset_dict[language]['train'] = self.train[self.train['lang']==language].copy()\n            # get the dev data\n            dataset_dict[language]['dev'] = self.dev[self.dev['lang']==language].copy()\n            # get the test data\n            dataset_dict[language]['test'] = self.test[self.test['lang']==language].copy()\n        return dataset_dict\n\n    def print_label_distrib(self, dataset: pd.DataFrame):\n        \"\"\"\n        Print the normalized distribution of labels in the dataset.\n        \n        Args:\n            dataset (pd.DataFrame): The dataset containing a 'label' column.\n            \n        Returns:\n            None: Prints the percentage distribution of each label.\n        \"\"\"\n        print(dataset['label'].value_counts(normalize=True))\n\n    def get_tokenizer(self, model_card: str = \"microsoft/mdeberta-v3-base\"):\n        \"\"\"\n        Load a tokenizer from the Hugging Face model hub.\n        \n        Args:\n            model_card (str): Identifier for the pre-trained tokenizer to load.\n            \n        Returns:\n            AutoTokenizer: The loaded tokenizer.\n        \"\"\"\n        tokenizer = AutoTokenizer.from_pretrained(model_card)\n        self.tokenizer = tokenizer\n        return tokenizer\n\n    def get_model(self, model_card: str = \"microsoft/mdeberta-v3-base\", *args, **kwargs):\n        \"\"\"\n        Load a pre-trained model from the Hugging Face model hub.\n        \n        Args:\n            model_card (str): Identifier for the pre-trained model to load.\n            *args: Variable length argument list to pass to the model constructor.\n            **kwargs: Arbitrary keyword arguments to pass to the model constructor.\n            \n        Returns:\n            AutoModelForSequenceClassification: The loaded model.\n        \"\"\"\n        model = AutoModelForSequenceClassification.from_pretrained(model_card, *args, **kwargs)\n        self.model = model\n        return model\n\n    def get_class_weights(self, dataset: pd.DataFrame):\n        \"\"\"\n        Compute class weights for imbalanced datasets.\n        \n        Args:\n            dataset (pd.DataFrame): Dataset containing a 'label' column.\n            \n        Returns:\n            numpy.ndarray: Array of class weights where the index corresponds to the class label.\n        \"\"\"\n        class_weights = compute_class_weight('balanced', classes=np.unique(dataset['label']), y=dataset['label'])\n        return class_weights","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:05.423178Z","iopub.execute_input":"2025-05-02T16:03:05.423752Z","iopub.status.idle":"2025-05-02T16:03:05.439337Z","shell.execute_reply.started":"2025-05-02T16:03:05.423728Z","shell.execute_reply":"2025-05-02T16:03:05.438615Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"## Creating Our Detector Object  \n\nThe `__init__()` method of the `Subjectivity` class will load the dataset, set the device and the seeds, and automatically create the `test`, `dev`, and `train` splits from the datasets. It will also convert the `SUBJ` and `OBJ` labels to their corresponding numerical versions, so they are ready to be fed into a model.  \n","metadata":{}},{"cell_type":"code","source":"detector = Subjectivity(data_folder=data_folder, seed=SEED, device=device)","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:07.035210Z","iopub.execute_input":"2025-05-02T16:03:07.035707Z","iopub.status.idle":"2025-05-02T16:03:07.229463Z","shell.execute_reply.started":"2025-05-02T16:03:07.035683Z","shell.execute_reply":"2025-05-02T16:03:07.228864Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train: (6418, 6)\nDev: (2332, 6)\nTest: (5102, 6)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"As we can see the combined training set is composed of 5 languages with a total of 6418 samples. The development set is composed of 5 languages with a total of 2401 samples. The test set is composed of 5 languages with a total of 2332 samples.","metadata":{}},{"cell_type":"code","source":"detector.print_label_distrib(detector.train)\ndetector.print_label_distrib(detector.dev)\ndetector.print_label_distrib(detector.test)","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:07.356056Z","iopub.execute_input":"2025-05-02T16:03:07.356327Z","iopub.status.idle":"2025-05-02T16:03:07.364759Z","shell.execute_reply.started":"2025-05-02T16:03:07.356308Z","shell.execute_reply":"2025-05-02T16:03:07.363757Z"},"trusted":true},"outputs":[{"name":"stdout","text":"label\n0    0.631349\n1    0.368651\nName: proportion, dtype: float64\nlabel\n0    0.657376\n1    0.342624\nName: proportion, dtype: float64\nlabel\n1    1.0\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"The label distribution is imbalanced in all the splits, with the objective class being the majority class. This is important to keep in mind when evaluating the models. The proportion of the classes is similar in all the splits with approximately 63% of the samples belonging to the objective class and 37% to the subjective class.","metadata":{}},{"cell_type":"markdown","source":"Since each language might have a different sentence length, we will need to pad the sentences and truncate (if necessary) to a fixed length. \n\nWe will use as maximum sentence length the power of 2 that is closest to the 75th percentile of the sentence lengths in the training set. This is done to avoid padding too much and to save memory and computation time.","metadata":{}},{"cell_type":"code","source":"for lang in detector.languages:\n    print(f\"Language: {lang}\")\n    print(f\"{detector.all_data[lang]['train']['sentence'].str.len().describe().iloc[6:8]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:07.827664Z","iopub.execute_input":"2025-05-02T16:03:07.828410Z","iopub.status.idle":"2025-05-02T16:03:07.854157Z","shell.execute_reply.started":"2025-05-02T16:03:07.828381Z","shell.execute_reply":"2025-05-02T16:03:07.853333Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Language: arabic\n75%    203.75\nmax    444.00\nName: sentence, dtype: float64\n\nLanguage: greek\n75%   NaN\nmax   NaN\nName: sentence, dtype: float64\n\nLanguage: bulgarian\n75%    132.0\nmax    439.0\nName: sentence, dtype: float64\n\nLanguage: english\n75%    178.0\nmax    715.0\nName: sentence, dtype: float64\n\nLanguage: polish\n75%   NaN\nmax   NaN\nName: sentence, dtype: float64\n\nLanguage: ukrainian\n75%   NaN\nmax   NaN\nName: sentence, dtype: float64\n\nLanguage: romanian\n75%   NaN\nmax   NaN\nName: sentence, dtype: float64\n\nLanguage: german\n75%    161.0\nmax    625.0\nName: sentence, dtype: float64\n\nLanguage: multilingual\n75%   NaN\nmax   NaN\nName: sentence, dtype: float64\n\nLanguage: italian\n75%    169.0\nmax    550.0\nName: sentence, dtype: float64\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"We will select the maximum sentence length as 256 (the closest power of 2 to the 75th percentile of the sentence lengths in the training set).","metadata":{}},{"cell_type":"markdown","source":"Here we create the `results` dictionary and the `predictions_dict` dictionary. They will be used to store all the model's output","metadata":{}},{"cell_type":"code","source":"results = {}\npredictions_dict = {}","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:10.385542Z","iopub.execute_input":"2025-05-02T16:03:10.386192Z","iopub.status.idle":"2025-05-02T16:03:10.389608Z","shell.execute_reply.started":"2025-05-02T16:03:10.386167Z","shell.execute_reply":"2025-05-02T16:03:10.388851Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"# Custom Trainer  \n\nThis class extends the `Trainer` provided by Hugging Face. Since we needed to tweak some details in the training process, we opted to override some `Trainer` functions with custom ones.  \n","metadata":{}},{"cell_type":"code","source":"class CustomTrainer(Trainer):\n    \"\"\"\n    Custom Trainer class extending Hugging Face's Trainer with additional functionality:\n    - Support for class weights to handle imbalanced datasets\n    - Custom loss computation with weighted cross-entropy\n    - Threshold optimization for binary classification\n    - Custom prediction with threshold application\n    \"\"\"\n    \n    def __init__(self, *args, class_weights=None, weights_dtype=torch.float32, **kwargs):\n        \"\"\"\n        Initialize the CustomTrainer.\n        \n        Args:\n            class_weights (array-like, optional): Weights for each class to handle class imbalance.\n            weights_dtype (torch.dtype): Data type for the class weights tensor.\n            *args, **kwargs: Arguments passed to the parent Trainer class.\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        # Ensure label_weights is a tensor\n        if class_weights is not None:\n            self.class_weights = torch.tensor(class_weights, dtype=weights_dtype).to(self.args.device)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        \"\"\"\n        Compute the training loss with optional class weighting.\n        \n        Args:\n            model: The model to train\n            inputs: The inputs and targets of the model\n            return_outputs (bool): Whether to return the outputs along with the loss\n            num_items_in_batch: Not used but kept for compatibility\n            \n        Returns:\n            torch.Tensor or tuple: Loss value alone or with model outputs\n        \"\"\"\n        # Extract labels\n        labels = inputs.get(\"labels\")\n\n        # Forward pass\n        outputs = model(**inputs)\n\n        # Extract logits \n        logits = outputs.get('logits')\n\n        # Compute loss with class weights for imbalanced data handling\n        if self.class_weights is not None:\n            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n        else:\n            loss = F.cross_entropy(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss\n\n    def compute_best_threshold(self, dataset, ignore_keys=None, metric_key_prefix=\"test\"):\n        \"\"\"\n        Find the optimal classification threshold that maximizes macro F1 score.\n        \n        Args:\n            dataset: The dataset to use for threshold optimization\n            ignore_keys (list, optional): Keys to ignore in the model outputs\n            metric_key_prefix (str): Prefix for metric keys in the output\n            \n        Returns:\n            float: The optimal threshold value\n        \"\"\"\n        # Get raw predictions from parent class\n        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n\n        # Convert logits to probabilities using softmax (for binary classification)\n        logits = output.predictions\n        logits_tensor = torch.tensor(logits)\n        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n\n        # Calculate optimal threshold\n        labels = output.label_ids\n        thresholds = np.linspace(0.1, 0.9, 100) \n\n        best_threshold = 0.5  # Default threshold\n        best_f1 = 0\n\n        for threshold in thresholds:\n            predictions = (probabilities[:, 1] >= threshold).astype(int)\n            _, _, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"macro\", zero_division=0)\n            \n            if f1 > best_f1:\n                best_f1 = f1\n                best_threshold = threshold\n\n        # Return the best threshold found\n        return best_threshold\n        \n    def predict(self, dataset, threshold: float = 0.5, ignore_keys=None, metric_key_prefix=\"test\"):\n        \"\"\"\n        Generate predictions with a custom threshold for binary classification.\n        \n        Args:\n            dataset: The dataset to generate predictions for\n            threshold (float): The classification threshold (default: 0.5)\n            ignore_keys (list, optional): Keys to ignore in model outputs\n            metric_key_prefix (str): Prefix for metric keys in the output\n            \n        Returns:\n            PredictionOutput: Object containing predictions, labels, and metrics\n        \"\"\"\n        # Get raw predictions from parent class\n        output = super().predict(dataset, ignore_keys, metric_key_prefix)\n        \n        # Convert logits to probabilities using softmax (for binary classification)\n        logits = output.predictions\n        logits_tensor = torch.tensor(logits)\n        probabilities = torch.softmax(logits_tensor, dim=-1).numpy()\n        \n        final_predictions = (probabilities[:, 1] >= threshold).astype(int)\n\n        # Update predictions in the output object\n        return PredictionOutput(\n            predictions=final_predictions,\n            label_ids=output.label_ids,\n            metrics=output.metrics\n        )","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:10.650625Z","iopub.execute_input":"2025-05-02T16:03:10.650923Z","iopub.status.idle":"2025-05-02T16:03:10.661371Z","shell.execute_reply.started":"2025-05-02T16:03:10.650906Z","shell.execute_reply":"2025-05-02T16:03:10.660770Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## Text Tokenizer\n\nThe `tokenize_text` method will tokenize the text using the tokenizer provided by the model. It will also pad the text and truncate to the maximum length discussed before.","metadata":{}},{"cell_type":"code","source":"def tokenize_text(texts):\n    \"\"\"\n    Tokenize text data using the current tokenizer.\n    \n    Args:\n        texts (dict): Dictionary containing text data with a 'sentence' field\n        \n    Returns:\n        dict: Dictionary with tokenized text features including input_ids, \n              attention_mask, and potentially token_type_ids\n    \"\"\"\n    return tokenizer(texts['sentence'], padding=True, truncation=True, max_length=256, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:11.788245Z","iopub.execute_input":"2025-05-02T16:03:11.788495Z","iopub.status.idle":"2025-05-02T16:03:11.792300Z","shell.execute_reply.started":"2025-05-02T16:03:11.788480Z","shell.execute_reply":"2025-05-02T16:03:11.791608Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## Evaluation Metrics\n\nThe `evaluate_metrics` method will compute the accuracy, macro f1 and SUBJ f1 scores for the model. Here the predictions are computed with the argmax function, however this is not a problem since we are not taking into account these metrics for saving the best model at the end of the training process.\n\nThe model with the lowest validation loss is saved at the end of the training process making the evaluation metrics not necessary for the training process but rather an informative tool to understand the model's performance.","metadata":{}},{"cell_type":"code","source":"def evaluate_metrics(eval_pred):\n    \"\"\"\n    Calculate evaluation metrics for subjectivity classification models.\n    \n    This function computes various performance metrics for classification results:\n    - Accuracy: Overall correctness of predictions\n    - Macro-averaged precision, recall, and F1: Averages across both classes with equal weight\n    - Class-specific metrics: Precision, recall, and F1 specifically for the subjective class\n    \n    Args:\n        eval_pred (tuple): Tuple containing (predictions, labels) where:\n            - predictions: Raw model outputs/logits with shape (n_samples, n_classes)\n            - labels: Ground truth labels with shape (n_samples,)\n            \n    Returns:\n        dict: Dictionary containing the following metrics:\n            - macro_F1: Macro-averaged F1 score across all classes\n            - macro_P: Macro-averaged precision across all classes\n            - macro_R: Macro-averaged recall across all classes\n            - SUBJ_F1: F1 score for the subjective class (label 1)\n            - SUBJ_P: Precision for the subjective class\n            - SUBJ_R: Recall for the subjective class\n            - accuracy: Overall accuracy\n    \"\"\"\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    acc = accuracy_score(labels, predictions)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                                zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                                zero_division=0)\n\n    return {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:13.052617Z","iopub.execute_input":"2025-05-02T16:03:13.053184Z","iopub.status.idle":"2025-05-02T16:03:13.058355Z","shell.execute_reply.started":"2025-05-02T16:03:13.053164Z","shell.execute_reply":"2025-05-02T16:03:13.057601Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Saving Predictions\n\nFor the challenge, we need to save the predictions in a specific format. The `save_predictions` method will save the predictions in the required format by converting back the numerical labels to the original `SUBJ` and `OBJ` labels and saving them in a `.tsv` file.","metadata":{}},{"cell_type":"code","source":"def save_predictions(test_data, predictions, filename: str, save_dir: str = 'results'):\n    \"\"\"\n    Save model predictions to a TSV file with sentence IDs and predicted labels.\n    \n    Args:\n        test_data: Dataset containing the 'sentence_id' field to match with predictions\n        predictions: Array of binary predictions (0 for OBJ, 1 for SUBJ)\n        filename: Name of the output file (should end with .tsv)\n        save_dir: Directory to save the predictions file (default: 'results')\n        \n    Returns:\n        str: Full path to the saved predictions file\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    pred_df = pd.DataFrame()\n    pred_df['sentence_id'] = test_data['sentence_id']\n    pred_df['label'] = predictions\n    pred_df['label'] = pred_df['label'].apply(lambda x: 'OBJ' if x == 0 else 'SUBJ')\n\n    predictions_filepath = os.path.join(save_dir, filename)\n    pred_df.to_csv(predictions_filepath, index=False, sep='\\t')\n\n    print(f\"Saved predictions into file:\", predictions_filepath)\n    return predictions_filepath","metadata":{"execution":{"iopub.status.busy":"2025-05-02T16:03:13.371499Z","iopub.execute_input":"2025-05-02T16:03:13.372244Z","iopub.status.idle":"2025-05-02T16:03:13.377237Z","shell.execute_reply.started":"2025-05-02T16:03:13.372222Z","shell.execute_reply":"2025-05-02T16:03:13.376369Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"# Custom Model\n\nThe `CustomModel` class is used to customize the DeBERTa-V3 model architecture.\n\nThe idea is that we want to use the DeBERTa-V3 model while providing sentiment information to the model. To do this, we will concatenate the sentiment information to the CLS token output of the model. The sentiment information will be a vector of size 3, where the elements are scalar values representing the sentiment of the sentence. The sentiment information will be concatenated to the CLS token output of the model and then fed to a linear layer to predict the output (as in the original DeBERTa model).\n\nThe `forward` method of the `CustomModel` class will take as input the input_ids, attention_mask, and sentiment tensor. The sentiment tensor will be concatenated to the CLS token output of the model and then fed to a linear layer to predict the output.\n\n> Smarter approaches could be used to concatenate the sentiment information to the model's output, but for simplicity, we will use this approach. More feature fusion techniques could be used to concatenate the sentiment information to the model's output, but this is out of the scope of this notebook.","metadata":{}},{"cell_type":"code","source":"class CustomModel(PreTrainedModel):\n    config_class = DebertaV2Config\n\n    def __init__(self, config, sentiment_dim=3, num_labels=2, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n        self.deberta = DebertaV2Model(config)\n        self.pooler = ContextPooler(config)\n        output_dim = self.pooler.output_dim\n        self.dropout = nn.Dropout(0.1)\n\n        self.classifier = nn.Linear(output_dim + sentiment_dim, num_labels)\n\n    def forward(self, input_ids, positive, neutral, negative, attention_mask=None, labels=None):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n\n        encoder_layer = outputs[0]\n        pooled_output = self.pooler(encoder_layer)\n        \n        # Sentiment features as a single tensor\n        sentiment_features = torch.stack((positive, neutral, negative), dim=1)  # Shape: (batch_size, 3)\n        \n        # Combine CLS embedding with sentiment features\n        combined_features = torch.cat((pooled_output, sentiment_features), dim=1)\n        \n        # Classification head\n        logits = self.classifier(self.dropout(combined_features))\n        \n        return {'logits': logits}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:03:14.345612Z","iopub.execute_input":"2025-05-02T16:03:14.346357Z","iopub.status.idle":"2025-05-02T16:03:14.352158Z","shell.execute_reply.started":"2025-05-02T16:03:14.346333Z","shell.execute_reply":"2025-05-02T16:03:14.351418Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"## Sentiment Pipeline for Data Augmentation\n\nThe Sentiment pipeline is used to augment the data by adding sentiment information to the input. The sentiment information is a vector of size 3, where the elements are scalar values representing the sentiment of the sentence. The sentiment information is added to the input of the model to provide additional information to the model.\n\nThe selected model to extract sentiment information is the `cardiffnlp/twitter-xlm-roberta-base-sentiment` model from the Hugging Face library. This model is a sentiment analysis model trained on the Twitter dataset. The model will output the entire sentiment distribution (note the `top_k=None` parameter) for the input text.","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", tokenizer=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", top_k=None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:01:26.088521Z","iopub.execute_input":"2025-05-02T16:01:26.089170Z","iopub.status.idle":"2025-05-02T16:01:32.484274Z","shell.execute_reply.started":"2025-05-02T16:01:26.089145Z","shell.execute_reply":"2025-05-02T16:01:32.483689Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Before proceeding with the data augmentation, we will define the function `extract_sentiment` that will extract the sentiment information from the text. ","metadata":{}},{"cell_type":"code","source":"def extract_sentiment(text):\n    sentiments = pipe(text)[0]\n    return {k:v for k,v in [(list(sentiment.values())[0], list(sentiment.values())[1]) for sentiment in sentiments]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:01:32.485373Z","iopub.execute_input":"2025-05-02T16:01:32.485630Z","iopub.status.idle":"2025-05-02T16:01:32.489896Z","shell.execute_reply.started":"2025-05-02T16:01:32.485609Z","shell.execute_reply":"2025-05-02T16:01:32.489210Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Monolingual Training without Sentiment Pipeline\n\nIn this section we will train the DeBERTa-V3 model on the monolingual setting without using the sentiment pipeline. The model will be trained on the training set and validated on the development set. The model will be evaluated on the test set.\n\nThe model will be fine-tuned for 6 epochs with a batch size of 16 and a learning rate of 1e-5. The model will be trained using the AdamW optimizer and the CrossEntropyLoss function. No weight decay will be used.","metadata":{}},{"cell_type":"markdown","source":"## mDeBERTta v3 base (Arabic)\n\nThis section will test the [mDeBERta v3 base](https://huggingface.co/microsoft/mdeberta-v3-base) model on the Arabic langauge","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'arabic'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\nlabel_smoothing = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=f\"MatteoFasulo/mdeberta-v3-base-subjectivity-{language}\",\n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    #weight_decay=1e-1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Trainer instance\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n    data_collator=collator_fn,\n    compute_metrics=evaluate_metrics,\n    class_weights=class_weights\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mDeBERTta v3 base (Bulgarian)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'bulgarian'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\nlabel_smoothing = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=f\"MatteoFasulo/mdeberta-v3-base-subjectivity-{language}\", \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    #weight_decay=1e-1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Trainer instance\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n    data_collator=collator_fn,\n    compute_metrics=evaluate_metrics,\n    class_weights=class_weights\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mDeBERTa-base (English)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'english'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\nlabel_smoothing = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=f\"MatteoFasulo/mdeberta-v3-base-subjectivity-{language}\", \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    #weight_decay=1e-1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Trainer instance\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n    data_collator=collator_fn,\n    compute_metrics=evaluate_metrics,\n    class_weights=class_weights\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ModernBERT-base (English)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"answerdotai/ModernBERT-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'english'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=f\"MatteoFasulo/ModernBERT-base-subjectivity-{language}\", \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"ModernBERT-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Trainer instance\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n    data_collator=collator_fn,\n    compute_metrics=evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_modern_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_modern_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Llama-3.2-1B (English)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"meta-llama/Llama-3.2-1B\" # meta-llama/Meta-Llama-3-8B\nlanguage = 'english'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit = False, # enable 4-bit quantization\n    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n    bnb_4bit_use_double_quant = True, # quantize quantized weights\n    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n)\n\nlora_config = LoraConfig(\n    r = 16, # the dimension of the low-rank matrices\n    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, # dropout probability of the LoRA layers\n    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n    task_type = 'SEQ_CLS'\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_card, add_prefix_space=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    f\"MatteoFasulo/Llama-3.2-1B-subjectivity-{language}\",\n    #quantization_config=quantization_config,\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\nmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = get_peft_model(model, lora_config)\nmodel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 6\nbatch_size = 16\nlr = 1e-4\nweight_decay = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"Llama-3.2-1B-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_llama_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_llama_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"llama3.2-1B-{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mDeBERTta v3 base (German)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'german'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\nlabel_smoothing = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=f\"MatteoFasulo/mdeberta-v3-base-subjectivity-{language}\", \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    #weight_decay=1e-1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Trainer instance\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n    data_collator=collator_fn,\n    compute_metrics=evaluate_metrics,\n    class_weights=class_weights\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mDeBERTa-base (italian)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'italian'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\nlabel_smoothing = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=f\"MatteoFasulo/mdeberta-v3-base-subjectivity-{language}\", \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    #weight_decay=1e-1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Trainer instance\ntrainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=dev_data,\n    data_collator=collator_fn,\n    compute_metrics=evaluate_metrics,\n    class_weights=class_weights\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"# Test - Translating Arab to English\n\nwith torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()\n\nmodel_card = \"answerdotai/ModernBERT-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_card, \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)\n\nlanguage = 'arabic'\n\n!ct2-transformers-converter --model Helsinki-NLP/opus-mt-ar-en --output_dir opus-mt-ar-en\n\nimport ctranslate2\nimport transformers\n\ntranslator = ctranslate2.Translator(\"opus-mt-ar-en\", device='cuda')\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ar-en\")\n\ndef translate_text(raw_text):\n    source = tokenizer.convert_ids_to_tokens(tokenizer.encode(raw_text))\n    results = translator.translate_batch([source])\n    target = results[0].hypotheses[0]\n    translation = tokenizer.decode(tokenizer.convert_tokens_to_ids(target))\n    return translation\n\ndetector.all_data[language]['train']['translated_sentence'] = detector.all_data[language]['train']['sentence'].progress_apply(translate_text)\n\ndetector.all_data[language]['dev']['translated_sentence'] = detector.all_data[language]['dev']['sentence'].progress_apply(translate_text)\n\ndetector.all_data[language]['test']['translated_sentence'] = detector.all_data[language]['test']['sentence'].progress_apply(translate_text)\n\ndetector.all_data[language]['train'].to_csv('/kaggle/working/train_ar.csv')\ndetector.all_data[language]['dev'].to_csv('/kaggle/working/dev_ar.csv')\ndetector.all_data[language]['test'].to_csv('/kaggle/working/dev_test_ar.csv')\n\ndetector.all_data[language]['train'] = pd.read_csv('/kaggle/working/train_ar.csv')\ndetector.all_data[language]['dev'] = pd.read_csv('/kaggle/working/dev_ar.csv')\ndetector.all_data[language]['test'] = pd.read_csv('/kaggle/working/dev_test_ar.csv')\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0\n\ntrain_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ndef tokenize_text(texts):\n    return tokenizer(texts['translated_sentence'], padding=True, truncation=True, max_length=256, return_tensors='pt')\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])\n\n# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"ModernBERT-base-subjectivity-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)\n\ntrainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights\n)\n\ntrainer.train()\n\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"llama3.2-1B-{language}\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Monolingual Training with Sentiment Pipeline\n\nIn this section we will train the DeBERTa-V3 model on the monolingual setting using the sentiment pipeline. The model will be trained on the training set and validated on the development set. The model will be evaluated on the test set.\n\nThe sentiment is added to the input of the model to provide additional information using the sentiment pipeline described before. However, the sentiment information is used only with the CLS token output of the model.\n\n>More advanced techniques could explore the use of sentiment in early stages of the model, but this is out of the scope of this notebook.","metadata":{}},{"cell_type":"markdown","source":"## mdeberta-v3-base (arabic + sentiment)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'arabic'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\n\ntokenizer = detector.get_tokenizer(model_card=model_card)\n\n# Load the config\nconfig = DebertaV2Config.from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Initialize the custom model\nmodel = CustomModel(config=config, sentiment_dim=3, num_labels=2).from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-sentiment-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mdeberta-v3-base (bulgarian + sentiment)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'bulgarian'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\n\ntokenizer = detector.get_tokenizer(model_card=model_card)\n\n# Load the config\nconfig = DebertaV2Config.from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Initialize the custom model\nmodel = CustomModel(config=config, sentiment_dim=3, num_labels=2).from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-sentiment-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mdeberta-v3-base (english + sentiment)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'english'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\n\ntokenizer = detector.get_tokenizer(model_card=model_card)\n\n# Load the config\nconfig = DebertaV2Config.from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Initialize the custom model\nmodel = CustomModel(config=config, sentiment_dim=3, num_labels=2).from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-sentiment-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ModernBERT (english + sentiment)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomModernBertModel(ModernBertPreTrainedModel):\n    config_class = ModernBertConfig\n\n    def __init__(self, config, sentiment_dim=3, num_labels=2, *args, **kwargs):\n        super().__init__(config, *args, **kwargs)\n\n        self.config = config\n        self.model = ModernBertModel(config)\n        self.head = torch.nn.Sequential(\n            nn.Linear(config.hidden_size + sentiment_dim, config.hidden_size + sentiment_dim, config.classifier_bias),\n            GELUActivation(),\n            nn.LayerNorm(config.hidden_size + sentiment_dim, eps=config.norm_eps, bias=config.norm_bias)\n        )\n            \n        self.dropout = nn.Dropout(0.1)\n\n        self.classifier = nn.Linear(config.hidden_size + sentiment_dim, num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(self, input_ids, positive, neutral, negative, attention_mask=None, labels=None):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n\n        last_hidden_state = outputs[0]\n\n        if self.config.classifier_pooling == \"cls\":\n            last_hidden_state = last_hidden_state[:, 0]\n        elif self.config.classifier_pooling == \"mean\":\n            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n                dim=1, keepdim=True\n            )\n        \n        # Sentiment features as a single tensor\n        sentiment_features = torch.stack((positive, neutral, negative), dim=1)  # Shape: (batch_size, 3)\n        \n        # Combine output embedding with sentiment features\n        combined_features = torch.cat((last_hidden_state, sentiment_features), dim=1)\n\n        pooled_output = self.head(combined_features)\n        \n        # Classification head\n        logits = self.classifier(self.dropout(pooled_output))\n        \n        return {'logits': logits}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'english'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"answerdotai/ModernBERT-base\"\n\ntokenizer = detector.get_tokenizer(model_card=model_card)\n\n# Load the config\nconfig = ModernBertConfig.from_pretrained(\n    model_card,\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False,\n    classifier_pooling='mean'\n)\n\n# Initialize the custom model\nmodel = CustomModernBertModel(config=config, sentiment_dim=3, num_labels=2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"ModernBERT-base-subjectivity-sentiment-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-sentiment-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mdeberta-v3-base (german + sentiment)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"language = 'german'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\n\ntokenizer = detector.get_tokenizer(model_card=model_card)\n\n# Load the config\nconfig = DebertaV2Config.from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Initialize the custom model\nmodel = CustomModel(config=config, sentiment_dim=3, num_labels=2).from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"dev_{language}_sentiment_predicted.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"test_{language}_sentiment_predicted.tsv\")\n\nacc = accuracy_score(labels, predictions)\nm_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(labels, predictions, average=\"macro\",\n                                                            zero_division=0)\np_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(labels, predictions, labels=[1],\n                                                            zero_division=0)\nstats = {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc\n    }\n\nprint(stats)\nresults[f\"{language}-sentiment-thr\"] = stats\n\ncm = confusion_matrix(labels, predictions, normalize='all')\nConfusionMatrixDisplay(cm, display_labels=['OBJ', 'SUBJ']).plot()\nplt.title(f\"Confusion Matrix ({language})\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## mdeberta-v3-base (italian + sentiment)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:03:24.106590Z","iopub.execute_input":"2025-05-02T16:03:24.107350Z","iopub.status.idle":"2025-05-02T16:03:24.504952Z","shell.execute_reply.started":"2025-05-02T16:03:24.107323Z","shell.execute_reply":"2025-05-02T16:03:24.504187Z"}},"outputs":[{"name":"stdout","text":"Model deleted!\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"402"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"language = 'italian'\n\nepochs = 6\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:03:24.506012Z","iopub.execute_input":"2025-05-02T16:03:24.506226Z","iopub.status.idle":"2025-05-02T16:03:24.509618Z","shell.execute_reply.started":"2025-05-02T16:03:24.506209Z","shell.execute_reply":"2025-05-02T16:03:24.509098Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\n\ntokenizer = detector.get_tokenizer(model_card=model_card)\n\n# Load the config\nconfig = DebertaV2Config.from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    num_labels=2,\n    id2label={0: 'OBJ', 1: 'SUBJ'},\n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions=False,\n    output_hidden_states=False\n)\n\n# Initialize the custom model\nmodel = CustomModel(config=config, sentiment_dim=3, num_labels=2).from_pretrained(\n    f\"MatteoFasulo/mdeberta-v3-base-subjectivity-sentiment-{language}\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:03:24.694738Z","iopub.execute_input":"2025-05-02T16:03:24.695316Z","iopub.status.idle":"2025-05-02T16:03:34.138021Z","shell.execute_reply.started":"2025-05-02T16:03:24.695297Z","shell.execute_reply":"2025-05-02T16:03:34.137369Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"detector.all_data[language]['train'][['positive', 'neutral', 'negative']] = detector.all_data[language]['train'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['dev'][['positive', 'neutral', 'negative']] = detector.all_data[language]['dev'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')\ndetector.all_data[language]['test'][['positive', 'neutral', 'negative']] = detector.all_data[language]['test'].progress_apply(lambda x: extract_sentiment(x['sentence']), axis=1, result_type='expand')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:03:42.437195Z","iopub.execute_input":"2025-05-02T16:03:42.437816Z","iopub.status.idle":"2025-05-02T16:04:01.318303Z","shell.execute_reply.started":"2025-05-02T16:03:42.437795Z","shell.execute_reply":"2025-05-02T16:04:01.317573Z"}},"outputs":[{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1613/1613 [00:12<00:00, 129.13it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 513/513 [00:03<00:00, 128.28it/s]\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 299/299 [00:02<00:00, 125.87it/s]\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"train_data = Dataset.from_pandas(detector.all_data[language]['train'])\ndev_data = Dataset.from_pandas(detector.all_data[language]['dev'])\ntest_data = Dataset.from_pandas(detector.all_data[language]['test'])\n\ntrain_data = train_data.map(tokenize_text, batched=True)\ndev_data = dev_data.map(tokenize_text, batched=True)\ntest_data = test_data.map(tokenize_text, batched=True)\n\ncollator_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:04:01.319384Z","iopub.execute_input":"2025-05-02T16:04:01.319635Z","iopub.status.idle":"2025-05-02T16:04:01.864230Z","shell.execute_reply.started":"2025-05-02T16:04:01.319618Z","shell.execute_reply":"2025-05-02T16:04:01.863538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1613 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ef4b6e21dfa4c8c9f474a07b9929068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c9df75c7a2497188093e2406a5e5f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705d57ad5c9d4d46a772eca0430fa58b"}},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"# Define training args\ntraining_args = TrainingArguments(\n    output_dir=f\"mdeberta-v3-base-subjectivity-sentiment-{language}\",\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=lr,\n    num_train_epochs=epochs,\n    weight_decay=weight_decay,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    #warmup_ratio=0.5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:04:01.864971Z","iopub.execute_input":"2025-05-02T16:04:01.865274Z","iopub.status.idle":"2025-05-02T16:04:01.892260Z","shell.execute_reply.started":"2025-05-02T16:04:01.865256Z","shell.execute_reply":"2025-05-02T16:04:01.891709Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_data,\n    eval_dataset = dev_data,\n    data_collator = collator_fn,\n    compute_metrics = evaluate_metrics,\n    class_weights=class_weights,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:04:01.893706Z","iopub.execute_input":"2025-05-02T16:04:01.893980Z","iopub.status.idle":"2025-05-02T16:04:01.905720Z","shell.execute_reply.started":"2025-05-02T16:04:01.893961Z","shell.execute_reply":"2025-05-02T16:04:01.905156Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Decision threshold calibration on dev set\nbest_thr = trainer.compute_best_threshold(dataset=dev_data)\n# Predictions on dev set (with best threshold on dev set)\npred_info = trainer.predict(dataset=dev_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save dev set predictions\nsave_predictions(dev_data, predictions, filename = f\"task1_dev_mono_{language}.tsv\")\n\n# Predictions on test set (with best threshold on dev set)\npred_info = trainer.predict(dataset=test_data, threshold=best_thr)\n\npredictions, labels = pred_info.predictions, pred_info.label_ids\n\n# Save test set predictions\nsave_predictions(test_data, predictions, filename = f\"task1_test_mono_{language}.tsv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-02T16:04:19.666029Z","iopub.execute_input":"2025-05-02T16:04:19.666588Z","iopub.status.idle":"2025-05-02T16:04:28.001946Z","shell.execute_reply.started":"2025-05-02T16:04:19.666567Z","shell.execute_reply":"2025-05-02T16:04:28.001347Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Saved predictions into file: results/task_1_dev_mono_italian.tsv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Saved predictions into file: results/task1_test_mono_italian.tsv\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'results/task1_test_mono_italian.tsv'"},"metadata":{}}],"execution_count":49},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(results).T.sort_values(by='macro_F1', ascending=False).round(4)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}