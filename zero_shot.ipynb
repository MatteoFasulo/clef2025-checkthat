{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10881030,"sourceType":"datasetVersion","datasetId":6742324}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subjectivity in News Articles\n## Zero Shot with LLMs","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:43:11.960351Z","iopub.execute_input":"2025-03-03T08:43:11.960719Z","iopub.status.idle":"2025-03-03T08:43:19.494123Z","shell.execute_reply.started":"2025-03-03T08:43:11.960695Z","shell.execute_reply":"2025-03-03T08:43:19.493211Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-03T08:43:20.616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict\nimport os\nimport gc\nfrom pathlib import Path\n\nimport csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom joblib import delayed, Parallel\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport optuna\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import Dataset\nfrom huggingface_hub import notebook_login\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, RobertaTokenizerFast, RobertaForSequenceClassification, pipeline, get_linear_schedule_with_warmup, BitsAndBytesConfig, AutoModelForCausalLM\nfrom copy import deepcopy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:03:33.775431Z","iopub.execute_input":"2025-03-03T09:03:33.775827Z","iopub.status.idle":"2025-03-03T09:03:33.781938Z","shell.execute_reply.started":"2025-03-03T09:03:33.775800Z","shell.execute_reply":"2025-03-03T09:03:33.781202Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"SEED = 42\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:44:02.287564Z","iopub.execute_input":"2025-03-03T08:44:02.288078Z","iopub.status.idle":"2025-03-03T08:44:02.344602Z","shell.execute_reply.started":"2025-03-03T08:44:02.288042Z","shell.execute_reply":"2025-03-03T08:44:02.343571Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"data_folder = '/kaggle/input/clef2025-checkthat/data' # data\ndataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n\nfor language in os.listdir(data_folder):\n    for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n        if '.tsv' in filename:\n            abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n            df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n            if 'solved_conflict' in df.columns:\n                df.drop(columns=['solved_conflict'], inplace=True)\n            df['lang'] = language\n            df['split'] = Path(filename).stem\n            dataset = pd.concat([dataset, df], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:39.873640Z","iopub.execute_input":"2025-03-03T08:57:39.873993Z","iopub.status.idle":"2025-03-03T08:57:40.186511Z","shell.execute_reply.started":"2025-03-03T08:57:39.873965Z","shell.execute_reply":"2025-03-03T08:57:40.185594Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"dataset = dataset[dataset['lang'] == 'english']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:40.318941Z","iopub.execute_input":"2025-03-03T08:57:40.319280Z","iopub.status.idle":"2025-03-03T08:57:40.326000Z","shell.execute_reply.started":"2025-03-03T08:57:40.319252Z","shell.execute_reply":"2025-03-03T08:57:40.325106Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train = dataset[dataset['split'].str.contains('train')].copy()\ndev = dataset[dataset['split'].str.contains('dev')].copy()\ntest = dataset[dataset['split'].str.contains('dev_test')].copy()\n\nprint(f\"Train: {train.shape}\")\nprint(f\"Dev: {dev.shape}\")\nprint(f\"Test: {test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:42.331140Z","iopub.execute_input":"2025-03-03T08:57:42.331491Z","iopub.status.idle":"2025-03-03T08:57:42.343338Z","shell.execute_reply.started":"2025-03-03T08:57:42.331465Z","shell.execute_reply":"2025-03-03T08:57:42.342387Z"}},"outputs":[{"name":"stdout","text":"Train: (830, 5)\nDev: (946, 5)\nTest: (484, 5)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(f\"Train: {train['label'].value_counts(normalize=True)}\")\nprint(f\"Dev: {dev['label'].value_counts(normalize=True)}\")\nprint(f\"Test: {test['label'].value_counts(normalize=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:47.554158Z","iopub.execute_input":"2025-03-03T08:57:47.554519Z","iopub.status.idle":"2025-03-03T08:57:47.564969Z","shell.execute_reply.started":"2025-03-03T08:57:47.554493Z","shell.execute_reply":"2025-03-03T08:57:47.563575Z"}},"outputs":[{"name":"stdout","text":"Train: label\nOBJ     0.640964\nSUBJ    0.359036\nName: proportion, dtype: float64\nDev: label\nOBJ     0.617336\nSUBJ    0.382664\nName: proportion, dtype: float64\nTest: label\nOBJ     0.747934\nSUBJ    0.252066\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\ndev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\ntest.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:51.547680Z","iopub.execute_input":"2025-03-03T08:57:51.547990Z","iopub.status.idle":"2025-03-03T08:57:51.555498Z","shell.execute_reply.started":"2025-03-03T08:57:51.547967Z","shell.execute_reply":"2025-03-03T08:57:51.554733Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train['label'] = train['label'].astype(int)\ndev['label'] = dev['label'].astype(int)\ntest['label'] = test['label'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:52.370324Z","iopub.execute_input":"2025-03-03T08:57:52.370622Z","iopub.status.idle":"2025-03-03T08:57:52.375896Z","shell.execute_reply.started":"2025-03-03T08:57:52.370602Z","shell.execute_reply":"2025-03-03T08:57:52.375085Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:44:02.691350Z","iopub.execute_input":"2025-03-03T08:44:02.691579Z","iopub.status.idle":"2025-03-03T08:44:02.730407Z","shell.execute_reply.started":"2025-03-03T08:44:02.691558Z","shell.execute_reply":"2025-03-03T08:44:02.729476Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c327868f2cfc433e90bc6b6726e0e626"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:57:56.371562Z","iopub.execute_input":"2025-03-03T08:57:56.371873Z","iopub.status.idle":"2025-03-03T08:57:56.381145Z","shell.execute_reply.started":"2025-03-03T08:57:56.371851Z","shell.execute_reply":"2025-03-03T08:57:56.380121Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                            sentence_id  \\\n0  b9e1635a-72aa-467f-86d6-f56ef09f62c3   \n1  f99b5143-70d2-494a-a2f5-c68f10d09d0a   \n2  4076639c-aa56-4202-ae0f-9d9217f8da68   \n3  b057c366-698e-419d-a284-9b16d835c64e   \n4  a5a9645e-7850-41ba-90a2-5def725cd5b8   \n\n                                            sentence  label     lang     split  \n0  Gone are the days when they led the world in r...      1  english  train_en  \n1  The trend is expected to reverse as soon as ne...      0  english  train_en  \n2             But there is the specious point again.      0  english  train_en  \n3  He added he wouldn’t be surprised to see a new...      0  english  train_en  \n4  Not less government, you see; the same amount ...      1  english  train_en  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>lang</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b9e1635a-72aa-467f-86d6-f56ef09f62c3</td>\n      <td>Gone are the days when they led the world in r...</td>\n      <td>1</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f99b5143-70d2-494a-a2f5-c68f10d09d0a</td>\n      <td>The trend is expected to reverse as soon as ne...</td>\n      <td>0</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4076639c-aa56-4202-ae0f-9d9217f8da68</td>\n      <td>But there is the specious point again.</td>\n      <td>0</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b057c366-698e-419d-a284-9b16d835c64e</td>\n      <td>He added he wouldn’t be surprised to see a new...</td>\n      <td>0</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a5a9645e-7850-41ba-90a2-5def725cd5b8</td>\n      <td>Not less government, you see; the same amount ...</td>\n      <td>1</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"def clear_cache():\n        \"\"\"\n        Clears the GPU cache and performs garbage collection.\n        This method uses PyTorch's `torch.cuda.empty_cache()` to release all unoccupied cached memory\n        currently held by the caching allocator so that those can be used in other GPU applications.\n        It also calls Python's garbage collector to free up memory that is no longer in use.\n        Returns:\n            None\n        \"\"\"\n        torch.cuda.empty_cache()\n\n        with torch.no_grad():\n            torch.cuda.empty_cache()\n\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:46:45.321853Z","iopub.execute_input":"2025-03-03T08:46:45.322153Z","iopub.status.idle":"2025-03-03T08:46:45.326620Z","shell.execute_reply.started":"2025-03-03T08:46:45.322131Z","shell.execute_reply":"2025-03-03T08:46:45.325610Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def load_model(model_card: str, with_4_bit: bool = True) -> tuple:\n        \"\"\"\n        Loads a pre-trained model and its tokenizer with optional 4-bit quantization.\n\n        Args:\n            model_card (str): The identifier of the pre-trained model to load.\n            with_4_bit (bool, optional): If True, loads the model with 4-bit quantization.\n                                         If False, loads the model with 8-bit quantization.\n                                         Defaults to True.\n\n        Returns:\n            tuple: A tuple containing the loaded model and tokenizer.\n        \"\"\"\n        # Clear cache\n        clear_cache()\n\n        # Setup the quantization config\n        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n        if with_4_bit:\n            quantization_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_quant_type=\"fp4\",\n                bnb_4bit_compute_dtype=torch.bfloat16\n            )\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_card)\n        # Load the model\n        model = AutoModelForCausalLM.from_pretrained(model_card, quantization_config=quantization_config)\n        # Set the model to evaluation (inference) mode\n        model.eval()\n\n        return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:46:45.834775Z","iopub.execute_input":"2025-03-03T08:46:45.835072Z","iopub.status.idle":"2025-03-03T08:46:45.840120Z","shell.execute_reply.started":"2025-03-03T08:46:45.835051Z","shell.execute_reply":"2025-03-03T08:46:45.839000Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_card = 'mistralai/Mistral-7B-Instruct-v0.3'\nmistral_model, mistral_tokenizer = load_model(model_card, with_4_bit=True)\n\ndevice = mistral_model.device\nprint(f'Model loaded on {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:46:50.167714Z","iopub.execute_input":"2025-03-03T08:46:50.168023Z","iopub.status.idle":"2025-03-03T08:48:42.065121Z","shell.execute_reply.started":"2025-03-03T08:46:50.168001Z","shell.execute_reply":"2025-03-03T08:48:42.064117Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35492fc149994236ba7a1490a324da78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c5671cb6a6450eb21832e239c38b0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f0dd9a07ff46d282c0d34eaf47286f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4d36034d0cb49aabd3bb72b42cf1cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"449561020c6c432896478c4511021d37"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138ad689c1a94352ab2af84819bb6e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c354e39873134925bf74d3735669fc0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"502060bc3d6f4f6c8b3afaf5c9c7f49f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96eb92ba9c7040c1b54dac24f948943d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdb4b427114a42b0a93d452be52f4c52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a91bffe5b2ec4109841959036a384770"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15968a0a0a654123b017986a4346c473"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"prompt = [\n    {\n        'role': 'system',\n        'content': 'You are an annotator for subjectivity detection.'\n    },\n    {\n        'role': 'user',\n        'content': \"\"\"Your task is to classify input text as containing sexism or not. \n        A sentence is subjective if its content is based on or influenced by personal feelings, tastes, or opinions (asnwer with YES). Otherwise, the sentence is objective (answer with NO).\n        More precisely, a sentence is subjective if one or more of the following conditions apply:\n        1. expresses an explicit personal opinion from the author (e.g., speculations to draw conclusions);\n        2. includes sarcastic or ironic expressions;\n        3. gives exhortations of personal auspices;\n        4. contains discriminating or downgrading expressions;\n        5. contains rhetorical figures that convey the author’s opinion.\n        \n        The following ambiguous cases are objective: thirdparty’s opinions, comments that do not draw conclusions and leave open questions, and factual conclusions.\n        Note 1: Reported speech verbatim cannot contain elements that we identify as markers of subjectivity as it is not content created by thewriter, and, thus, is objective.\n        Note 2: Personal feelings, emotions, or mood of the author, without conveying opinions on the matter, are considered objective since the author is the most reliable source for information regarding their own emotions. Emotion-carrying statements are not excluded since they frequently occur in news articles and excluding them from the corpus would turn it less useful in real application scenarios.\n        \n        Respond only YES or NO.\n\n        TEXT:\n        {text}\n\n        ANSWER:\n        \"\"\"\n    }\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:59:36.817871Z","iopub.execute_input":"2025-03-03T08:59:36.818225Z","iopub.status.idle":"2025-03-03T08:59:36.823069Z","shell.execute_reply.started":"2025-03-03T08:59:36.818196Z","shell.execute_reply":"2025-03-03T08:59:36.822045Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"\ndef prepare_prompts(texts: pd.Series, prompt_template: list, tokenizer: AutoTokenizer):\n    \"\"\"\n    Prepares prompts by applying a chat template to a series of input texts.\n\n    Args:\n        texts (pd.Series): A pandas Series containing the input texts.\n        prompt_template (list): A list representing the prompt template with placeholders.\n        tokenizer (AutoTokenizer): An instance of the AutoTokenizer used to tokenize the prompts.\n\n    Returns:\n        list: A list of tokenized prompts ready for generation.\n    \"\"\"\n    # Store the prompts\n    prompts = []\n    # Iterate over the input texts\n    for text in texts:\n\n        # Create a deepcopy of the prompt template\n        prompt_with_text = deepcopy(prompt_template)\n\n        # Replace the placeholder with the input text\n        prompt_with_text[1]['content'] = prompt_with_text[1]['content'].replace('{text}', text)\n\n        # Apply the chat template to the input text\n        full_prompt = tokenizer.apply_chat_template(\n            prompt_with_text,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_dict=True,\n            return_tensors=\"pt\")\n        # Move the full prompt to the device\n        full_prompt = full_prompt.to(device)\n        # Append the full prompt to the list of prompts\n        prompts.append(full_prompt)\n\n    return prompts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:04:08.635551Z","iopub.execute_input":"2025-03-03T09:04:08.635924Z","iopub.status.idle":"2025-03-03T09:04:08.641276Z","shell.execute_reply.started":"2025-03-03T09:04:08.635897Z","shell.execute_reply":"2025-03-03T09:04:08.640405Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"prompts = prepare_prompts(texts=train['sentence'], prompt_template=prompt, tokenizer=mistral_tokenizer)\n\nprint(f\"Prompt example: \\n{mistral_tokenizer.decode(prompts[0].input_ids[0], skip_special_tokens=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:04:11.092340Z","iopub.execute_input":"2025-03-03T09:04:11.092654Z","iopub.status.idle":"2025-03-03T09:04:12.234996Z","shell.execute_reply.started":"2025-03-03T09:04:11.092629Z","shell.execute_reply":"2025-03-03T09:04:12.234096Z"}},"outputs":[{"name":"stdout","text":"Prompt example: \nYou are an annotator for subjectivity detection.\n\nYour task is to classify input text as containing sexism or not. \n        A sentence is subjective if its content is based on or influenced by personal feelings, tastes, or opinions (asnwer with YES). Otherwise, the sentence is objective (answer with NO).\n        More precisely, a sentence is subjective if one or more of the following conditions apply:\n        1. expresses an explicit personal opinion from the author (e.g., speculations to draw conclusions);\n        2. includes sarcastic or ironic expressions;\n        3. gives exhortations of personal auspices;\n        4. contains discriminating or downgrading expressions;\n        5. contains rhetorical figures that convey the author’s opinion.\n        \n        The following ambiguous cases are objective: thirdparty’s opinions, comments that do not draw conclusions and leave open questions, and factual conclusions.\n        Note 1: Reported speech verbatim cannot contain elements that weidentify as markers of subjectivity as it is not content created by thewriter, and, thus, is objective.\n        Note 2: Personal feelings, emotions, or mood of the author, without conveying opinions on the matter, are considered objective since the author is the most reliable source for information regarding their own emotions. Emotion-carrying statements are not excluded since they frequently occur in news articles and excluding them from the corpus would turn it less useful in real application scenarios.\n        \n        Respond only YES or NO.\n\n        TEXT:\n        Gone are the days when they led the world in recession-busting\n\n        ANSWER:\n        \n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"def generate_responses(model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompt_examples: list, max_new_tokens: int = 1000) -> list:\n    \"\"\"\n    Generates responses for a list of prompt examples using a specified language model and tokenizer.\n\n    Args:\n        model (AutoModelForCausalLM): The language model to use for generating responses.\n        tokenizer (AutoTokenizer): The tokenizer associated with the language model.\n        prompt_examples (list): A list of prompt examples to generate responses for.\n        max_new_tokens (int, optional): The maximum number of new tokens to generate for each prompt. Defaults to 1000.\n\n    Returns:\n        list: A list of generated responses corresponding to each prompt example.\n    \"\"\"\n    answers = []\n    for prompt in tqdm(prompt_examples):\n        response = model.generate(**prompt, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n        answers.append(response)\n    return answers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:05:33.789664Z","iopub.execute_input":"2025-03-03T09:05:33.790034Z","iopub.status.idle":"2025-03-03T09:05:33.794930Z","shell.execute_reply.started":"2025-03-03T09:05:33.790009Z","shell.execute_reply":"2025-03-03T09:05:33.794007Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"answers = generate_responses(model=mistral_model, tokenizer=mistral_tokenizer, prompt_examples=prompts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:05:37.782700Z","iopub.execute_input":"2025-03-03T09:05:37.782996Z","iopub.status.idle":"2025-03-03T09:23:36.410561Z","shell.execute_reply.started":"2025-03-03T09:05:37.782976Z","shell.execute_reply":"2025-03-03T09:23:36.409802Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 830/830 [17:58<00:00,  1.30s/it]\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"def process_response(response: torch.Tensor, tokenizer: AutoTokenizer) -> int:\n    \"\"\"\n    Processes the response tensor and determines if it contains a positive indication.\n\n    Args:\n        response (torch.Tensor): The tensor containing the response from the model.\n        tokenizer (AutoTokenizer): The tokenizer used to decode the response tensor.\n\n    Returns:\n        int: Returns 1 if the decoded response contains 'YES' after 'ANSWER', otherwise returns 0.\n    \"\"\"\n    response_text = tokenizer.decode(response[0])\n    if 'YES' in response_text.split('ANSWER')[-1]:\n        return 1\n    elif 'NO' in response_text.split('ANSWER')[-1]:\n        return 0\n    else:\n        return -1 # Model answer is not compliant with the classification task","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:25:13.482293Z","iopub.execute_input":"2025-03-03T09:25:13.482619Z","iopub.status.idle":"2025-03-03T09:25:13.487391Z","shell.execute_reply.started":"2025-03-03T09:25:13.482597Z","shell.execute_reply":"2025-03-03T09:25:13.486405Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import re\ndef get_generated_response(response: torch.Tensor, tokenizer: AutoTokenizer) -> str:\n    \"\"\"\n    Decodes a tensor response into a cleaned string.\n\n    Args:\n        response (torch.Tensor): The tensor containing the response to decode.\n        tokenizer (AutoTokenizer): The tokenizer used to decode the response.\n\n    Returns:\n        str: The cleaned and decoded response string.\n    \"\"\"\n    # Get the decoded response text\n    response_text = tokenizer.decode(response[0])\n    # Assign the default tokenizer name\n    tokenizer_name = 'mistral'\n    # Check if the tokenizer has a name or path attribute\n    if hasattr(tokenizer, 'name_or_path'):\n        # In that case, assign the name or path to the tokenizer_name variable\n        tokenizer_name = tokenizer.name_or_path\n    # If the tokenizer name contains 'llama', split the response text with the special tokens of the Llama model\n    if 'llama' in tokenizer_name:\n        response_text = response_text.split('<|end_header_id|>')[-1]\n    # If it is mistral, split the response text with the special tokens of the Mistral model\n    else:\n        response_text = response_text.split('[/INST]')[-1]\n\n    # Remove the special tokens from the response text\n    cleaned_string = re.sub(r'</s>', '', response_text).strip()\n    # If the tokenizer name contains 'llama', remove the special token '<|eot_id|>'\n    if 'llama' in tokenizer_name:\n        cleaned_string = re.sub(r'<\\|eot_id\\|>', '', cleaned_string).strip()\n    return cleaned_string","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:25:32.228015Z","iopub.execute_input":"2025-03-03T09:25:32.228359Z","iopub.status.idle":"2025-03-03T09:25:32.233419Z","shell.execute_reply.started":"2025-03-03T09:25:32.228329Z","shell.execute_reply":"2025-03-03T09:25:32.232622Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"batch_predictions = [process_response(response=item, tokenizer=mistral_tokenizer) for item in answers]\ngenerated_answers = [get_generated_response(response=item, tokenizer=mistral_tokenizer) for item in answers]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:25:34.553237Z","iopub.execute_input":"2025-03-03T09:25:34.553559Z","iopub.status.idle":"2025-03-03T09:25:35.104877Z","shell.execute_reply.started":"2025-03-03T09:25:34.553534Z","shell.execute_reply":"2025-03-03T09:25:35.103950Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:27:02.618949Z","iopub.execute_input":"2025-03-03T09:27:02.619318Z","iopub.status.idle":"2025-03-03T09:27:02.628633Z","shell.execute_reply.started":"2025-03-03T09:27:02.619289Z","shell.execute_reply":"2025-03-03T09:27:02.627917Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                            sentence_id  \\\n0  b9e1635a-72aa-467f-86d6-f56ef09f62c3   \n1  f99b5143-70d2-494a-a2f5-c68f10d09d0a   \n2  4076639c-aa56-4202-ae0f-9d9217f8da68   \n3  b057c366-698e-419d-a284-9b16d835c64e   \n4  a5a9645e-7850-41ba-90a2-5def725cd5b8   \n\n                                            sentence  label     lang     split  \n0  Gone are the days when they led the world in r...      1  english  train_en  \n1  The trend is expected to reverse as soon as ne...      0  english  train_en  \n2             But there is the specious point again.      0  english  train_en  \n3  He added he wouldn’t be surprised to see a new...      0  english  train_en  \n4  Not less government, you see; the same amount ...      1  english  train_en  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_id</th>\n      <th>sentence</th>\n      <th>label</th>\n      <th>lang</th>\n      <th>split</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b9e1635a-72aa-467f-86d6-f56ef09f62c3</td>\n      <td>Gone are the days when they led the world in r...</td>\n      <td>1</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f99b5143-70d2-494a-a2f5-c68f10d09d0a</td>\n      <td>The trend is expected to reverse as soon as ne...</td>\n      <td>0</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4076639c-aa56-4202-ae0f-9d9217f8da68</td>\n      <td>But there is the specious point again.</td>\n      <td>0</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b057c366-698e-419d-a284-9b16d835c64e</td>\n      <td>He added he wouldn’t be surprised to see a new...</td>\n      <td>0</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a5a9645e-7850-41ba-90a2-5def725cd5b8</td>\n      <td>Not less government, you see; the same amount ...</td>\n      <td>1</td>\n      <td>english</td>\n      <td>train_en</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"models_predictions = pd.DataFrame({\n    'text': train['sentence'],\n    'original_labels': train['label'],\n    'Mistralv3_zero_shot_labels': batch_predictions,\n    'Mistralv3_zero_shot_answers': generated_answers\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:27:10.810627Z","iopub.execute_input":"2025-03-03T09:27:10.811009Z","iopub.status.idle":"2025-03-03T09:27:10.816480Z","shell.execute_reply.started":"2025-03-03T09:27:10.810963Z","shell.execute_reply":"2025-03-03T09:27:10.815650Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"models_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:27:14.524485Z","iopub.execute_input":"2025-03-03T09:27:14.524807Z","iopub.status.idle":"2025-03-03T09:27:14.535707Z","shell.execute_reply.started":"2025-03-03T09:27:14.524782Z","shell.execute_reply":"2025-03-03T09:27:14.534749Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"                                                  text  original_labels  \\\n0    Gone are the days when they led the world in r...                1   \n1    The trend is expected to reverse as soon as ne...                0   \n2               But there is the specious point again.                0   \n3    He added he wouldn’t be surprised to see a new...                0   \n4    Not less government, you see; the same amount ...                1   \n..                                                 ...              ...   \n825  Local governments and their financing vehicles...                1   \n826  That fact alone underscores the biggest proble...                1   \n827  Presumably it had in mind those Russian offici...                1   \n828  From bad taxation, reckless borrowing and reck...                1   \n829  Foreign Ministry spokeswoman Mariya Zakharova,...                1   \n\n     Mistralv3_zero_shot_labels Mistralv3_zero_shot_answers  \n0                             0                          NO  \n1                             0                          NO  \n2                             1                         YES  \n3                             0                          NO  \n4                             1                         YES  \n..                          ...                         ...  \n825                           0                          NO  \n826                           0                          NO  \n827                           1                         YES  \n828                           0                          NO  \n829                           1                         YES  \n\n[830 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>original_labels</th>\n      <th>Mistralv3_zero_shot_labels</th>\n      <th>Mistralv3_zero_shot_answers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Gone are the days when they led the world in r...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The trend is expected to reverse as soon as ne...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>But there is the specious point again.</td>\n      <td>0</td>\n      <td>1</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>He added he wouldn’t be surprised to see a new...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NO</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Not less government, you see; the same amount ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>825</th>\n      <td>Local governments and their financing vehicles...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NO</td>\n    </tr>\n    <tr>\n      <th>826</th>\n      <td>That fact alone underscores the biggest proble...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NO</td>\n    </tr>\n    <tr>\n      <th>827</th>\n      <td>Presumably it had in mind those Russian offici...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>828</th>\n      <td>From bad taxation, reckless borrowing and reck...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NO</td>\n    </tr>\n    <tr>\n      <th>829</th>\n      <td>Foreign Ministry spokeswoman Mariya Zakharova,...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>YES</td>\n    </tr>\n  </tbody>\n</table>\n<p>830 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"mistal_base_metrics = compute_metrics(responses=answers, y_true=original_labels, tokenizer=mistral_tokenizer)\nprint(f\"Mistral v3 Zero-Shot Metrics: {mistal_base_metrics}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Correggere la compute metrics function,\npossiamo eliminare il richiamo alla funzione process_response visto che é giá stato fatto","metadata":{}},{"cell_type":"code","source":"import sklearn\n\ndef compute_metrics(responses: list, y_true: list, tokenizer: AutoTokenizer) -> dict:\n    \"\"\"\n    Compute the accuracy and fail ratio of the model's predictions.\n\n    Args:\n        responses (list): A list of responses generated by the model.\n        y_true (list): A list of true labels corresponding to the responses.\n\n    Returns:\n        dict: A dictionary containing the accuracy and fail ratio of the predictions.\n            - 'accuracy' (float): The proportion of correct predictions.\n            - 'fail_ratio' (float): The proportion of incorrect answers which are not compliant with the classification task.\n    \"\"\"\n    y_pred = [process_response(response=response, tokenizer=tokenizer) for response in responses]\n    accuracy = (np.array(y_pred) == np.array(y_true)).mean()\n\n    f1_score = sklearn.metrics.f1_score(y_true, y_pred)\n\n    \n    return {'accuracy': accuracy, 'f1_score': f1_score}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:38:23.181543Z","iopub.execute_input":"2025-03-03T09:38:23.181861Z","iopub.status.idle":"2025-03-03T09:38:23.186467Z","shell.execute_reply.started":"2025-03-03T09:38:23.181836Z","shell.execute_reply":"2025-03-03T09:38:23.185693Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"mistal_base_metrics = compute_metrics(responses=answers, y_true=train['label'], tokenizer=mistral_tokenizer)\n\nprint(f\"Mistral v3 Zero-Shot Metrics: {mistal_base_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:38:25.191492Z","iopub.execute_input":"2025-03-03T09:38:25.191817Z","iopub.status.idle":"2025-03-03T09:38:25.468971Z","shell.execute_reply.started":"2025-03-03T09:38:25.191790Z","shell.execute_reply":"2025-03-03T09:38:25.468325Z"}},"outputs":[{"name":"stdout","text":"Mistral v3 Zero-Shot Metrics: {'accuracy': 0.7024096385542169, 'f1_score': 0.4969450101832994}\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"cm = confusion_matrix(train['label'], batch_predictions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:40:38.659090Z","iopub.execute_input":"2025-03-03T09:40:38.659497Z","iopub.status.idle":"2025-03-03T09:40:38.666448Z","shell.execute_reply.started":"2025-03-03T09:40:38.659465Z","shell.execute_reply":"2025-03-03T09:40:38.665694Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"disp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:40:59.461982Z","iopub.execute_input":"2025-03-03T09:40:59.462303Z","iopub.status.idle":"2025-03-03T09:40:59.731613Z","shell.execute_reply.started":"2025-03-03T09:40:59.462278Z","shell.execute_reply":"2025-03-03T09:40:59.730714Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fef357f7010>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":59},{"cell_type":"markdown","source":"# llama ","metadata":{}},{"cell_type":"code","source":"#load Llama3.1 8B model\nmodel_card = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\nllama_model, llama_tokenizer = load_model(model_card, with_4_bit=True)\n\ndevice = llama_model.device\nprint(f'Model loaded on {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T09:57:50.254443Z","iopub.execute_input":"2025-03-03T09:57:50.254776Z","iopub.status.idle":"2025-03-03T10:00:08.379584Z","shell.execute_reply.started":"2025-03-03T09:57:50.254753Z","shell.execute_reply":"2025-03-03T10:00:08.378887Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4757d455ae62426b811ab1dd88310a75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:  25%|##5       | 1.25G/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2156faa7e2441428be510720ca69c55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"440b5b8265504982b0875d558cb32ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a706a61e8b4c1db09b135ed7a51f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ae721806524c55aaf76f10a6f67ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82ac73a72bb14fd792461704fc029f3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78ca1da66f0c4612bac15f8fe4aa9121"}},"metadata":{}},{"name":"stdout","text":"Model loaded on cuda:0\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"prompts = prepare_prompts(texts=train['sentence'], prompt_template=prompt, tokenizer=llama_tokenizer)\n\nprint(f\"Prompt example: \\n{llama_tokenizer.decode(prompts[0].input_ids[0], skip_special_tokens=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:00:08.380546Z","iopub.execute_input":"2025-03-03T10:00:08.380765Z","iopub.status.idle":"2025-03-03T10:00:09.515043Z","shell.execute_reply.started":"2025-03-03T10:00:08.380746Z","shell.execute_reply":"2025-03-03T10:00:09.514329Z"}},"outputs":[{"name":"stdout","text":"Prompt example: \nsystem\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are an annotator for subjectivity detection.user\n\nYour task is to classify input text as containing sexism or not. \n        A sentence is subjective if its content is based on or influenced by personal feelings, tastes, or opinions (asnwer with YES). Otherwise, the sentence is objective (answer with NO).\n        More precisely, a sentence is subjective if one or more of the following conditions apply:\n        1. expresses an explicit personal opinion from the author (e.g., speculations to draw conclusions);\n        2. includes sarcastic or ironic expressions;\n        3. gives exhortations of personal auspices;\n        4. contains discriminating or downgrading expressions;\n        5. contains rhetorical figures that convey the author’s opinion.\n        \n        The following ambiguous cases are objective: thirdparty’s opinions, comments that do not draw conclusions and leave open questions, and factual conclusions.\n        Note 1: Reported speech verbatim cannot contain elements that weidentify as markers of subjectivity as it is not content created by thewriter, and, thus, is objective.\n        Note 2: Personal feelings, emotions, or mood of the author, without conveying opinions on the matter, are considered objective since the author is the most reliable source for information regarding their own emotions. Emotion-carrying statements are not excluded since they frequently occur in news articles and excluding them from the corpus would turn it less useful in real application scenarios.\n        \n        Respond only YES or NO.\n\n        TEXT:\n        Gone are the days when they led the world in recession-busting\n\n        ANSWER:assistant\n\n\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"answers = generate_responses(model=llama_model, tokenizer=llama_tokenizer, prompt_examples=prompts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:00:09.516354Z","iopub.execute_input":"2025-03-03T10:00:09.516663Z","iopub.status.idle":"2025-03-03T10:17:34.948663Z","shell.execute_reply.started":"2025-03-03T10:00:09.516638Z","shell.execute_reply":"2025-03-03T10:17:34.947849Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 830/830 [17:25<00:00,  1.26s/it]\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"batch_predictions = [process_response(response=item, tokenizer=llama_tokenizer) for item in answers]\ngenerated_answers = [get_generated_response(response=item, tokenizer=llama_tokenizer) for item in answers]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:17:34.949876Z","iopub.execute_input":"2025-03-03T10:17:34.950117Z","iopub.status.idle":"2025-03-03T10:17:35.369206Z","shell.execute_reply.started":"2025-03-03T10:17:34.950095Z","shell.execute_reply":"2025-03-03T10:17:35.368549Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"models_predictions['Llama3_zero_shot_labels'] = batch_predictions\nmodels_predictions['Llama3_zero_shot_answers'] = generated_answers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:17:35.369969Z","iopub.execute_input":"2025-03-03T10:17:35.370267Z","iopub.status.idle":"2025-03-03T10:17:35.375501Z","shell.execute_reply.started":"2025-03-03T10:17:35.370237Z","shell.execute_reply":"2025-03-03T10:17:35.374594Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"llama_base_metrics = compute_metrics(responses=answers, y_true=train['label'], tokenizer=llama_tokenizer)\nprint(f\"Llama3 Zero-Shot Metrics: {llama_base_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:20:58.949293Z","iopub.execute_input":"2025-03-03T10:20:58.949589Z","iopub.status.idle":"2025-03-03T10:20:59.163006Z","shell.execute_reply.started":"2025-03-03T10:20:58.949568Z","shell.execute_reply":"2025-03-03T10:20:59.162167Z"}},"outputs":[{"name":"stdout","text":"Llama3 Zero-Shot Metrics: {'accuracy': 0.7072289156626506, 'f1_score': 0.5699115044247788}\n","output_type":"stream"}],"execution_count":74},{"cell_type":"markdown","source":"# Stima numero token ","metadata":{}},{"cell_type":"code","source":"full_prompt = llama_tokenizer.apply_chat_template(\n                prompt,\n                tokenize=True,\n                add_generation_prompt=True,\n                return_dict=True,\n                return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:24:47.793869Z","iopub.execute_input":"2025-03-03T10:24:47.794168Z","iopub.status.idle":"2025-03-03T10:24:47.799952Z","shell.execute_reply.started":"2025-03-03T10:24:47.794146Z","shell.execute_reply":"2025-03-03T10:24:47.799261Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"full_prompt['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:25:22.711718Z","iopub.execute_input":"2025-03-03T10:25:22.712055Z","iopub.status.idle":"2025-03-03T10:25:22.719217Z","shell.execute_reply.started":"2025-03-03T10:25:22.712027Z","shell.execute_reply":"2025-03-03T10:25:22.718412Z"}},"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n            220,   1627,  10263,    220,   2366,     19,    271,   2675,    527,\n            459,  37142,    859,    369,   3917,   1968,  18468,     13, 128009,\n         128006,    882, 128007,    271,   7927,   3465,    374,    311,  49229,\n           1988,   1495,    439,   8649,  69095,    477,    539,     13,    720,\n            286,    362,  11914,    374,  44122,    422,   1202,   2262,    374,\n           3196,    389,    477,  28160,    555,   4443,  16024,     11,  36263,\n             11,    477,  18463,    320,  66636,   6703,    449,  14410,    570,\n          18715,     11,    279,  11914,    374,  16945,    320,   9399,    449,\n           5782,   4390,    286,   4497,  24559,     11,    264,  11914,    374,\n          44122,    422,    832,    477,    810,    315,    279,   2768,   4787,\n           3881,    512,    286,    220,     16,     13,  61120,    459,  11720,\n           4443,   9647,    505,    279,   3229,    320,     68,   1326,   2637,\n           1424,   7607,    311,   4128,  31342,    317,    286,    220,     17,\n             13,   5764,  83367,    292,    477,  59560,  24282,    280,    286,\n            220,     18,     13,   6835,    506,  22780,    811,    315,   4443,\n          90196,   1238,    280,    286,    220,     19,     13,   5727,  14572,\n          16252,    477,   1523,  33359,  24282,    280,    286,    220,     20,\n             13,   5727,  87068,  12678,    430,  20599,    279,   3229,    753,\n           9647,    627,   1827,    286,    578,   2768,  55861,   5157,    527,\n          16945,     25,   4948,  34057,    753,  18463,     11,   6170,    430,\n            656,    539,   4128,  31342,    323,   5387,   1825,   4860,     11,\n            323,  61001,  31342,    627,    286,   7181,    220,     16,     25,\n          79711,   8982,   2807,  55848,   4250,   6782,   5540,    430,    584,\n          81496,    439,  24915,    315,   3917,   1968,    439,    433,    374,\n            539,   2262,   3549,    555,    279,  18688,     11,    323,     11,\n           8617,     11,    374,  16945,    627,    286,   7181,    220,     17,\n             25,  19758,  16024,     11,  21958,     11,    477,  20247,    315,\n            279,   3229,     11,   2085,  94165,  18463,    389,    279,   5030,\n             11,    527,   6646,  16945,   2533,    279,   3229,    374,    279,\n           1455,  15062,   2592,    369,   2038,   9002,    872,   1866,  21958,\n             13,   5867,   6082,   1824,  11687,    287,  12518,    527,    539,\n          28544,   2533,    814,  14134,  12446,    304,   3754,   9908,    323,\n          44878,   1124,    505,    279,  43194,   1053,   2543,    433,   2753,\n           5505,    304,   1972,   3851,  26350,    627,   1827,    286,  40633,\n           1193,  14410,    477,   5782,    382,    286,  16139,    512,    286,\n            314,   1342,    633,    286,  97804,    643,     25, 128009, 128006,\n          78191, 128007,    271]])"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"len(full_prompt['input_ids'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:25:40.837788Z","iopub.execute_input":"2025-03-03T10:25:40.838085Z","iopub.status.idle":"2025-03-03T10:25:40.843418Z","shell.execute_reply.started":"2025-03-03T10:25:40.838062Z","shell.execute_reply":"2025-03-03T10:25:40.842344Z"}},"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"345"},"metadata":{}}],"execution_count":84},{"cell_type":"code","source":"(450 * len(train)) + (5 * len(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T10:26:57.742292Z","iopub.execute_input":"2025-03-03T10:26:57.742628Z","iopub.status.idle":"2025-03-03T10:26:57.747894Z","shell.execute_reply.started":"2025-03-03T10:26:57.742600Z","shell.execute_reply":"2025-03-03T10:26:57.747026Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"377650"},"metadata":{}}],"execution_count":86},{"cell_type":"markdown","source":"OpenaAI 4o mini","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}