{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":10881030,"datasetId":6742324,"databundleVersionId":11247848,"isSourceIdPinned":false}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subjectivity in News Articles\n\n## Group:\n- Luca Babboni - luca.babboni2@studio.unibo.it\n- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n\n## Description\n\nThis notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n\nThis is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n\nThe task comprises three settings:\n\n* Monolingual: train and test on data in a given language\n* Multilingual: train and test on data comprising several languages\n* Zero-shot: train on several languages and test on unseen languages\n\ntraining data in five languages:\n* Arabic\n* Bulgarian\n* English\n* German\n* Italian\n\nThe official evaluation is macro-averaged F1 between the two classes.","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nimport csv\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom joblib import delayed, Parallel\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport torch\nimport torch.nn as nn\n\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import Dataset\nfrom huggingface_hub import notebook_login\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, RobertaTokenizerFast, RobertaForSequenceClassification, pipeline","metadata":{"execution":{"iopub.status.busy":"2025-02-28T16:01:04.408289Z","iopub.execute_input":"2025-02-28T16:01:04.408660Z","iopub.status.idle":"2025-02-28T16:01:04.414929Z","shell.execute_reply.started":"2025-02-28T16:01:04.408633Z","shell.execute_reply":"2025-02-28T16:01:04.413577Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"SEED = 42\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:55:09.263803Z","iopub.execute_input":"2025-02-28T15:55:09.264582Z","iopub.status.idle":"2025-02-28T15:55:09.272815Z","shell.execute_reply.started":"2025-02-28T15:55:09.264553Z","shell.execute_reply":"2025-02-28T15:55:09.271395Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_folder = '/kaggle/input/clef2025-checkthat/data'\ndataset = pd.DataFrame(columns=['sentence_id','sentence','label','solved_conflicts','lang','split'])\n\nfor language in os.listdir(data_folder):\n    for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n        if '.tsv' in filename:\n            abs_path = str(Path(filename).absolute())\n            df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n            df['lang'] = language\n            df['split'] = Path(filename).stem\n            dataset = pd.concat([dataset, df], axis=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T16:02:22.548934Z","iopub.execute_input":"2025-02-28T16:02:22.549291Z","iopub.status.idle":"2025-02-28T16:02:22.588677Z","shell.execute_reply.started":"2025-02-28T16:02:22.549265Z","shell.execute_reply":"2025-02-28T16:02:22.587275Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c7846301506b>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'.tsv'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mabs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUOTE_NONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/train_ar.tsv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/train_ar.tsv'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"train_filepath = '/kaggle/input/clef2025-checkthat/data/english/train_en.tsv'\ntest_filepath = '/kaggle/input/clef2025-checkthat/data/english/dev_test_en.tsv'","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:48.901050Z","iopub.execute_input":"2025-02-28T15:39:48.901376Z","iopub.status.idle":"2025-02-28T15:39:48.905376Z","shell.execute_reply.started":"2025-02-28T15:39:48.901351Z","shell.execute_reply":"2025-02-28T15:39:48.904508Z"},"trusted":true},"outputs":[],"execution_count":21},{"cell_type":"code","source":"train_data = pd.read_csv(train_filepath, sep='\\t', quoting=csv.QUOTE_NONE)\ntest_data = pd.read_csv(test_filepath, sep='\\t', quoting=csv.QUOTE_NONE)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:49.057084Z","iopub.execute_input":"2025-02-28T15:39:49.057387Z","iopub.status.idle":"2025-02-28T15:39:49.071298Z","shell.execute_reply.started":"2025-02-28T15:39:49.057364Z","shell.execute_reply":"2025-02-28T15:39:49.070512Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"train_data.label.value_counts(), test_data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:49.222609Z","iopub.execute_input":"2025-02-28T15:39:49.222937Z","iopub.status.idle":"2025-02-28T15:39:49.230520Z","shell.execute_reply.started":"2025-02-28T15:39:49.222913Z","shell.execute_reply":"2025-02-28T15:39:49.229743Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(label\n OBJ     532\n SUBJ    298\n Name: count, dtype: int64,\n label\n OBJ     362\n SUBJ    122\n Name: count, dtype: int64)"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"Legend:\n* Objective -> 0\n* Subjective -> 1","metadata":{}},{"cell_type":"code","source":"train_data['label'] = train_data['label'].apply(lambda x: 1 if x == 'SUBJ' else 0)\ntest_data['label'] = test_data['label'].apply(lambda x: 1 if x == 'SUBJ' else 0)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:51.059200Z","iopub.execute_input":"2025-02-28T15:39:51.059556Z","iopub.status.idle":"2025-02-28T15:39:51.065554Z","shell.execute_reply.started":"2025-02-28T15:39:51.059526Z","shell.execute_reply":"2025-02-28T15:39:51.064499Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"notebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline Model","metadata":{}},{"cell_type":"code","source":"vect = SentenceTransformer(\"all-mpnet-base-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = LogisticRegression(class_weight=\"balanced\", random_state=SEED)\nmodel.fit(X=vect.encode(train_data['sentence'].values), y=train_data['label'].values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X=vect.encode(test_data['sentence'].values)).tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_df = pd.DataFrame()\npred_df['sentence_id'] = test_data['sentence_id']\npred_df['label'] = predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(gold_values, predicted_values):\n    acc = accuracy_score(gold_values, predicted_values)\n    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(gold_values, predicted_values, average=\"macro\",\n                                                               zero_division=0)\n    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(gold_values, predicted_values, labels=[1],\n                                                               zero_division=0)\n    #roc_auc = roc_auc_score(gold_values, predicted_probabilities)\n\n    return {\n        'macro_F1': m_f1,\n        'macro_P': m_prec,\n        'macro_R': m_rec,\n        'SUBJ_F1': p_f1[0],\n        'SUBJ_P': p_prec[0],\n        'SUBJ_R': p_rec[0],\n        'accuracy': acc,\n        #'roc_auc': roc_auc\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(gold_values=test_data.label.values, predicted_values=predictions)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Twitter RoBERTa-base 2022 154M","metadata":{}},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_card, use_Fast=False)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_card, num_labels=2, id2label={0: 'OBJ', 1: 'SUBJ'}, label2id={'OBJ': 0, 'SUBJ': 1})","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:54.860008Z","iopub.execute_input":"2025-02-28T15:39:54.860314Z","iopub.status.idle":"2025-02-28T15:39:57.927457Z","shell.execute_reply.started":"2025-02-28T15:39:54.860292Z","shell.execute_reply":"2025-02-28T15:39:57.926446Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def preprocess_text(texts):\n    return tokenizer(texts['sentence'])","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:57.929224Z","iopub.execute_input":"2025-02-28T15:39:57.929555Z","iopub.status.idle":"2025-02-28T15:39:57.934272Z","shell.execute_reply.started":"2025-02-28T15:39:57.929514Z","shell.execute_reply":"2025-02-28T15:39:57.933350Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"train_dl = Dataset.from_pandas(train_data)\ntest_dl = Dataset.from_pandas(test_data)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:57.936005Z","iopub.execute_input":"2025-02-28T15:39:57.936222Z","iopub.status.idle":"2025-02-28T15:39:57.971196Z","shell.execute_reply.started":"2025-02-28T15:39:57.936204Z","shell.execute_reply":"2025-02-28T15:39:57.970527Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"train_dl = train_dl.map(preprocess_text, batched=True)\ntest_dl = test_dl.map(preprocess_text, batched=True)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:57.972273Z","iopub.execute_input":"2025-02-28T15:39:57.972532Z","iopub.status.idle":"2025-02-28T15:39:58.210800Z","shell.execute_reply.started":"2025-02-28T15:39:57.972511Z","shell.execute_reply":"2025-02-28T15:39:58.209656Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/830 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b02231de89473783f1eeff3b975c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/484 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50ea969e22314345a52e44c47640f545"}},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:58.211812Z","iopub.execute_input":"2025-02-28T15:39:58.212103Z","iopub.status.idle":"2025-02-28T15:39:58.215877Z","shell.execute_reply.started":"2025-02-28T15:39:58.212072Z","shell.execute_reply":"2025-02-28T15:39:58.214950Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f'model',                 \n    learning_rate=6e-5,\n    per_device_train_batch_size=16,         \n    per_device_eval_batch_size=16,\n    #lr_scheduler_type='linear',\n    warmup_steps=200,\n    #label_smoothing_factor=0.1,\n    num_train_epochs=10,\n    #weight_decay=1e-1,\n    eval_strategy=\"epoch\",       \n    save_strategy=\"no\",           \n    #save_safetensors=True,\n    #load_best_model_at_end=True,\n    report_to='none',\n    seed=SEED,\n    data_seed=SEED\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:58.216754Z","iopub.execute_input":"2025-02-28T15:39:58.217036Z","iopub.status.idle":"2025-02-28T15:39:58.256026Z","shell.execute_reply.started":"2025-02-28T15:39:58.217004Z","shell.execute_reply":"2025-02-28T15:39:58.255102Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3700 (with some minor changes removing useless parts)\nclass CustomTrainer(Trainer):\n    def __init__(self, class_weights, device, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # You pass the class weights when instantiating the Trainer\n        self.class_weights = class_weights\n        self.device = device\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            # We extract the logits from the model outputs\n            logits = outputs.get('logits')\n            # We compute the loss manually passing the class weights to the loss function\n            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device)) # Modified to use the class weights\n            # We compute the loss using the modified criterion\n            loss = criterion(logits, inputs['labels'])\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:58.257566Z","iopub.execute_input":"2025-02-28T15:39:58.257902Z","iopub.status.idle":"2025-02-28T15:39:58.264121Z","shell.execute_reply.started":"2025-02-28T15:39:58.257866Z","shell.execute_reply":"2025-02-28T15:39:58.263335Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_data['label']), y=train_data['label'])\nclass_weights = torch.tensor(class_weights, dtype=torch.float32)\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:39:58.797499Z","iopub.execute_input":"2025-02-28T15:39:58.797814Z","iopub.status.idle":"2025-02-28T15:39:58.808594Z","shell.execute_reply.started":"2025-02-28T15:39:58.797781Z","shell.execute_reply":"2025-02-28T15:39:58.807667Z"},"trusted":true},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([0.7801, 1.3926])"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"def compute_metrics(output_info):\n    \"\"\"\n    Compute various evaluation metrics for model predictions.\n    \n    Args:\n        output_info (tuple): A tuple containing the model predictions and the true labels.\n            - predictions (np.ndarray): The predicted labels from the model.\n            - labels (np.ndarray): The true labels.\n    \n    Returns:\n        dict: A dictionary containing the computed metrics:\n            - 'f1': The F1 score (macro average).\n            - 'accuracy': The accuracy score.\n            - 'precision': The precision score (macro average).\n            - 'recall': The recall score (macro average).\n    \"\"\"\n    predictions, labels = output_info\n    predictions = np.array(predictions)\n    labels = np.array(labels)\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n    acc = accuracy_score(labels, predictions)\n    \n    return {\"f1-score\" : f1, \"Accuracy\" : acc}","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:40:00.265044Z","iopub.execute_input":"2025-02-28T15:40:00.265461Z","iopub.status.idle":"2025-02-28T15:40:00.272004Z","shell.execute_reply.started":"2025-02-28T15:40:00.265427Z","shell.execute_reply":"2025-02-28T15:40:00.270956Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dl,\n    eval_dataset=test_dl,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    class_weights=class_weights,\n    device=device,\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:40:00.716159Z","iopub.execute_input":"2025-02-28T15:40:00.716583Z","iopub.status.idle":"2025-02-28T15:40:01.066472Z","shell.execute_reply.started":"2025-02-28T15:40:00.716548Z","shell.execute_reply":"2025-02-28T15:40:01.065560Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-02-28T15:40:01.321393Z","iopub.execute_input":"2025-02-28T15:40:01.321741Z","iopub.status.idle":"2025-02-28T15:41:38.637725Z","shell.execute_reply.started":"2025-02-28T15:40:01.321714Z","shell.execute_reply":"2025-02-28T15:41:38.636409Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='376' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [376/520 01:36 < 00:37, 3.88 it/s, Epoch 7.21/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>F1-score</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.663418</td>\n      <td>0.427896</td>\n      <td>0.747934</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.850798</td>\n      <td>0.614496</td>\n      <td>0.785124</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.766957</td>\n      <td>0.633194</td>\n      <td>0.801653</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.711430</td>\n      <td>0.635658</td>\n      <td>0.785124</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>No log</td>\n      <td>0.517617</td>\n      <td>0.771848</td>\n      <td>0.811983</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>No log</td>\n      <td>1.504593</td>\n      <td>0.656678</td>\n      <td>0.797521</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>No log</td>\n      <td>1.756751</td>\n      <td>0.643722</td>\n      <td>0.797521</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3686\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2248\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":35},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\nprediction, labels, _ = trainer.predict(test_dl)\nprediction = np.argmax(prediction, axis=1)\ncm = confusion_matrix(y_true=labels, y_pred=prediction, normalize='all')\nprint(roc_auc_score(labels, prediction))\n\nConfusionMatrixDisplay(cm).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Emotions","metadata":{}},{"cell_type":"code","source":"model_card = \"arpanghoshal/EmoRoBERTa\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_card)\nmodel = RobertaForSequenceClassification.from_pretrained(model_card, from_tf=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa', return_all_scores= True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example\nprint(train_data.iloc[0]['sentence'], train_data.iloc[0]['label'])\nemotion_labels = emotion(train_data.iloc[0]['sentence'])\npd.DataFrame(emotion_labels[0]).sort_values(by='score', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_array = np.zeros((train_data.shape[0], 28))\n\nfor i, sentence in enumerate(tqdm(train_data['sentence'])):\n    result = emotion(sentence)[0]\n    emotion_array[i] = np.array([list(d.values())[1] for d in result])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_df_train = pd.DataFrame(emotion_array, columns=[list(d.values())[0] for d in result])\nemotion_df_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_augmented = pd.concat([train_data, emotion_df_train], axis=1)\ntrain_data_augmented.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_array = np.zeros((test_data.shape[0], 28))\n\nfor i, sentence in enumerate(tqdm(test_data['sentence'])):\n    result = emotion(sentence)[0]\n    emotion_array[i] = np.array([list(d.values())[1] for d in result])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_df_test = pd.DataFrame(emotion_array, columns=[list(d.values())[0] for d in result])\nemotion_df_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data_augmented = pd.concat([test_data, emotion_df_test], axis=1)\ntest_data_augmented.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists('/kaggle/input/clef2025-checkthat/data/english/train_en_aug.csv'):\n    train_data_augmented.to_csv('train_en_aug.csv', encoding='UTF-8')\n    test_data_augmented.to_csv('dev_test_en_aug.csv', encoding='UTF-8')\nelse:\n    train_data_augmented = pd.read_csv('/kaggle/input/clef2025-checkthat/data/english/train_en_aug.csv', encoding='UTF-8', index_col=0)\n    test_data_augmented = pd.read_csv('/kaggle/input/clef2025-checkthat/data/english/dev_test_en_aug.csv', encoding='UTF-8', index_col=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(text):\n    preprocessed_text = []\n    for t in text.split():\n        if len(t) > 1:\n            t = '@user' if t[0] == '@' and t.count('@') == 1 else t\n            t = 'http' if t.startswith('http') else t\n        preprocessed_text.append(t)\n    return ' '.join(preprocessed_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3700 (with some minor changes removing useless parts)\nclass CustomTrainer(Trainer):\n    def __init__(self, class_weights, device, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # You pass the class weights when instantiating the Trainer\n        self.class_weights = class_weights\n        self.device = device\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            # We extract the logits from the model outputs\n            logits = outputs.logits\n            # We compute the loss manually passing the class weights to the loss function\n            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device)) # Modified to use the class weights\n            # We compute the loss using the modified criterion\n            loss = criterion(logits, inputs['labels'])\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomEmotionModel(nn.Module):\n    def __init__(self, model_card: str, num_labels: int, num_emotions: int, class_weights: torch.Tensor, device):\n        super(CustomEmotionModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(model_card)\n        #self.emotion_branch = nn.Linear(num_emotions, 128)  # Example: 128 hidden units\n        self.classifier = nn.Linear(self.base_model.config.hidden_size + 28, num_labels)\n        self.class_weights = class_weights.to(device)\n\n    def forward(self, input_ids, attention_mask, emotion_features, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n        \n        # Process emotion features\n        #emotion_output = torch.relu(self.emotion_branch(emotion_features))\n        \n        # Concatenate base model output with emotion features\n        combined_output = torch.cat((pooled_output, emotion_features), dim=1)\n        \n        # Apply classification layer\n        logits = self.classifier(combined_output)\n\n        loss = None\n        if labels is not None:\n            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n            loss = criterion(logits, labels)\n\n        return SequenceClassifierOutput(loss=loss, logits=logits)\n    \n\n\"\"\"Should be something like\n    def forward(self, input_ids, attention_mask, emotion_features, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n        \n        # Process emotion features\n        emotion_output = torch.relu(self.emotion_branch(emotion_features))\n        \n        # Concatenate base model output with emotion features\n        combined_output = torch.cat((pooled_output, emotion_output), dim=1)\n        \n        # Apply dropout and classification layer\n        combined_output = self.dropout(combined_output)\n        logits = self.classifier(combined_output)\n\n        loss = None\n        if labels is not None:\n            if self.class_weights is not None:\n                criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n            else:\n                criterion = torch.nn.CrossEntropyLoss()\n            loss = criterion(logits, labels)\n\n        return SequenceClassifierOutput(loss=loss, logits=logits)\n\n    but it doesn't have the class_weight in input so it yields errors\n\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_augmented['all_emotions'] = train_data_augmented[train_data_augmented.columns[-28:]].apply(lambda x: np.array(x.values, dtype=np.float32), axis=1)\ntest_data_augmented['all_emotions'] = test_data_augmented[test_data_augmented.columns[-28:]].apply(lambda x: np.array(x.values, dtype=np.float32), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dl = Dataset.from_pandas(train_data_augmented)\ntest_dl = Dataset.from_pandas(test_data_augmented)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_data['label']), y=train_data['label'])\nclass_weights = torch.tensor(class_weights, dtype=torch.float32)\nclass_weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"cardiffnlp/twitter-roberta-base-2022-154m\"\ntokenizer = AutoTokenizer.from_pretrained(model_card, use_Fast=False)\nmodel = CustomEmotionModel(model_card, num_labels = 2, num_emotions=28, class_weights=class_weights, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_and_prepare(texts):\n    tokenized = tokenizer(texts['sentence'])\n    return {**tokenized, 'emotion_features': texts['all_emotions']}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dl = train_dl.map(tokenize_and_prepare, batched=True)\ntest_dl = test_dl.map(tokenize_and_prepare, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f'model',                 \n    learning_rate=5e-6,\n    per_device_train_batch_size=16,         \n    per_device_eval_batch_size=16,\n    lr_scheduler_type='linear',\n    label_smoothing_factor=0.1,\n    num_train_epochs=10,\n    weight_decay=1e-2,\n    eval_strategy=\"epoch\",       \n    save_strategy=\"no\",           \n    #save_safetensors=True,\n    #load_best_model_at_end=True,\n    report_to='none',\n    seed=SEED,\n    data_seed=SEED\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(output_info):\n    \"\"\"\n    Compute various evaluation metrics for model predictions.\n    \n    Args:\n        output_info (tuple): A tuple containing the model predictions and the true labels.\n            - predictions (np.ndarray): The predicted labels from the model.\n            - labels (np.ndarray): The true labels.\n    \n    Returns:\n        dict: A dictionary containing the computed metrics:\n            - 'f1': The F1 score (macro average).\n            - 'accuracy': The accuracy score.\n            - 'precision': The precision score (macro average).\n            - 'recall': The recall score (macro average).\n    \"\"\"\n    predictions, labels = output_info\n    predictions = np.array(predictions)\n    labels = np.array(labels)\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n    acc = accuracy_score(labels, predictions)\n    \n    return {\"f1-score\" : f1, \"Accuracy\" : acc}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dl,\n    eval_dataset=test_dl,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    class_weights=class_weights,\n    device=device,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\nprediction, labels, _ = trainer.predict(test_dl)\nprediction = np.argmax(prediction, axis=1)\ncm = confusion_matrix(y_true=labels, y_pred=prediction, normalize='all')\nprint(roc_auc_score(labels, prediction))\n\nConfusionMatrixDisplay(cm).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}