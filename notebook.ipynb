{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10881030,"sourceType":"datasetVersion","datasetId":6742324,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Subjectivity in News Articles\n\n## Group:\n- Luca Babboni - luca.babboni2@studio.unibo.it\n- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n\n## Description\n\nThis notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n\nThis is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n\nThe task comprises three settings:\n\n* Monolingual: train and test on data in a given language\n* Multilingual: train and test on data comprising several languages\n* Zero-shot: train on several languages and test on unseen languages\n\ntraining data in five languages:\n* Arabic\n* Bulgarian\n* English\n* German\n* Italian\n\nThe official evaluation is macro-averaged F1 between the two classes.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nimport os\nimport gc\nfrom pathlib import Path\n\nimport csv\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom joblib import delayed, Parallel\n\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import Dataset\nfrom huggingface_hub import notebook_login\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, RobertaTokenizerFast, RobertaForSequenceClassification, pipeline, get_linear_schedule_with_warmup","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:20:55.318141Z","iopub.execute_input":"2025-03-03T11:20:55.318486Z","iopub.status.idle":"2025-03-03T11:20:55.349328Z","shell.execute_reply.started":"2025-03-03T11:20:55.318452Z","shell.execute_reply":"2025-03-03T11:20:55.348537Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"SEED = 42\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:20:55.350115Z","iopub.execute_input":"2025-03-03T11:20:55.350338Z","iopub.status.idle":"2025-03-03T11:20:55.369579Z","shell.execute_reply.started":"2025-03-03T11:20:55.350316Z","shell.execute_reply":"2025-03-03T11:20:55.368844Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"np.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:20:55.374201Z","iopub.execute_input":"2025-03-03T11:20:55.374468Z","iopub.status.idle":"2025-03-03T11:20:55.389660Z","shell.execute_reply.started":"2025-03-03T11:20:55.374446Z","shell.execute_reply":"2025-03-03T11:20:55.388973Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Subjectivity:\n    def __init__(self, data_folder: str = 'data', seed: int = 42, device: str = 'cuda'):\n        self.seed = seed\n        self.device = device\n        self.languages = [language for language in os.listdir(data_folder)]\n\n        dataset = self.create_dataset(data_folder=data_folder)\n        self.dataset = dataset\n        \n        train, dev, test = self.get_splits(dataset, print_shapes=True)\n        self.train = train\n        self.dev = dev\n        self.test = test\n\n        self.all_data = self.get_per_lang_dataset()\n        \n\n    def create_dataset(self, data_folder: str = 'data'):\n        dataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n        for language in os.listdir(data_folder):\n            for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n                if '.tsv' in filename:\n                    abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n                    df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n                    if 'solved_conflict' in df.columns:\n                        df.drop(columns=['solved_conflict'], inplace=True)\n                    df['lang'] = language\n                    df['split'] = Path(filename).stem\n                    dataset = pd.concat([dataset, df], axis=0)\n        return dataset\n\n    def get_splits(self, dataset: pd.DataFrame, print_shapes: bool = True):\n        train = dataset[dataset['split'].str.contains('train')].copy()\n        dev = dataset[(dataset['split'].str.contains('dev')) & ~(dataset['split'].str.contains('dev_test'))].copy()\n        test = dataset[dataset['split'].str.contains('dev_test')].copy()\n\n        # encode the target variable to int (0: obj; 1: subj)\n        train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n        dev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n        test.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n\n        # cast to int\n        train['label'] = train['label'].astype(int)\n        dev['label'] = dev['label'].astype(int)\n        test['label'] = test['label'].astype(int)\n\n        if print_shapes:\n            print(f\"Train: {train.shape}\")\n            print(f\"Dev: {dev.shape}\")\n            print(f\"Test: {test.shape}\")\n            \n        return train, dev, test\n\n    def get_per_lang_dataset(self):\n        \"\"\"\n        dataset_dict = {\n            'english': {\n                'train': ...\n                'dev': ...\n                'test': ...\n            },\n        }\n        \"\"\"\n        dataset_dict = {}\n        for language in self.languages:\n            dataset_dict[language] = {}\n            # get the train data\n            dataset_dict[language]['train'] = self.train[self.train['lang']==language].copy()\n            # get the dev data\n            dataset_dict[language]['dev'] = self.dev[self.dev['lang']==language].copy()\n            # get the test data\n            dataset_dict[language]['test'] = self.test[self.test['lang']==language].copy()\n        return dataset_dict\n\n    def print_label_distrib(self, dataset: pd.DataFrame):\n        print(dataset['label'].value_counts(normalize=True))\n\n    def get_baseline_model(self, model_name: str = \"paraphrase-multilingual-MiniLM-L12-v2\"):\n        vect = SentenceTransformer(model_name)\n        self.vect = vect\n        return vect\n\n    def train_baseline_model(self, vect, train_data: pd.DataFrame, test_data: pd.DataFrame, solver: str = 'saga'):\n        model = LogisticRegression(class_weight=\"balanced\", solver=solver, random_state=self.seed)\n        model.fit(X=vect.encode(train_data['sentence'].values), y=train_data['label'].values)\n        predictions = model.predict(X=vect.encode(test_data['sentence'].values)).tolist()\n\n        # eval performances\n        perfs = self.evaluate_model(gold_values=test_data['label'].values, predicted_values=predictions)\n\n        return perfs\n\n    def get_tokenizer(self, model_card: str = \"microsoft/mdeberta-v3-base\"):\n        tokenizer = AutoTokenizer.from_pretrained(model_card)\n        self.tokenizer = tokenizer\n        return tokenizer\n\n    def get_model(self, model_card: str = \"microsoft/mdeberta-v3-base\", *args, **kwargs):\n        model = AutoModelForSequenceClassification.from_pretrained(model_card, *args, **kwargs)\n        self.model = model\n        return model\n\n    def get_class_weights(self, dataset: pd.DataFrame):\n        class_weights = compute_class_weight('balanced', classes=np.unique(dataset['label']), y=dataset['label'])\n        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)\n        return class_weights\n\n    def tokenize_text(self, sentences: pd.Series, max_length: int = 100):\n        input_ids = []\n        attention_masks = []\n        total_length = 0\n        i = 0\n        for sent in tqdm(sentences, desc='Tokenize of sentences'):\n            total_length += len(sent)\n            i += 1\n            encoded = self.tokenizer.encode_plus(\n                sent,\n                add_special_tokens=True,\n                max_length=max_length,\n                padding='max_length',\n                truncation=True,\n                return_attention_mask = True,\n                return_tensors = 'pt')\n\n            input_ids.append(encoded['input_ids'])\n            attention_masks.append(encoded['attention_mask'])\n\n        input_ids = torch.cat(input_ids, dim=0)\n        attention_masks = torch.cat(attention_masks, dim=0)\n        return input_ids, attention_masks\n\n    def create_dataloader(self, input_ids, attention_masks, labels, batch_size: int, shuffle: bool = True, pin_mem: bool = False):\n        dataset = TensorDataset(input_ids, attention_masks, labels)\n        return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=pin_mem)\n\n    def train_model(self, model, model_name: str, train_dl, dev_dl, criterion, optimizer, scheduler, epochs: int, device):\n\n        # Resize model token embeddings to the len of tokenizer\n        model.resize_token_embeddings(len(self.tokenizer))\n\n        # Move model to device\n        model.to(device)\n        \n        true_labels = defaultdict(list)\n        predictions = defaultdict(list)\n        val_losses = {}\n        training_stats = []\n        \n        best_val_loss = float('inf')\n        for i, epoch in enumerate(range(epochs), start=1):\n        \n            total_train_loss = 0\n            model.train()\n        \n            for batch in tqdm(train_dl, desc=f\"Epoch {i}/{epochs}\"):\n                b_input_ids, b_input_mask, b_labels = batch\n                b_input_ids = b_input_ids.to(device)\n                b_input_mask = b_input_mask.to(device)\n                b_labels = b_labels.to(device)\n        \n                model.zero_grad()\n        \n                result = model(b_input_ids,\n                attention_mask=b_input_mask,\n                labels=b_labels,\n                return_dict=True)\n        \n                logits = result.logits\n        \n                # Compute weighted loss manually\n                loss = criterion(logits, b_labels) # Use weighted loss\n                total_train_loss += loss.item()\n        \n                loss.backward()\n        \n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n                optimizer.step()\n                scheduler.step()\n                \n            avg_train_loss = total_train_loss / len(train_dl)\n        \n            model.eval()\n        \n            total_eval_loss = 0\n        \n            for batch in tqdm(dev_dl, desc=\"Evaluating\"):\n                b_input_ids, b_input_mask, b_labels = batch\n                b_input_ids = b_input_ids.to(device)\n                b_input_mask = b_input_mask.to(device)\n                b_labels = b_labels.to(device)\n        \n                with torch.no_grad():\n                    result = model(b_input_ids,\n                                token_type_ids=None,\n                                attention_mask=b_input_mask,\n                                labels=b_labels,\n                                return_dict=True)\n        \n                logits = result.logits\n        \n                loss = criterion(logits, b_labels) # Use weighted loss\n                total_eval_loss += loss.item()\n        \n                logits = logits.detach().cpu().numpy()\n                label_ids = b_labels.to('cpu').numpy()\n        \n                true_labels[i].append(label_ids)\n                predictions[i].append(logits)\n        \n            # Printing the cr\n            flat_predictions = np.concatenate(predictions[i], axis=0)\n            flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n            flat_true_labels = np.concatenate(true_labels[i], axis=0)\n            \n            stats = self.evaluate_model(flat_true_labels, flat_predictions)\n        \n            # Calculate the average loss over all of the batches.\n            avg_val_loss = total_eval_loss / len(dev_dl)\n            val_losses[i] = avg_val_loss\n        \n            print(f\"Epoch {i}/{epochs}\\tTrain Loss: {avg_train_loss:.6f}\\tVal Loss: {avg_val_loss:.6f}\\tL.rate: {optimizer.param_groups[0]['lr']:.6f}\\tVal F1: {stats['macro_F1']:.4f}\")\n            \n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                print(\"Saving model with new val loss:\", best_val_loss)\n                torch.save(model.state_dict(), model_name)\n        \n            training_stats.append(stats)\n\n        print(\"Loading the best model checkpoint\")\n        model.load_state_dict(torch.load(model_name, weights_only=True))\n\n        return model\n\n    def evaluate_model(self, gold_values, predicted_values):\n        acc = accuracy_score(gold_values, predicted_values)\n        m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(gold_values, predicted_values, average=\"macro\",\n                                                                   zero_division=0)\n        p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(gold_values, predicted_values, labels=[1],\n                                                                   zero_division=0)\n    \n        return {\n            'macro_F1': m_f1,\n            'macro_P': m_prec,\n            'macro_R': m_rec,\n            'SUBJ_F1': p_f1[0],\n            'SUBJ_P': p_prec[0],\n            'SUBJ_R': p_rec[0],\n            'accuracy': acc\n        }","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:33.299936Z","iopub.execute_input":"2025-03-03T11:23:33.300237Z","iopub.status.idle":"2025-03-03T11:23:33.327175Z","shell.execute_reply.started":"2025-03-03T11:23:33.300218Z","shell.execute_reply":"2025-03-03T11:23:33.326192Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"data_folder = '/kaggle/input/clef2025-checkthat/data' # data","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:35.398544Z","iopub.execute_input":"2025-03-03T11:23:35.398897Z","iopub.status.idle":"2025-03-03T11:23:35.402647Z","shell.execute_reply.started":"2025-03-03T11:23:35.398871Z","shell.execute_reply":"2025-03-03T11:23:35.401840Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"detector = Subjectivity(data_folder=data_folder, seed=SEED, device=device)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:35.615430Z","iopub.execute_input":"2025-03-03T11:23:35.615725Z","iopub.status.idle":"2025-03-03T11:23:35.757427Z","shell.execute_reply.started":"2025-03-03T11:23:35.615703Z","shell.execute_reply":"2025-03-03T11:23:35.756676Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Train: (6418, 5)\nDev: (2401, 5)\nTest: (2332, 5)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"detector.print_label_distrib(detector.train)\ndetector.print_label_distrib(detector.dev)\ndetector.print_label_distrib(detector.test)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:35.887031Z","iopub.execute_input":"2025-03-03T11:23:35.887330Z","iopub.status.idle":"2025-03-03T11:23:35.896485Z","shell.execute_reply.started":"2025-03-03T11:23:35.887309Z","shell.execute_reply":"2025-03-03T11:23:35.895806Z"},"trusted":true},"outputs":[{"name":"stdout","text":"label\n0    0.631349\n1    0.368651\nName: proportion, dtype: float64\nlabel\n0    0.612245\n1    0.387755\nName: proportion, dtype: float64\nlabel\n0    0.657376\n1    0.342624\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"#notebook_login()","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:36.064891Z","iopub.execute_input":"2025-03-03T11:23:36.065261Z","iopub.status.idle":"2025-03-03T11:23:36.068964Z","shell.execute_reply.started":"2025-03-03T11:23:36.065236Z","shell.execute_reply":"2025-03-03T11:23:36.067933Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Baseline Model (English)","metadata":{}},{"cell_type":"code","source":"vect = detector.get_baseline_model(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\nvect","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:20:55.777943Z","iopub.execute_input":"2025-03-03T11:20:55.778206Z","iopub.status.idle":"2025-03-03T11:21:03.351506Z","shell.execute_reply.started":"2025-03-03T11:20:55.778174Z","shell.execute_reply":"2025-03-03T11:21:03.350733Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58c51de3ee546edb0043d9475923cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4f0429b8dff4fc4b3d4ef9c15a1ed18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c1eb30425e4b0eaa51d50f170afe38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a96e14b811d47f29dd36244c5acbe43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e27b2e0b6447c88aa4cf1c97f09a48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91396e84a5c1448bb8c40099aa4b0310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4453964190224b5abbcb4fb26f6e067b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8ba4e6cc9154c4fb8e6e43be099eac3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49c89193b6ce478b80e97224baac5d8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling%2Fconfig.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a5d28ec2fd946bf9147b49236d3f6c3"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"SentenceTransformer(\n  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"detector.train_baseline_model(vect, detector.all_data['english']['train'], detector.all_data['english']['test'])","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:21:03.352359Z","iopub.execute_input":"2025-03-03T11:21:03.352682Z","iopub.status.idle":"2025-03-03T11:21:05.095164Z","shell.execute_reply.started":"2025-03-03T11:21:03.352650Z","shell.execute_reply":"2025-03-03T11:21:05.094391Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"052d18871fd745db98cb8bdcab519a8f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a7cfce5d2d474da66f97ad167cc610"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'macro_F1': 0.6328683996014854,\n 'macro_P': 0.6328683996014853,\n 'macro_R': 0.6328683996014853,\n 'SUBJ_F1': 0.4508196721311476,\n 'SUBJ_P': 0.45081967213114754,\n 'SUBJ_R': 0.45081967213114754,\n 'accuracy': 0.7231404958677686}"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Baseline Model (italian)","metadata":{}},{"cell_type":"code","source":"detector.train_baseline_model(vect, detector.all_data['italian']['train'], detector.all_data['italian']['test'])","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:21:05.095848Z","iopub.execute_input":"2025-03-03T11:21:05.096109Z","iopub.status.idle":"2025-03-03T11:21:07.109369Z","shell.execute_reply.started":"2025-03-03T11:21:05.096087Z","shell.execute_reply":"2025-03-03T11:21:07.108471Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5abc2ab7eed4c7697ab6035fc275ed2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/17 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31023941ae44a56973c5507eff51c45"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'macro_F1': 0.6320502510378404,\n 'macro_P': 0.6294871794871795,\n 'macro_R': 0.656605944765174,\n 'SUBJ_F1': 0.5015105740181269,\n 'SUBJ_P': 0.4256410256410256,\n 'SUBJ_R': 0.6102941176470589,\n 'accuracy': 0.6783625730994152}"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"# Baseline Model (multilingual)","metadata":{}},{"cell_type":"code","source":"detector.train_baseline_model(vect, detector.train, detector.test)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:21:07.110355Z","iopub.execute_input":"2025-03-03T11:21:07.110707Z","iopub.status.idle":"2025-03-03T11:21:15.701963Z","shell.execute_reply.started":"2025-03-03T11:21:07.110675Z","shell.execute_reply":"2025-03-03T11:21:15.700752Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e172521aff73455d81223d939a54743c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/73 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f8015a34ef4360a4cbd3872553882b"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'macro_F1': 0.6220011992495018,\n 'macro_P': 0.6223227143343395,\n 'macro_R': 0.633496534725811,\n 'SUBJ_F1': 0.538037486218302,\n 'SUBJ_P': 0.4807881773399015,\n 'SUBJ_R': 0.6107634543178974,\n 'accuracy': 0.6406518010291595}"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# mDeBERTta v3 base (German)","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    torch.cuda.empty_cache()\n\nif 'model' in locals() or 'model' in globals():\n    del model\n    print(\"Model deleted!\")\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:40.794112Z","iopub.execute_input":"2025-03-03T11:23:40.794414Z","iopub.status.idle":"2025-03-03T11:23:41.671171Z","shell.execute_reply.started":"2025-03-03T11:23:40.794394Z","shell.execute_reply":"2025-03-03T11:23:41.670114Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model deleted!\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"4788"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model_card = \"microsoft/mdeberta-v3-base\"\ntokenizer = detector.get_tokenizer(model_card=model_card)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:41.672336Z","iopub.execute_input":"2025-03-03T11:23:41.672576Z","iopub.status.idle":"2025-03-03T11:23:43.865696Z","shell.execute_reply.started":"2025-03-03T11:23:41.672554Z","shell.execute_reply":"2025-03-03T11:23:43.864972Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"model = detector.get_model(\n    model_card=model_card, \n    num_labels=2, \n    id2label={0: 'OBJ', 1: 'SUBJ'}, \n    label2id={'OBJ': 0, 'SUBJ': 1},\n    output_attentions = False,\n    output_hidden_states = False\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:43.872039Z","iopub.execute_input":"2025-03-03T11:23:43.872272Z","iopub.status.idle":"2025-03-03T11:23:45.157461Z","shell.execute_reply.started":"2025-03-03T11:23:43.872254Z","shell.execute_reply":"2025-03-03T11:23:45.156226Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"language = 'german'\n\nepochs = 5\nbatch_size = 16\nlr = 1e-5\nweight_decay = 0.3\nn_warmup_steps = 200\nlabel_smoothing = 0.1\n\n# Get the data\nX_train, y_train = detector.all_data[language]['train']['sentence'], detector.all_data[language]['train']['label']\nX_dev, y_dev = detector.all_data[language]['dev']['sentence'], detector.all_data[language]['dev']['label']\nX_test, y_test = detector.all_data[language]['test']['sentence'], detector.all_data[language]['test']['label']\n\n# Convert y to tensor\ny_train = torch.tensor(y_train.values)\ny_dev = torch.tensor(y_dev.values)\ny_test = torch.tensor(y_test.values)\n\n# Tokenize the text\ntrain_input_ids, train_attention_masks = detector.tokenize_text(X_train)\ndev_input_ids, dev_attention_masks = detector.tokenize_text(X_dev)\ntest_input_ids, test_attention_masks = detector.tokenize_text(X_test)\n\n# Create dataloaders\ntrain_dl = detector.create_dataloader(train_input_ids, train_attention_masks, y_train, batch_size=batch_size, shuffle=True, pin_mem=True)\ndev_dl = detector.create_dataloader(dev_input_ids, dev_attention_masks, y_dev, batch_size=batch_size, shuffle=False, pin_mem=True)\ntest_dl = detector.create_dataloader(test_input_ids, test_attention_masks, y_test, batch_size=batch_size, shuffle=False, pin_mem=False)\n\nclass_weights = detector.get_class_weights(detector.all_data[language]['train'])\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\ntotal_steps = len(train_dl) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_warmup_steps, num_training_steps=total_steps)\ncriterion = torch.nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T11:23:45.159572Z","iopub.execute_input":"2025-03-03T11:23:45.159922Z","iopub.status.idle":"2025-03-03T11:23:45.807682Z","shell.execute_reply.started":"2025-03-03T11:23:45.159888Z","shell.execute_reply":"2025-03-03T11:23:45.806860Z"}},"outputs":[{"name":"stderr","text":"Tokenize of sentences: 100%|██████████| 800/800 [00:00<00:00, 2489.46it/s]\nTokenize of sentences: 100%|██████████| 491/491 [00:00<00:00, 2821.24it/s]\nTokenize of sentences: 100%|██████████| 337/337 [00:00<00:00, 2970.74it/s]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"model = detector.train_model(model, \"mDeBERTa-base-subjectivity.pth\", train_dl, dev_dl, criterion, optimizer, scheduler, epochs, device)","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:23:45.808569Z","iopub.execute_input":"2025-03-03T11:23:45.808891Z","iopub.status.idle":"2025-03-03T11:25:16.470554Z","shell.execute_reply.started":"2025-03-03T11:23:45.808862Z","shell.execute_reply":"2025-03-03T11:25:16.469532Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 50/50 [00:12<00:00,  3.86it/s]\nEvaluating: 100%|██████████| 31/31 [00:02<00:00, 14.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\tTrain Loss: 0.706034\tVal Loss: 0.697733\tL.rate: 0.000003\tVal F1: 0.3923\nSaving model with new val loss: 0.6977327292965304\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 50/50 [00:12<00:00,  4.11it/s]\nEvaluating: 100%|██████████| 31/31 [00:02<00:00, 14.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5\tTrain Loss: 0.700585\tVal Loss: 0.694488\tL.rate: 0.000005\tVal F1: 0.4710\nSaving model with new val loss: 0.6944883369630382\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 50/50 [00:12<00:00,  4.10it/s]\nEvaluating: 100%|██████████| 31/31 [00:02<00:00, 14.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5\tTrain Loss: 0.650890\tVal Loss: 0.627306\tL.rate: 0.000008\tVal F1: 0.7340\nSaving model with new val loss: 0.6273060729426723\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 50/50 [00:12<00:00,  4.11it/s]\nEvaluating: 100%|██████████| 31/31 [00:02<00:00, 14.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5\tTrain Loss: 0.555607\tVal Loss: 0.590654\tL.rate: 0.000010\tVal F1: 0.7751\nSaving model with new val loss: 0.5906543106802048\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 50/50 [00:12<00:00,  4.11it/s]\nEvaluating: 100%|██████████| 31/31 [00:02<00:00, 14.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5\tTrain Loss: 0.489816\tVal Loss: 0.583956\tL.rate: 0.000000\tVal F1: 0.7596\nSaving model with new val loss: 0.5839562214189961\nLoading the best model checkpoint\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"results_df = pd.DataFrame(columns=['model','f1','acc','lang'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T11:25:16.472535Z","iopub.execute_input":"2025-03-03T11:25:16.472833Z","iopub.status.idle":"2025-03-03T11:25:16.478723Z","shell.execute_reply.started":"2025-03-03T11:25:16.472805Z","shell.execute_reply":"2025-03-03T11:25:16.477994Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"model.eval()\n\nlabels = []\npredictions = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_dl, desc=\"Predicting\"):\n        b_input_ids, b_input_mask, b_labels = batch\n        b_input_ids = b_input_ids.to(device)\n        b_input_mask = b_input_mask.to(device)\n        b_labels = b_labels.to(device)\n\n        result = model(b_input_ids,\n                        token_type_ids=None,\n                        attention_mask=b_input_mask,\n                        labels=b_labels,\n                        return_dict=True)\n\n        logits = result.logits\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        labels.extend(label_ids)\n        logits = np.argmax(logits, axis=1)\n        predictions.extend(logits)\n    \n    labels = np.array(labels)\n    predictions = np.array(predictions)\n\n    results_df.loc[len(results_df)] = [model_card, f1_macro, acc, language]\n\n    print(detector.evaluate_model(gold_values=labels, predicted_values=predictions))\n    \n    ConfusionMatrixDisplay.from_predictions(labels, predictions, normalize='all')\n    plt.title(f\"Confusion Matrix ({language})\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:27:36.627429Z","iopub.execute_input":"2025-03-03T11:27:36.627786Z","iopub.status.idle":"2025-03-03T11:27:38.218135Z","shell.execute_reply.started":"2025-03-03T11:27:36.627742Z","shell.execute_reply":"2025-03-03T11:27:38.217187Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Predicting: 100%|██████████| 22/22 [00:01<00:00, 15.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"{'macro_F1': 0.8130968346963265, 'macro_P': 0.8171404067630483, 'macro_R': 0.8095551303515905, 'SUBJ_F1': 0.7465437788018433, 'SUBJ_P': 0.7641509433962265, 'SUBJ_R': 0.7297297297297297, 'accuracy': 0.8367952522255193}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"results_df","metadata":{"execution":{"iopub.status.busy":"2025-03-03T11:27:45.992349Z","iopub.execute_input":"2025-03-03T11:27:45.992651Z","iopub.status.idle":"2025-03-03T11:27:46.010556Z","shell.execute_reply.started":"2025-03-03T11:27:45.992629Z","shell.execute_reply":"2025-03-03T11:27:46.009790Z"},"trusted":true},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"                        model        f1       acc    lang\n0  microsoft/mdeberta-v3-base  0.813097  0.836795  german\n1  microsoft/mdeberta-v3-base  0.813097  0.836795  german\n2  microsoft/mdeberta-v3-base  0.813097  0.836795  german\n3  microsoft/mdeberta-v3-base  0.813097  0.836795  german","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>f1</th>\n      <th>acc</th>\n      <th>lang</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>microsoft/mdeberta-v3-base</td>\n      <td>0.813097</td>\n      <td>0.836795</td>\n      <td>german</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>microsoft/mdeberta-v3-base</td>\n      <td>0.813097</td>\n      <td>0.836795</td>\n      <td>german</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>microsoft/mdeberta-v3-base</td>\n      <td>0.813097</td>\n      <td>0.836795</td>\n      <td>german</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>microsoft/mdeberta-v3-base</td>\n      <td>0.813097</td>\n      <td>0.836795</td>\n      <td>german</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":43},{"cell_type":"markdown","source":"# Emotions","metadata":{}},{"cell_type":"code","source":"model_card = \"arpanghoshal/EmoRoBERTa\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_card)\nmodel = RobertaForSequenceClassification.from_pretrained(model_card, from_tf=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa', return_all_scores= True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example\nprint(train_data.iloc[0]['sentence'], train_data.iloc[0]['label'])\nemotion_labels = emotion(train_data.iloc[0]['sentence'])\npd.DataFrame(emotion_labels[0]).sort_values(by='score', ascending=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_array = np.zeros((train_data.shape[0], 28))\n\nfor i, sentence in enumerate(tqdm(train_data['sentence'])):\n    result = emotion(sentence)[0]\n    emotion_array[i] = np.array([list(d.values())[1] for d in result])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_df_train = pd.DataFrame(emotion_array, columns=[list(d.values())[0] for d in result])\nemotion_df_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_augmented = pd.concat([train_data, emotion_df_train], axis=1)\ntrain_data_augmented.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_array = np.zeros((test_data.shape[0], 28))\n\nfor i, sentence in enumerate(tqdm(test_data['sentence'])):\n    result = emotion(sentence)[0]\n    emotion_array[i] = np.array([list(d.values())[1] for d in result])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"emotion_df_test = pd.DataFrame(emotion_array, columns=[list(d.values())[0] for d in result])\nemotion_df_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data_augmented = pd.concat([test_data, emotion_df_test], axis=1)\ntest_data_augmented.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not os.path.exists('/kaggle/input/clef2025-checkthat/data/english/train_en_aug.csv'):\n    train_data_augmented.to_csv('train_en_aug.csv', encoding='UTF-8')\n    test_data_augmented.to_csv('dev_test_en_aug.csv', encoding='UTF-8')\nelse:\n    train_data_augmented = pd.read_csv('/kaggle/input/clef2025-checkthat/data/english/train_en_aug.csv', encoding='UTF-8', index_col=0)\n    test_data_augmented = pd.read_csv('/kaggle/input/clef2025-checkthat/data/english/dev_test_en_aug.csv', encoding='UTF-8', index_col=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(text):\n    preprocessed_text = []\n    for t in text.split():\n        if len(t) > 1:\n            t = '@user' if t[0] == '@' and t.count('@') == 1 else t\n            t = 'http' if t.startswith('http') else t\n        preprocessed_text.append(t)\n    return ' '.join(preprocessed_text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3700 (with some minor changes removing useless parts)\nclass CustomTrainer(Trainer):\n    def __init__(self, class_weights, device, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # You pass the class weights when instantiating the Trainer\n        self.class_weights = class_weights\n        self.device = device\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        if self.label_smoother is not None and \"labels\" in inputs:\n            labels = inputs.pop(\"labels\")\n        else:\n            labels = None\n        outputs = model(**inputs)\n        if self.args.past_index >= 0:\n            self._past = outputs[self.args.past_index]\n\n        if labels is not None:\n            loss = self.label_smoother(outputs, labels)\n        else:\n            # We extract the logits from the model outputs\n            logits = outputs.logits\n            # We compute the loss manually passing the class weights to the loss function\n            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device)) # Modified to use the class weights\n            # We compute the loss using the modified criterion\n            loss = criterion(logits, inputs['labels'])\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomEmotionModel(nn.Module):\n    def __init__(self, model_card: str, num_labels: int, num_emotions: int, class_weights: torch.Tensor, device):\n        super(CustomEmotionModel, self).__init__()\n        self.base_model = AutoModel.from_pretrained(model_card)\n        #self.emotion_branch = nn.Linear(num_emotions, 128)  # Example: 128 hidden units\n        self.classifier = nn.Linear(self.base_model.config.hidden_size + 28, num_labels)\n        self.class_weights = class_weights.to(device)\n\n    def forward(self, input_ids, attention_mask, emotion_features, labels=None):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n        \n        # Process emotion features\n        #emotion_output = torch.relu(self.emotion_branch(emotion_features))\n        \n        # Concatenate base model output with emotion features\n        combined_output = torch.cat((pooled_output, emotion_features), dim=1)\n        \n        # Apply classification layer\n        logits = self.classifier(combined_output)\n\n        loss = None\n        if labels is not None:\n            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n            loss = criterion(logits, labels)\n\n        return SequenceClassifierOutput(loss=loss, logits=logits)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data_augmented['all_emotions'] = train_data_augmented[train_data_augmented.columns[-28:]].apply(lambda x: np.array(x.values, dtype=np.float32), axis=1)\ntest_data_augmented['all_emotions'] = test_data_augmented[test_data_augmented.columns[-28:]].apply(lambda x: np.array(x.values, dtype=np.float32), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dl = Dataset.from_pandas(train_data_augmented)\ntest_dl = Dataset.from_pandas(test_data_augmented)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_data['label']), y=train_data['label'])\nclass_weights = torch.tensor(class_weights, dtype=torch.float32)\nclass_weights","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = \"cardiffnlp/twitter-roberta-base-2022-154m\"\ntokenizer = AutoTokenizer.from_pretrained(model_card, use_Fast=False)\nmodel = CustomEmotionModel(model_card, num_labels = 2, num_emotions=28, class_weights=class_weights, device=device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_and_prepare(texts):\n    tokenized = tokenizer(texts['sentence'])\n    return {**tokenized, 'emotion_features': texts['all_emotions']}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dl = train_dl.map(tokenize_and_prepare, batched=True)\ntest_dl = test_dl.map(tokenize_and_prepare, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=f'model',                 \n    learning_rate=5e-6,\n    per_device_train_batch_size=16,         \n    per_device_eval_batch_size=16,\n    lr_scheduler_type='linear',\n    label_smoothing_factor=0.1,\n    num_train_epochs=10,\n    weight_decay=1e-2,\n    eval_strategy=\"epoch\",       \n    save_strategy=\"no\",           \n    #save_safetensors=True,\n    #load_best_model_at_end=True,\n    report_to='none',\n    seed=SEED,\n    data_seed=SEED\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(output_info):\n    \"\"\"\n    Compute various evaluation metrics for model predictions.\n    \n    Args:\n        output_info (tuple): A tuple containing the model predictions and the true labels.\n            - predictions (np.ndarray): The predicted labels from the model.\n            - labels (np.ndarray): The true labels.\n    \n    Returns:\n        dict: A dictionary containing the computed metrics:\n            - 'f1': The F1 score (macro average).\n            - 'accuracy': The accuracy score.\n            - 'precision': The precision score (macro average).\n            - 'recall': The recall score (macro average).\n    \"\"\"\n    predictions, labels = output_info\n    predictions = np.array(predictions)\n    labels = np.array(labels)\n    predictions = np.argmax(predictions, axis=-1)\n    \n    f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n    acc = accuracy_score(labels, predictions)\n    \n    return {\"f1-score\" : f1, \"Accuracy\" : acc}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dl,\n    eval_dataset=test_dl,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    class_weights=class_weights,\n    device=device,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\nprediction, labels, _ = trainer.predict(test_dl)\nprediction = np.argmax(prediction, axis=1)\ncm = confusion_matrix(y_true=labels, y_pred=prediction, normalize='all')\nprint(roc_auc_score(labels, prediction))\n\nConfusionMatrixDisplay(cm).plot()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    epochs = trial.suggest_int('epochs', 3, 5)\n    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n    lr = trial.suggest_categorical('lr', [2e-5, 3e-5, 4e-5, 5e-5, 6e-5])\n    weight_decay = trial.suggest_categorical('weight_decay', [0, 1e-1, 2e-1, 3e-1, 4e-1, 5e-1])\n    n_warmup_steps = trial.suggest_categorical('n_warmup_steps', [100, 200, 300, 400, 500])\n\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_card, \n        num_labels=2, \n        id2label={0: 'OBJ', 1: 'SUBJ'}, \n        label2id={'OBJ': 0, 'SUBJ': 1},\n        output_attentions = False,\n        output_hidden_states = False\n    )\n    model.to(device)\n    \n    train_dl = create_dataloader(\n        train_input_ids, \n        train_attention_masks, \n        torch.tensor(train['label'].values), \n        batch_size=batch_size,\n        shuffle=True,\n        pin_mem=True\n    )\n    dev_dl = create_dataloader(\n        dev_input_ids, \n        dev_attention_masks, \n        torch.tensor(dev['label'].values), \n        batch_size=batch_size,\n        shuffle=False,\n        pin_mem=True\n    )\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    total_steps = len(train_dl) * epochs\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=n_warmup_steps, num_training_steps=total_steps)\n    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n    \n    true_labels = defaultdict(list)\n    predictions = defaultdict(list)\n    val_losses = {}\n    training_stats = []\n    \n    best_val_loss = float('inf')\n    for i, epoch in enumerate(range(epochs), start=1):\n    \n        total_train_loss = 0\n        model.train()\n    \n        for batch in tqdm(train_dl, desc=f\"Epoch {i}/{epochs}\"):\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n    \n            model.zero_grad()\n    \n            result = model(b_input_ids,\n              attention_mask=b_input_mask,\n              labels=b_labels,\n              return_dict=True)\n    \n            logits = result.logits\n    \n            # Compute weighted loss manually\n            loss = criterion(logits, b_labels) # Use weighted loss\n            total_train_loss += loss.item()\n    \n            loss.backward()\n    \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n            optimizer.step()\n            scheduler.step()\n            \n        avg_train_loss = total_train_loss / len(train_dl)\n    \n        model.eval()\n    \n        total_eval_loss = 0\n    \n        for batch in tqdm(dev_dl, desc=\"Evaluating\"):\n            b_input_ids, b_input_mask, b_labels = batch\n            b_input_ids = b_input_ids.to(device)\n            b_input_mask = b_input_mask.to(device)\n            b_labels = b_labels.to(device)\n    \n            with torch.no_grad():\n                result = model(b_input_ids,\n                              token_type_ids=None,\n                              attention_mask=b_input_mask,\n                              labels=b_labels,\n                              return_dict=True)\n    \n            logits = result.logits\n    \n            loss = criterion(logits, b_labels) # Use weighted loss\n            total_eval_loss += loss.item()\n    \n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n    \n            true_labels[i].append(label_ids)\n            predictions[i].append(logits)\n    \n        # Printing the cr\n        flat_predictions = np.concatenate(predictions[i], axis=0)\n        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n        flat_true_labels = np.concatenate(true_labels[i], axis=0)\n        \n        stats = evaluate_model(flat_true_labels, flat_predictions)\n    \n        # Calculate the average loss over all of the batches.\n        avg_val_loss = total_eval_loss / len(dev_dl)\n        val_losses[i] = avg_val_loss\n    \n        print(f\"Epoch {i}/{epochs}\\tTrain Loss: {avg_train_loss:.6f}\\tVal Loss: {avg_val_loss:.6f}\\tL.rate: {optimizer.param_groups[0]['lr']:.6f}\\tVal F1: {stats['macro_F1']:.4f}\")\n        \n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            print(\"Saving model with new val loss:\", best_val_loss)\n            torch.save(model.state_dict(), \"mDeBERTa-base-subjectivity\")\n    \n        training_stats.append(stats)\n\n    return best_val_loss","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=25)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null}]}