{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subjectivity in News Articles\n",
    "\n",
    "## Group:\n",
    "- Luca Babboni - luca.babboni2@studio.unibo.it\n",
    "- Matteo Fasulo - matteo.fasulo@studio.unibo.it\n",
    "- Luca Tedeschini - luca.tedeschini3@studio.unibo.it\n",
    "\n",
    "## Description\n",
    "\n",
    "This notebook addresses Task 1 proposed in [CheckThat Lab](https://checkthat.gitlab.io/clef2025/) of CLEF 2025. In this task, systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author behind it or presents an objective view on the covered topic instead.\n",
    "\n",
    "This is a binary classification tasks in which systems have to identify whether a text sequence (a sentence or a paragraph) is subjective (SUBJ) or objective (OBJ).\n",
    "\n",
    "The task comprises three settings:\n",
    "\n",
    "* Monolingual: train and test on data in a given language\n",
    "* Multilingual: train and test on data comprising several languages\n",
    "* Zero-shot: train on several languages and test on unseen languages\n",
    "\n",
    "training data in five languages:\n",
    "* Arabic\n",
    "* Bulgarian\n",
    "* English\n",
    "* German\n",
    "* Italian\n",
    "\n",
    "The official evaluation is macro-averaged F1 between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T16:01:04.408660Z",
     "iopub.status.busy": "2025-02-28T16:01:04.408289Z",
     "iopub.status.idle": "2025-02-28T16:01:04.414929Z",
     "shell.execute_reply": "2025-02-28T16:01:04.413577Z",
     "shell.execute_reply.started": "2025-02-28T16:01:04.408633Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, RobertaTokenizerFast, RobertaForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:55:09.264582Z",
     "iopub.status.busy": "2025-02-28T15:55:09.263803Z",
     "iopub.status.idle": "2025-02-28T15:55:09.272815Z",
     "shell.execute_reply": "2025-02-28T15:55:09.271395Z",
     "shell.execute_reply.started": "2025-02-28T15:55:09.264553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T16:02:22.549291Z",
     "iopub.status.busy": "2025-02-28T16:02:22.548934Z",
     "iopub.status.idle": "2025-02-28T16:02:22.588677Z",
     "shell.execute_reply": "2025-02-28T16:02:22.587275Z",
     "shell.execute_reply.started": "2025-02-28T16:02:22.549265Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_folder = 'data'\n",
    "dataset = pd.DataFrame(columns=['sentence_id','sentence','label','lang','split'])\n",
    "\n",
    "for language in os.listdir(data_folder):\n",
    "    for filename in os.listdir(f\"{data_folder}{os.sep}{language}\"):\n",
    "        if '.tsv' in filename:\n",
    "            abs_path = f\"{data_folder}{os.sep}{language}{os.sep}{filename}\"\n",
    "            df = pd.read_csv(abs_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "            if 'solved_conflict' in df.columns:\n",
    "                df.drop(columns=['solved_conflict'], inplace=True)\n",
    "            df['lang'] = language\n",
    "            df['split'] = Path(filename).stem\n",
    "            dataset = pd.concat([dataset, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6418, 5)\n",
      "Dev: (4733, 5)\n",
      "Test: (2332, 5)\n"
     ]
    }
   ],
   "source": [
    "train = dataset[dataset['split'].str.contains('train')]\n",
    "dev = dataset[dataset['split'].str.contains('dev')]\n",
    "test = dataset[dataset['split'].str.contains('dev_test')]\n",
    "\n",
    "print(f\"Train: {train.shape}\")\n",
    "print(f\"Dev: {dev.shape}\")\n",
    "print(f\"Test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:49.222937Z",
     "iopub.status.busy": "2025-02-28T15:39:49.222609Z",
     "iopub.status.idle": "2025-02-28T15:39:49.230520Z",
     "shell.execute_reply": "2025-02-28T15:39:49.229743Z",
     "shell.execute_reply.started": "2025-02-28T15:39:49.222913Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: label\n",
      "OBJ     0.631349\n",
      "SUBJ    0.368651\n",
      "Name: proportion, dtype: float64\n",
      "Dev: label\n",
      "OBJ     0.634481\n",
      "SUBJ    0.365519\n",
      "Name: proportion, dtype: float64\n",
      "Test: label\n",
      "OBJ     0.657376\n",
      "SUBJ    0.342624\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {train['label'].value_counts(normalize=True)}\")\n",
    "print(f\"Dev: {dev['label'].value_counts(normalize=True)}\")\n",
    "print(f\"Test: {test['label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legend:\n",
    "* Objective -> 0\n",
    "* Subjective -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:51.059556Z",
     "iopub.status.busy": "2025-02-28T15:39:51.059200Z",
     "iopub.status.idle": "2025-02-28T15:39:51.065554Z",
     "shell.execute_reply": "2025-02-28T15:39:51.064499Z",
     "shell.execute_reply.started": "2025-02-28T15:39:51.059526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.loc[:, 'label'] = train['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "dev.loc[:, 'label'] = dev['label'].apply(lambda x: 0 if x == 'OBJ' else 1)\n",
    "test.loc[:, 'label'] = test['label'].apply(lambda x: 0 if x == 'OBJ' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vect = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight=\"balanced\", random_state=SEED)\n",
    "model.fit(X=vect.encode(train['sentence'].values), y=train['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X=vect.encode(test['sentence'].values)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "pred_df['sentence_id'] = test['sentence_id']\n",
    "pred_df['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(gold_values, predicted_values):\n",
    "    acc = accuracy_score(gold_values, predicted_values)\n",
    "    m_prec, m_rec, m_f1, m_s = precision_recall_fscore_support(gold_values, predicted_values, average=\"macro\",\n",
    "                                                               zero_division=0)\n",
    "    p_prec, p_rec, p_f1, p_s = precision_recall_fscore_support(gold_values, predicted_values, labels=[1],\n",
    "                                                               zero_division=0)\n",
    "    #roc_auc = roc_auc_score(gold_values, predicted_probabilities)\n",
    "\n",
    "    return {\n",
    "        'macro_F1': m_f1,\n",
    "        'macro_P': m_prec,\n",
    "        'macro_R': m_rec,\n",
    "        'SUBJ_F1': p_f1[0],\n",
    "        'SUBJ_P': p_prec[0],\n",
    "        'SUBJ_R': p_rec[0],\n",
    "        'accuracy': acc,\n",
    "        #'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "evaluate_model(gold_values=test.label.values, predicted_values=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter RoBERTa-base 2022 154M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:54.860314Z",
     "iopub.status.busy": "2025-02-28T15:39:54.860008Z",
     "iopub.status.idle": "2025-02-28T15:39:57.927457Z",
     "shell.execute_reply": "2025-02-28T15:39:57.926446Z",
     "shell.execute_reply.started": "2025-02-28T15:39:54.860292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_card = \"microsoft/mdeberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card, use_Fast=False)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_card, num_labels=2, id2label={0: 'OBJ', 1: 'SUBJ'}, label2id={'OBJ': 0, 'SUBJ': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:57.929555Z",
     "iopub.status.busy": "2025-02-28T15:39:57.929224Z",
     "iopub.status.idle": "2025-02-28T15:39:57.934272Z",
     "shell.execute_reply": "2025-02-28T15:39:57.933350Z",
     "shell.execute_reply.started": "2025-02-28T15:39:57.929514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    return tokenizer(texts['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:57.936222Z",
     "iopub.status.busy": "2025-02-28T15:39:57.936005Z",
     "iopub.status.idle": "2025-02-28T15:39:57.971196Z",
     "shell.execute_reply": "2025-02-28T15:39:57.970527Z",
     "shell.execute_reply.started": "2025-02-28T15:39:57.936204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl = Dataset.from_pandas(train)\n",
    "dev_dl = Dataset.from_pandas(dev)\n",
    "test_dl = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:57.972532Z",
     "iopub.status.busy": "2025-02-28T15:39:57.972273Z",
     "iopub.status.idle": "2025-02-28T15:39:58.210800Z",
     "shell.execute_reply": "2025-02-28T15:39:58.209656Z",
     "shell.execute_reply.started": "2025-02-28T15:39:57.972511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b02231de89473783f1eeff3b975c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/830 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ea969e22314345a52e44c47640f545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dl = train_dl.map(preprocess_text, batched=True)\n",
    "dev_dl = dev_dl.map(preprocess_text, batched=True)\n",
    "test_dl = test_dl.map(preprocess_text, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:58.212103Z",
     "iopub.status.busy": "2025-02-28T15:39:58.211812Z",
     "iopub.status.idle": "2025-02-28T15:39:58.215877Z",
     "shell.execute_reply": "2025-02-28T15:39:58.214950Z",
     "shell.execute_reply.started": "2025-02-28T15:39:58.212072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:58.217036Z",
     "iopub.status.busy": "2025-02-28T15:39:58.216754Z",
     "iopub.status.idle": "2025-02-28T15:39:58.256026Z",
     "shell.execute_reply": "2025-02-28T15:39:58.255102Z",
     "shell.execute_reply.started": "2025-02-28T15:39:58.217004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'model',                 \n",
    "    learning_rate=6e-5,\n",
    "    per_device_train_batch_size=16,         \n",
    "    per_device_eval_batch_size=16,\n",
    "    #lr_scheduler_type='linear',\n",
    "    warmup_steps=200,\n",
    "    #label_smoothing_factor=0.1,\n",
    "    num_train_epochs=10,\n",
    "    #weight_decay=1e-1,\n",
    "    eval_strategy=\"epoch\",       \n",
    "    save_strategy=\"no\",           \n",
    "    #save_safetensors=True,\n",
    "    #load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    seed=SEED,\n",
    "    data_seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:58.257902Z",
     "iopub.status.busy": "2025-02-28T15:39:58.257566Z",
     "iopub.status.idle": "2025-02-28T15:39:58.264121Z",
     "shell.execute_reply": "2025-02-28T15:39:58.263335Z",
     "shell.execute_reply.started": "2025-02-28T15:39:58.257866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3700 (with some minor changes removing useless parts)\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights, device, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # You pass the class weights when instantiating the Trainer\n",
    "        self.class_weights = class_weights\n",
    "        self.device = device\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We extract the logits from the model outputs\n",
    "            logits = outputs.get('logits')\n",
    "            # We compute the loss manually passing the class weights to the loss function\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device)) # Modified to use the class weights\n",
    "            # We compute the loss using the modified criterion\n",
    "            loss = criterion(logits, inputs['labels'])\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:39:58.797814Z",
     "iopub.status.busy": "2025-02-28T15:39:58.797499Z",
     "iopub.status.idle": "2025-02-28T15:39:58.808594Z",
     "shell.execute_reply": "2025-02-28T15:39:58.807667Z",
     "shell.execute_reply.started": "2025-02-28T15:39:58.797781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7801, 1.3926])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train['label']), y=train['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:40:00.265461Z",
     "iopub.status.busy": "2025-02-28T15:40:00.265044Z",
     "iopub.status.idle": "2025-02-28T15:40:00.272004Z",
     "shell.execute_reply": "2025-02-28T15:40:00.270956Z",
     "shell.execute_reply.started": "2025-02-28T15:40:00.265427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(output_info):\n",
    "    \"\"\"\n",
    "    Compute various evaluation metrics for model predictions.\n",
    "    \n",
    "    Args:\n",
    "        output_info (tuple): A tuple containing the model predictions and the true labels.\n",
    "            - predictions (np.ndarray): The predicted labels from the model.\n",
    "            - labels (np.ndarray): The true labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed metrics:\n",
    "            - 'f1': The F1 score (macro average).\n",
    "            - 'accuracy': The accuracy score.\n",
    "            - 'precision': The precision score (macro average).\n",
    "            - 'recall': The recall score (macro average).\n",
    "    \"\"\"\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"f1-score\" : f1, \"Accuracy\" : acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:40:00.716583Z",
     "iopub.status.busy": "2025-02-28T15:40:00.716159Z",
     "iopub.status.idle": "2025-02-28T15:40:01.066472Z",
     "shell.execute_reply": "2025-02-28T15:40:01.065560Z",
     "shell.execute_reply.started": "2025-02-28T15:40:00.716548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dl,\n",
    "    eval_dataset=dev_dl,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-28T15:40:01.321741Z",
     "iopub.status.busy": "2025-02-28T15:40:01.321393Z",
     "iopub.status.idle": "2025-02-28T15:41:38.637725Z",
     "shell.execute_reply": "2025-02-28T15:41:38.636409Z",
     "shell.execute_reply.started": "2025-02-28T15:40:01.321714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/520 01:36 < 00:37, 3.88 it/s, Epoch 7.21/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.663418</td>\n",
       "      <td>0.427896</td>\n",
       "      <td>0.747934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.850798</td>\n",
       "      <td>0.614496</td>\n",
       "      <td>0.785124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.766957</td>\n",
       "      <td>0.633194</td>\n",
       "      <td>0.801653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.711430</td>\n",
       "      <td>0.635658</td>\n",
       "      <td>0.785124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.771848</td>\n",
       "      <td>0.811983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.504593</td>\n",
       "      <td>0.656678</td>\n",
       "      <td>0.797521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.756751</td>\n",
       "      <td>0.643722</td>\n",
       "      <td>0.797521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2520\u001b[0m                     )\n\u001b[1;32m   2521\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3686\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m             \u001b[0;31m# Finally we need to normalize the loss for reporting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2248\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prediction, labels, _ = trainer.predict(test_dl)\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "cm = confusion_matrix(y_true=labels, y_pred=prediction, normalize='all')\n",
    "print(roc_auc_score(labels, prediction))\n",
    "\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_card = \"arpanghoshal/EmoRoBERTa\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_card)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_card, from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion = pipeline('sentiment-analysis', model='arpanghoshal/EmoRoBERTa', return_all_scores= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Example\n",
    "print(train_data.iloc[0]['sentence'], train_data.iloc[0]['label'])\n",
    "emotion_labels = emotion(train_data.iloc[0]['sentence'])\n",
    "pd.DataFrame(emotion_labels[0]).sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_array = np.zeros((train_data.shape[0], 28))\n",
    "\n",
    "for i, sentence in enumerate(tqdm(train_data['sentence'])):\n",
    "    result = emotion(sentence)[0]\n",
    "    emotion_array[i] = np.array([list(d.values())[1] for d in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_df_train = pd.DataFrame(emotion_array, columns=[list(d.values())[0] for d in result])\n",
    "emotion_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_augmented = pd.concat([train_data, emotion_df_train], axis=1)\n",
    "train_data_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_array = np.zeros((test_data.shape[0], 28))\n",
    "\n",
    "for i, sentence in enumerate(tqdm(test_data['sentence'])):\n",
    "    result = emotion(sentence)[0]\n",
    "    emotion_array[i] = np.array([list(d.values())[1] for d in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_df_test = pd.DataFrame(emotion_array, columns=[list(d.values())[0] for d in result])\n",
    "emotion_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data_augmented = pd.concat([test_data, emotion_df_test], axis=1)\n",
    "test_data_augmented.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('/kaggle/input/clef2025-checkthat/data/english/train_en_aug.csv'):\n",
    "    train_data_augmented.to_csv('train_en_aug.csv', encoding='UTF-8')\n",
    "    test_data_augmented.to_csv('dev_test_en_aug.csv', encoding='UTF-8')\n",
    "else:\n",
    "    train_data_augmented = pd.read_csv('/kaggle/input/clef2025-checkthat/data/english/train_en_aug.csv', encoding='UTF-8', index_col=0)\n",
    "    test_data_augmented = pd.read_csv('/kaggle/input/clef2025-checkthat/data/english/dev_test_en_aug.csv', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    preprocessed_text = []\n",
    "    for t in text.split():\n",
    "        if len(t) > 1:\n",
    "            t = '@user' if t[0] == '@' and t.count('@') == 1 else t\n",
    "            t = 'http' if t.startswith('http') else t\n",
    "        preprocessed_text.append(t)\n",
    "    return ' '.join(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/trainer.py#L3700 (with some minor changes removing useless parts)\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, class_weights, device, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # You pass the class weights when instantiating the Trainer\n",
    "        self.class_weights = class_weights\n",
    "        self.device = device\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We extract the logits from the model outputs\n",
    "            logits = outputs.logits\n",
    "            # We compute the loss manually passing the class weights to the loss function\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(self.device)) # Modified to use the class weights\n",
    "            # We compute the loss using the modified criterion\n",
    "            loss = criterion(logits, inputs['labels'])\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomEmotionModel(nn.Module):\n",
    "    def __init__(self, model_card: str, num_labels: int, num_emotions: int, class_weights: torch.Tensor, device):\n",
    "        super(CustomEmotionModel, self).__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_card)\n",
    "        #self.emotion_branch = nn.Linear(num_emotions, 128)  # Example: 128 hidden units\n",
    "        self.classifier = nn.Linear(self.base_model.config.hidden_size + 28, num_labels)\n",
    "        self.class_weights = class_weights.to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, emotion_features, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        \n",
    "        # Process emotion features\n",
    "        #emotion_output = torch.relu(self.emotion_branch(emotion_features))\n",
    "        \n",
    "        # Concatenate base model output with emotion features\n",
    "        combined_output = torch.cat((pooled_output, emotion_features), dim=1)\n",
    "        \n",
    "        # Apply classification layer\n",
    "        logits = self.classifier(combined_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "    \n",
    "\n",
    "\"\"\"Should be something like\n",
    "    def forward(self, input_ids, attention_mask, emotion_features, labels=None):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        \n",
    "        # Process emotion features\n",
    "        emotion_output = torch.relu(self.emotion_branch(emotion_features))\n",
    "        \n",
    "        # Concatenate base model output with emotion features\n",
    "        combined_output = torch.cat((pooled_output, emotion_output), dim=1)\n",
    "        \n",
    "        # Apply dropout and classification layer\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        logits = self.classifier(combined_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.class_weights is not None:\n",
    "                criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            else:\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "    but it doesn't have the class_weight in input so it yields errors\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_augmented['all_emotions'] = train_data_augmented[train_data_augmented.columns[-28:]].apply(lambda x: np.array(x.values, dtype=np.float32), axis=1)\n",
    "test_data_augmented['all_emotions'] = test_data_augmented[test_data_augmented.columns[-28:]].apply(lambda x: np.array(x.values, dtype=np.float32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl = Dataset.from_pandas(train_data_augmented)\n",
    "test_dl = Dataset.from_pandas(test_data_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_data['label']), y=train_data['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_card = \"cardiffnlp/twitter-roberta-base-2022-154m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card, use_Fast=False)\n",
    "model = CustomEmotionModel(model_card, num_labels = 2, num_emotions=28, class_weights=class_weights, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_prepare(texts):\n",
    "    tokenized = tokenizer(texts['sentence'])\n",
    "    return {**tokenized, 'emotion_features': texts['all_emotions']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl = train_dl.map(tokenize_and_prepare, batched=True)\n",
    "test_dl = test_dl.map(tokenize_and_prepare, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f'model',                 \n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=16,         \n",
    "    per_device_eval_batch_size=16,\n",
    "    lr_scheduler_type='linear',\n",
    "    label_smoothing_factor=0.1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=1e-2,\n",
    "    eval_strategy=\"epoch\",       \n",
    "    save_strategy=\"no\",           \n",
    "    #save_safetensors=True,\n",
    "    #load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    seed=SEED,\n",
    "    data_seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(output_info):\n",
    "    \"\"\"\n",
    "    Compute various evaluation metrics for model predictions.\n",
    "    \n",
    "    Args:\n",
    "        output_info (tuple): A tuple containing the model predictions and the true labels.\n",
    "            - predictions (np.ndarray): The predicted labels from the model.\n",
    "            - labels (np.ndarray): The true labels.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed metrics:\n",
    "            - 'f1': The F1 score (macro average).\n",
    "            - 'accuracy': The accuracy score.\n",
    "            - 'precision': The precision score (macro average).\n",
    "            - 'recall': The recall score (macro average).\n",
    "    \"\"\"\n",
    "    predictions, labels = output_info\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    f1 = f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\"f1-score\" : f1, \"Accuracy\" : acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dl,\n",
    "    eval_dataset=test_dl,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "prediction, labels, _ = trainer.predict(test_dl)\n",
    "prediction = np.argmax(prediction, axis=1)\n",
    "cm = confusion_matrix(y_true=labels, y_pred=prediction, normalize='all')\n",
    "print(roc_auc_score(labels, prediction))\n",
    "\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11247848,
     "datasetId": 6742324,
     "isSourceIdPinned": false,
     "sourceId": 10881030,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
