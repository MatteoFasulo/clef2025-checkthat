# ğŸ“Œ Fine-tuning mDeBERTaV3 & ModernBERT for Subjectivity Detection (Final Version)

## CLEFâ€¯2025 CheckThat! Lab â€“ Task 1

Detecting **subjectivity**â€”distinguishing subjective vs. objective sentencesâ€”is crucial for tasks like **fake-news detection** and **fact-checking**. Our final system supports five languages: **Arabic, German, English, Italian, and Bulgarian**.

---

## ğŸ” Approaches

### 1. **BERT-Like Models**

* **mDeBERTaV3â€‘base** ([Hugging Face link](https://huggingface.co/microsoft/mdeberta-v3-base)) for multilingual support.
* **ModernBERTâ€‘base** ([Hugging Face link](https://huggingface.co/answerdotai/ModernBERT-base)) specifically for English language given the results on NLU tasks comparable with DeBERTaV3.
* Fineâ€‘tuned per language, with **sentiment-aware augmentation** for enhanced detection.

### 2. **Large Language Models (LLMs)**

* **Llamaâ€¯3.2â€‘1B** ([Hugging Face link](https://huggingface.co/meta-llama/Llama-3.2-1B))
* Evaluated out-of-the-box to benchmark against BERT-like systems.

---

## ğŸ“Š Final Results

* **BERT-like models outperformed** LLM baselines across all languages.
* **Sentiment integration** yielded notable improvements in SUBJ F1 for English and Italian.
* **Calibrated decision thresholds** proved critical to managing class imbalance effectively.

---

### ğŸ… Challenge Results â€” Top 3 Scores per Category

| **Setting**                 | **Participant**           | **Macroâ€¯F1** | **Position** |
|----------------------------|---------------------------|-------------:|-------------:|
| **Monolingual â€“ Arabic**   | aelboua                   | 0.69         | 1st          |
|                            | tomasbernal01             | 0.59         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.56**     | **5th**      |
| **Monolingual â€“ English**  | msmadi                    | 0.81         | 1st          |
|                            | kishan_g                  | 0.80         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.66**     | **19th**     |
| **Monolingual â€“ German**   | smollab                   | 0.85         | 1st          |
|                            | cepanca_UNAM              | 0.83         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.77**     | **5th**      |
| **Monolingual â€“ Italian**  | aelboua                   | 0.69         | 1st          |
|                            | Sumitjais                 | 0.67         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.63**     | **4th**      |
| **Zeroshot â€“ Greek**       | **AIâ€¯Wizards**           | **0.51**     | **1st**      |
|                            | smollab                   | 0.49         | 2nd          |
|                            | KnowThySelf               | 0.49         | 3rd          |
| **Zeroshot â€“ Polish**      | aelboua                   | 0.69         | 1st          |
|                            | Sumitjais                 | 0.67         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.63**     | **4th**      |
| **Zeroshot â€“ Romanian**    | msmadi                    | 0.81         | 1st          |
|                            | KnowThySelf               | 0.80         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.75**     | **7th**      |
| **Zeroshot â€“ Ukrainian**   | KnowThySelf               | 0.64         | 1st          |
|                            | Atherâ€‘Hashmi              | 0.64         | 2nd          |
|                            | **AIâ€¯Wizards**           | **0.64**     | **4th**      |
| **Multilingual**           | Bharatdeep_Hazarika       | 0.75         | 1st          |
|                            | kishan_g                  | 0.75         | 1st          |
|                            | **AIâ€¯Wizards**           | **0.24**     | **15th**     |

Due to a submission error during the challenge phase, our official **multilingual run** was accidentally low (Macroâ€¯F1â€¯=â€¯0.24, 15th place). However, after the fact, we corrected the submission issue offline and achieved a **corrected Macroâ€¯F1 of 0.68**, which would have placed **us 9th overall**.

### âš ï¸ Threshold Overfitting in the English Model

Upon reviewing the results, we suspect that our English model may have **overfit the decision threshold on the dev set**, which did not generalize well to the test data. This misalignment in threshold calibration likely caused the drop in performanceâ€”illustrating a common pitfall where **checkpoint and threshold selection on English dev data fails to translate effectively to unseen test sets**, especially in multilingual tasks.

---

## ğŸ“ Repository Layout

* `baseline/` â€“ baseline model (paraphrase-multilingual-MiniLM-L12-v2) from Sentence Transformers.
* `data/` â€“ task datasets.
* `img/` â€“ images with model pipeline schema and figures.
* `scorer/` â€“ evaluation utilities (`evaluate.py` supports F1 and threshold tuning).
* `requirements.txt` â€“ full Python environment dependencies if using `pip`.
* `pyproject.toml` â€“ uv dependencies if using [`uv`](https://github.com/astral-sh/uv).

---

## ğŸ—ï¸ System Architecture

![Model Pipeline Schema](img/model_pipeline_schema1.svg)

---

## ğŸ’» Usage Guide

### Setup

```bash
git clone https://github.com/MatteoFasulo/clef2025-checkthat.git
cd clef2025-checkthat
pip install -r requirements.txt
```

we recommend using a [`uv`](https://github.com/astral-sh/uv) environment for better dependency management. After installing `uv`, run:

```bash
uv sync
```

### Run Evaluation (English dev set)

The evaluation script requires the ground truth and predicted results in TSV format. The expected columns are `sentence_id`, and `label` (SUBJ or OBJ).

```bash
python scorer/evaluate.py \
  -g data/english/dev_en.tsv \
  -p results/dev_english_predicted.tsv
```

### Evaluate Sentiment-Augmented Variant

```bash
python scorer/evaluate.py \
  -g data/english/dev_en.tsv \
  -p results/dev_english_sentiment_predicted_.tsv
```

evaluation can be performed on any language by changing the `-g` and `-p` paths accordingly and providing the appropriate ground truth and predictions.

---

## ğŸ” License

Distributed under **CCâ€“BYâ€¯4.0**. See [LICENSE](LICENSE) for details.
